
Filename: .dockerignore
Content:
.env
.gitignore
docker.zip
Dockerfile
tree.txt
values.example.yaml
values.yaml
docker
.ubuntu
.venv
.vscode
logs
alembic.ini
alembic

================================================================================

Filename: .gitignore
Content:
.venv
.idea
.vscode
__pycache__
*.log
!logs/readme.md
.env
temp
docker/volumes/db/data
docker/volumes/storage
zoho_token_generator.py
docker
docker.zip
test_results.txt
values.yaml
values.example.yaml
tree.txt
repo_contents.txt
.ubuntu
.pytest_cache
*.prof
.coverage

================================================================================

Filename: .safety-project.ini
Content:
[project]
id = alert
url = /codebases/alert/findings
name = alert



================================================================================

Filename: Dockerfile
Content:
# Base Image: Using slim version for smaller image size while maintaining Python functionality
FROM python:3.12-slim AS builder

# Set the working directory for all subsequent commands
WORKDIR /app

# Copy requirements file first to leverage Docker cache layers
# This way, dependencies are only re-installed when requirements.txt changes
COPY requirements.txt .

# Create and configure Python virtual environment
# This isolates our dependencies from the system Python
RUN python -m venv /venv
ENV PATH="/venv/bin:$PATH"

# Upgrade pip and install dependencies
# --no-cache-dir reduces image size by not caching downloaded packages
# Requirements are installed in the virtual environment
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Start final stage for the actual runtime image
# This creates a clean image without build dependencies
FROM python:3.12-slim

# Copy only the virtual environment from builder stage
# This reduces final image size by excluding build artifacts
COPY --from=builder /venv /venv
ENV PATH="/venv/bin:$PATH"

# Configure working directory for application
WORKDIR /app

# Copy and configure entrypoint script
# chmod +x makes the script executable
COPY ./entrypoint.sh .
RUN chmod +x entrypoint.sh

# Copy application code
# The . . copies all files from the build context to the container
COPY . .

# Install minimal runtime dependencies and clean up
# ca-certificates are needed for HTTPS connections
# Cleanup reduces image size by removing package lists and cache
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/* \
    && find /venv -name "*.pyc" -delete \
    && find /venv -name "__pycache__" -delete

# This is for the API service mode
EXPOSE 8123

# Define the default command to run when container starts
# Uses entrypoint script to determine service type
CMD ["/app/entrypoint.sh"]

================================================================================

Filename: LICENSE
Content:
alert.ventory License
Grig Technologies Pvt Ltd Â· All rights reserved.
Version 0.0.0

1. License Grant
Permission is hereby granted to Grig Technologies Pvt. Ltd., its employees, and authorized contributors to view, modify, and contribute to the codebase of this project for internal use only.

2. Usage Restrictions
This project and its code, files, documentation, or any related materials may only be used within Grig Technologies or as authorized by Grig Technologies in writing.
No part of this project, including the code, documents, or other assets, may be shared, published, distributed, or disclosed to any third party, in whole or in part, without express written permission from Grig Technologies.
Contributors are not allowed to use or share the contents of this project in any form for personal, commercial, or public use outside the scope of this project.

3. Contribution Guidelines
Authorized contributors may only contribute code to the project.
Contributors must not copy or replicate any part of this project for use outside the project or share contributions with others without explicit written permission from Grig Technologies.
All contributions to this project are subject to approval and may be rejected by Grig Technologies at its sole discretion.

4. Confidentiality
This project contains proprietary and confidential information of Grig Technologies. By accessing and contributing to this project, contributors agree to maintain its confidentiality and not disclose any information regarding this project, either directly or indirectly, to unauthorized parties.

5. Intellectual Property
All contributions made to this project become the sole property of Grig Technologies. Contributors agree to waive all claims to intellectual property rights over any contribution.

6. Termination of Access
Grig Technologies reserves the right to terminate any contributor's access to this project at any time without prior notice, if the terms of this license are violated or for any other reason deemed necessary by Grig Technologies.

7. Legal Actions
Any violation of this license may result in legal action by Grig Technologies, including but not limited to, claims for damages or injunctions.

================================================================================

Filename: README.md
Content:
# alert.ventory
Version 0.1.3

A microservice to handle the alert system of Ventory. It manages user authentication via Zoho, email notifications using Temporal workflows, data storage with Supabase, and role-based access control.

- [LICENSE](LICENSE)
- [ADMIN CONTACT](https://github.com/shubhamistic)

## Table of Contents

- [Features](#features)
- [Technology Stack](#technology-stack)
- [Prerequisites](#prerequisites)
- [Getting Started](#getting-started)
  - [Clone Repository](#clone-repository)
  - [Environment Setup](#environment-setup)
- [Running Locally](#running-locally)
  - [Virtual Environment](#virtual-environment)
  - [Install Dependencies](#install-dependencies)
  - [Database Setup](#database-setup)
  - [Run API Server](#run-api-server)
  - [Run Temporal Workers](#run-temporal-workers)
- [Building and Pushing Docker Image](#building-and-pushing-docker-image)
- [Project Structure](#project-structure)
- [API Endpoints Overview](#api-endpoints-overview)
- [Testing](#testing)
- [Configuration](#configuration)
- [Contributing](#contributing)

## Features

* **User Authentication**: Secure login via Zoho OAuth2.
* **Session Management**: JWT-based access and refresh tokens.
* **Role-Based Access Control (RBAC)**: Granular control over users, permissions, and email group access.
* **Temporal Workflows for Emails**:
    * Priority-based email queues (high, medium, low).
    * Optional manual email verification step before sending.
    * Support for scheduled and recurring email notifications.
* **File Storage**: Integration with Supabase Storage for file uploads, downloads, and management (when Supabase is configured).
* **Tab Management**: API for managing user-specific UI states or data tabs.
* **Database Management**: Schema migrations handled by Alembic.
* **Comprehensive Logging**: Request and application logging.
* **Dockerized Application Image**: Build a portable Docker image for deployment.

## Technology Stack

* **Backend**: Python 3.12, FastAPI
* **Workflow Engine**: Temporal
* **Database**: PostgreSQL
* **Authentication**: Zoho OAuth2, PyJWT
* **ORM**: SQLAlchemy
* **Database Migrations**: Alembic
* **Storage Backend**: Compatible with Supabase Storage (or any S3-like storage if configured).
* **Containerization**: Docker

## Prerequisites

* Python 3.12+
* Docker (for building the application image)
* Access to a running Temporal service instance
* Access to a PostgreSQL database
* Zoho OAuth2 Client ID and Secret
* SMTP server credentials for email sending

## Getting Started

### Clone Repository

```bash
git clone <your-repository-url>
cd alert.ventory
```

### Environment Setup

Environment variables are crucial for configuring the application.
Create a `.env` file in the project root. This file will store your local configuration for running the application directly or for providing build arguments if needed.

Example key variables for a root `.env` file:

```
# Database (adjust for your local/target PostgreSQL setup)
DB_USER=your_db_user
DB_PASSWORD=your_db_password
DB_HOST=localhost # or your DB host
DB_PORT=5432
DB_NAME=alert_ventory_db
TENANT_ID=your_tenant_id # e.g., public or a specific schema user prefix for Supabase/Postgres
# SQLALCHEMY_DATABASE_URI can be constructed or set directly:
# SQLALCHEMY_DATABASE_URI=postgresql://{DB_USER}.{TENANT_ID}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}
# If not using tenant_id in username:
SQLALCHEMY_DATABASE_URI=postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}

# Supabase (if using Supabase client directly for some operations like storage)
SUPABASE_URL=http://your_supabase_instance_url # e.g., http://localhost:8000 if running Supabase locally
SERVICE_ROLE_KEY=your_supabase_service_role_key

# JWT
SECRET_KEY=a_very_strong_and_long_secret_key_for_jwt_please_change_me
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# Zoho OAuth
ZOHO_CLIENT_ID=your_zoho_client_id
ZOHO_CLIENT_SECRET=your_zoho_client_secret
ZOHO_REDIRECT_URI=http://localhost:8123/auth/callback # Example, adjust as needed for your deployment
ZOHO_TOKEN_URL=https://accounts.zoho.in/oauth/v2/token # Or your Zoho DC's token URL

# SMTP (for sending emails)
SMTP_HOST=your_smtp_server.com
SMTP_PORT=587 # or 465 for SSL
SMTP_USERNAME=your_smtp_username
SMTP_PASSWORD=your_smtp_password
EMAIL_FROM=noreply@yourdomain.com # Default sender address

# Temporal
TEMPORAL_HOST=localhost # Address of your Temporal frontend service
TEMPORAL_PORT=7233
# TEMPORAL_URL can be constructed or set directly:
# TEMPORAL_URL=${TEMPORAL_HOST}:${TEMPORAL_PORT}
```

These variables will be used by the application when run locally and can also be baked into the Docker image or supplied to the container at runtime.

## Running Locally

This method is for running the API server and workers directly on your machine, without using the main application Docker image.

### Virtual Environment

```bash
python -m venv .venv
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
# .venv\Scripts\activate
```

### Install Dependencies

It's recommended to use pip-compile if requirements.in is the primary source of dependencies.

```bash
pip install pip-tools
pip-compile requirements.in
pip install -r requirements.txt
```

If requirements.txt is already up-to-date:

```bash
pip install -r requirements.txt
```

### Database Setup

Ensure your PostgreSQL server is running and accessible as configured in your root `.env` file.

Apply Database Migrations:

```bash
alembic upgrade head
```

Initialize Roles, Permissions, and Admin Users:

The application attempts to do this on startup via `init_db()` in `models/__init__.py`.
If you need to run it manually (e.g., for setup scripts or troubleshooting):

```bash
python -c "from models import init_db; init_db()"
```

### Run API Server

```bash
python run_api.py
# Or, for development with auto-reload:
# uvicorn run_api:app --host 0.0.0.0 --port 8123 --reload
```

The API will be available at http://localhost:8123.
OpenAPI (Swagger) documentation: http://localhost:8123/docs.

### Run Temporal Workers

Ensure your Temporal server is running and accessible (configured in your root `.env`).

In a separate terminal window:

```bash
python run_worker.py
```

This will start the email workers, listening to task queues defined in `workers/email_workers.py`.

## Building and Pushing Docker Image

This project includes a root Dockerfile designed for building a portable application image. This image can be configured at runtime to start as either an API server or a Temporal worker via the `SERVICE_TYPE` environment variable (`api` or `worker`).

Use docker buildx to build a multi-platform image (if needed, though the example below specifies linux/amd64) and push it to a container registry.

```bash
docker buildx build --platform linux/amd64 -t harbor.grigtech.io/alert.ventory/backend:0.1.0 --push .
```

Explanation of the command:
- `docker buildx build`: Invokes the buildx builder.
- `--platform linux/amd64`: Specifies the target platform for the image. You can specify multiple platforms if needed (e.g., linux/amd64,linux/arm64).
- `-t harbor.grigtech.io/alert.ventory/backend:0.1.0`: Tags the image with the given name and tag. Replace with your registry and desired tag.
- `--push`: Pushes the image to the specified registry after a successful build. Ensure you are logged into the registry (`docker login harbor.grigtech.io`).
- `.`: Specifies the build context (the current directory), which means Docker will use the Dockerfile located in the project root.

After building and pushing, this image can be deployed to any container orchestration platform. Remember to provide the necessary environment variables (as defined in `.env`) to the running container.

## Project Structure

A brief overview of important directories:

```
alert.ventory/
âââ .dockerignore           # Files to ignore for Docker builds
âââ .gitignore              # Files ignored by Git
âââ Dockerfile              # Root Dockerfile for building the application image (used by `docker buildx`)
âââ LICENSE                 # Project License
âââ README.md               # This file
âââ alembic/                # Alembic database migration scripts
âââ alembic.ini             # Alembic configuration
âââ activities/             # Temporal activity definitions (e.g., email sending logic)
âââ configs/                # Application configuration files (DB, JWT, SMTP, roles.json, etc.)
âââ controllers/            # Business logic handlers for API routes
âââ docker/                 # Contains supplementary Docker-related files.
â                           # This includes Dockerfiles for specific services if needed for a
â                           # local composite development stack (e.g., using docker-compose.yml
â                           # for local testing of Supabase/Temporal alongside the app),
â                           # and configurations for such supporting services.
â                           # See docker/readme.md for details on this auxiliary setup.
âââ entrypoint.sh           # Entrypoint script for the root Dockerfile, allows selecting service type
âââ handlers/               # Global handlers (e.g., custom exception handlers)
âââ logs/                   # Runtime log files (mostly ignored by Git)
âââ models/                 # SQLAlchemy database models and initialization logic
âââ requirements.in         # Pip-tools input for dependencies
âââ requirements.txt        # Pinned Python dependencies
âââ routes/                 # FastAPI routers defining API endpoints
âââ run_api.py              # Entrypoint script to run the FastAPI server locally
âââ run_worker.py           # Entrypoint script to run Temporal workers locally
âââ schemas/                # Pydantic models for API request/response validation
âââ tests/                  # Automated tests (unit, integration)
âââ utils/                  # Shared utility functions (JWT, Zoho token helpers)
âââ workflows/              # Temporal workflow definitions
```

## API Endpoints Overview

The application exposes several groups of API endpoints:

- `/auth`: User authentication, token generation, and refresh.
- `/users`: User management (CRUD operations, role assignments).
- `/rbac`: Role and Permission management.
- `/email-groups`: Management of email distribution groups.
- `/emails`: Sending emails (immediate, scheduled, recurring) and handling confirmations.
- `/storage`: File upload, download, and management.
- `/tabs`: User-specific state/data management.
- `/workflows`: Listing user's workflows, checking status, and termination.

For a detailed and interactive API specification, run the application locally and navigate to `/docs` in your browser (e.g., http://localhost:8123/docs).

## Testing

The project uses pytest for automated testing.

Install test dependencies (if not already installed with main requirements):

```bash
pip install -r tests/requirements-test.txt
# or ensure pytest and related plugins are in your main requirements.txt
```

Run tests from the project root directory:

```bash
pytest
```

Test results and coverage information (if configured) might be output to the console or specific files (e.g., results_cov.txt).

## Configuration

- **Core Settings**: Primarily managed via environment variables loaded from a `.env` file (see Environment Setup). These are used both for local runs and should be supplied to Docker containers.
- **Static Data**:
  - Initial roles and their permissions: `configs/roles_permissions.json`
  - Initial admin user emails: `configs/admin.json`
- **Service Configurations**: Specific configurations for database, JWT, SMTP, and Temporal client are modularized within the `configs/` directory.
- **Database Schema**: Managed by Alembic (see `alembic.ini` and `alembic/` directory).

## Contributing

Please refer to the LICENSE file. Contributions and use of this project are subject to the terms outlined therein, currently restricted to authorized personnel of Grig Technologies Pvt. Ltd.

================================================================================

Filename: activities/README.md
Content:
# activities/

- Contains all activity definitions.
- Similar to workflows, activities should be modular, focusing on single responsibilities.
- Allows easy addition of new activities without disrupting others.

================================================================================

Filename: activities/email_activity.py
Content:
import smtplib
import ssl
from email.message import EmailMessage
from temporalio import activity
from configs.smtp_config import smtp_config
import json


def create_email_message(from_email: str, to_emails: list[str], cc_emails: list[str], bcc_emails: list[str],
                         subject: str, body: str) -> EmailMessage:
    msg = EmailMessage()
    msg['From'] = from_email
    msg['To'] = ", ".join(to_emails)  # EmailMessage can handle a list for multiple recipients
    if cc_emails:
        msg['Cc'] = ", ".join(cc_emails)
    if bcc_emails:
        msg['Bcc'] = ", ".join(bcc_emails)
    msg['Subject'] = subject
    msg.set_content(f"{body}", subtype='html')
    return msg


@activity.defn
async def send_test_email_activity(args_json: str) -> bool:
    try:
        # Parse arguments
        args = json.loads(args_json)
        from_email = args["from_email"]
        to_emails = args["to_emails"]
        cc_emails = args["cc_emails"]
        bcc_emails = args["bcc_emails"]
        subject = args["subject"]
        body = args["body"]

        test_body = f"""
        <div>
            <h2>Test Email - Requires Verification</h2>
            <p>Please verify the following email content:</p>
            <hr>
            <h3>Subject: {subject}</h3>
            <div>{body}</div>
            <hr>
            <p>Recipients:</p>
            <ul>
                <li>To: {", ".join(to_emails)}</li>
                {"<li>CC: " + ", ".join(cc_emails) + "</li>" if cc_emails else ""}
                {"<li>BCC: " + ", ".join(bcc_emails) + "</li>" if bcc_emails else ""}
            </ul>
            <hr>
            <p>To confirm sending this email, use the confirmation endpoint with the provided workflow ID.</p>
        </div>
        """

        msg = create_email_message(from_email, to_emails, cc_emails, bcc_emails, f"[TEST] {subject}", test_body)

        if smtp_config.PORT == 465:
            context = ssl.create_default_context()
            with smtplib.SMTP_SSL(smtp_config.SMTP_SERVER, smtp_config.PORT, context=context) as server:
                server.login(smtp_config.USERNAME, smtp_config.PASSWORD)
                server.send_message(msg)
        elif smtp_config.PORT == 587:
            with smtplib.SMTP(smtp_config.SMTP_SERVER, smtp_config.PORT, timeout=30) as server:
                server.set_debuglevel(1)
                server.starttls()
                server.login(smtp_config.USERNAME, smtp_config.PASSWORD)
                server.send_message(msg)
        else:
            activity.logger.error("Invalid port configuration. Use 465 or 587.")
            return False

        return True

    except Exception as e:
        activity.logger.error(f"Failed to send test email: {str(e)}")
        return False


@activity.defn
async def send_email_activity(args_json: str) -> bool:
    try:
        args = json.loads(args_json)
        from_email = args["from_email"]
        to_emails = args["to_emails"]
        cc_emails = args["cc_emails"]
        bcc_emails = args["bcc_emails"]
        subject = args["subject"]
        body = args["body"]

        msg = create_email_message(from_email, to_emails, cc_emails, bcc_emails, subject, body)

        activity.logger.info(f"Sending email to {to_emails} with subject: {subject}")
        if cc_emails:
            activity.logger.info(f"CC: {cc_emails}")
        if bcc_emails:
            activity.logger.info(f"BCC: {bcc_emails}")

        if smtp_config.PORT == 465:
            context = ssl.create_default_context()
            with smtplib.SMTP_SSL(smtp_config.SMTP_SERVER, smtp_config.PORT, context=context) as server:
                server.login(smtp_config.USERNAME, smtp_config.PASSWORD)
                server.send_message(msg)
        elif smtp_config.PORT == 587:
            with smtplib.SMTP(smtp_config.SMTP_SERVER, smtp_config.PORT, timeout=30) as server:
                server.set_debuglevel(1)
                server.starttls()
                server.login(smtp_config.USERNAME, smtp_config.PASSWORD)
                server.send_message(msg)
        else:
            activity.logger.error("Invalid port configuration. Use 465 or 587.")
            return False

        return True

    except Exception as e:
        activity.logger.error(f"Failed to send email: {str(e)}")
        return False


================================================================================

Filename: alembic.ini
Content:
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


================================================================================

Filename: alembic/README
Content:
Generic single-database configuration.

================================================================================

Filename: alembic/env.py
Content:
import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from dotenv import load_dotenv
load_dotenv()

from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
from models.base import Base, schema_name # Import Base and schema_name from your models
from configs.database import SQLALCHEMY_DATABASE_URI # Import your database URI

config.set_main_option('sqlalchemy.url', SQLALCHEMY_DATABASE_URI) # Set the database URL

target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.

def include_object(object, name, type_, reflected, compare_to):
    """Hook to filter objects during autogenerate.
    Only include objects within the 'public' schema.
    """
    if type_ == "table" and object.schema != schema_name:
        return False
    if type_ == "index" and object.table.schema != schema_name:
         return False
    # Add other object types like sequences if necessary
    # For sequences, check object.schema
    # if type_ == "sequence" and object.schema != schema_name:
    #     return False
    return True

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        version_table_schema=schema_name, # Add schema name here
        include_schemas=True, # Ensure schema is included
        include_object=include_object # Add filter hook
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    from configs.database import engine # Import your engine
    connectable = engine

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            version_table_schema=schema_name, # Add schema name here
            include_schemas=True, # Ensure schema is included
            include_object=include_object # Add filter hook
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


================================================================================

Filename: alembic/script.py.mako
Content:
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}


================================================================================

Filename: alembic/versions/de4eaff36c53_create_initial_tables_including_tabs_.py
Content:
"""Create initial tables (including tabs and workflows)

Revision ID: de4eaff36c53
Revises: 
Create Date: 2025-04-18 17:46:39.176220

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'de4eaff36c53'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('email_groups',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name'),
    schema='public'
    )
    op.create_table('permissions',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name'),
    schema='public'
    )
    op.create_table('roles',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name'),
    schema='public'
    )
    op.create_table('users',
    sa.Column('uid', sa.String(), nullable=False),
    sa.Column('email', sa.String(), nullable=True),
    sa.Column('UserName', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('profile_image_url', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('uid'),
    sa.UniqueConstraint('uid'),
    schema='public'
    )
    op.create_table('email_addresses',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('email', sa.String(), nullable=False),
    sa.Column('group_id', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['group_id'], ['public.email_groups.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id'),
    schema='public'
    )
    op.create_table('role_email_groups',
    sa.Column('role_id', sa.Integer(), nullable=True),
    sa.Column('email_group_id', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['email_group_id'], ['public.email_groups.id'], ),
    sa.ForeignKeyConstraint(['role_id'], ['public.roles.id'], ),
    schema='public'
    )
    op.create_table('role_permissions',
    sa.Column('role_id', sa.Integer(), nullable=True),
    sa.Column('permission_id', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['permission_id'], ['public.permissions.id'], ),
    sa.ForeignKeyConstraint(['role_id'], ['public.roles.id'], ),
    schema='public'
    )
    op.create_table('tabs',
    sa.Column('tab_id', sa.UUID(), nullable=False),
    sa.Column('uid', sa.String(), nullable=False),
    sa.Column('subject', sa.String(), nullable=True),
    sa.Column('from_address', sa.String(), nullable=True),
    sa.Column('to_address', sa.JSON(), nullable=True),
    sa.Column('cc_address', sa.JSON(), nullable=True),
    sa.Column('bcc_address', sa.JSON(), nullable=True),
    sa.Column('body', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['uid'], ['public.users.uid'], ),
    sa.PrimaryKeyConstraint('tab_id'),
    schema='public'
    )
    op.create_index(op.f('ix_public_tabs_uid'), 'tabs', ['uid'], unique=False, schema='public')
    op.create_table('user_roles',
    sa.Column('uid', sa.String(), nullable=True),
    sa.Column('role_id', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['role_id'], ['public.roles.id'], ),
    sa.ForeignKeyConstraint(['uid'], ['public.users.uid'], ),
    schema='public'
    )
    op.create_table('workflows',
    sa.Column('workflow_id', sa.String(), nullable=False),
    sa.Column('uid', sa.String(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['uid'], ['public.users.uid'], ),
    sa.PrimaryKeyConstraint('workflow_id'),
    schema='public'
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('workflows', schema='public')
    op.drop_table('user_roles', schema='public')
    op.drop_index(op.f('ix_public_tabs_uid'), table_name='tabs', schema='public')
    op.drop_table('tabs', schema='public')
    op.drop_table('role_permissions', schema='public')
    op.drop_table('role_email_groups', schema='public')
    op.drop_table('email_addresses', schema='public')
    op.drop_table('users', schema='public')
    op.drop_table('roles', schema='public')
    op.drop_table('permissions', schema='public')
    op.drop_table('email_groups', schema='public')
    # ### end Alembic commands ###


================================================================================

Filename: configs/README.md
Content:
# configs/

- Centralizes all configuration settings.
- Includes global settings, Temporal-specific configurations, and secrets.
- Makes it easy to switch configurations for different environments (e.g., dev, staging, prod).

================================================================================

Filename: configs/__init__.py
Content:


================================================================================

Filename: configs/admin.json
Content:
{
  "shubham.garg@grigtechnologies.com": true,
  "rudra.garg@grigtechnologies.com": true,
  "gautam.vhavle@grigtechnologies.com": true,
  "snehanshu.pal@grigtechnologies.com": true
}

================================================================================

Filename: configs/database.py
Content:
# Standard library imports
import os
from supabase import create_client, Client
from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Load environment variables
load_dotenv()

# Database connection parameters from environment variables
# These variables configure the PostgreSQL connection with tenant isolation
DB_USER = os.getenv('DB_USER', 'postgres')
DB_PASSWORD = os.getenv('DB_PASSWORD', 'your-super-secret-password')
DB_HOST = os.getenv('DB_HOST', 'localhost')
DB_PORT = os.getenv('DB_PORT', '5432')
DB_NAME = os.getenv('DB_NAME', 'postgres')
TENANT_ID = os.getenv('TENANT_ID', 'your-tenant-id')
SUPABASE_URL = os.getenv('SUPABASE_URL', 'http://localhost:8080')
# Construct database URI with tenant isolation
SQLALCHEMY_DATABASE_URI = f'postgresql://{DB_USER}.{TENANT_ID}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'

# Application secret key for security operations
secret_key = os.getenv("SECRET_KEY", "your_default_secret_key")

# Debug database connection string
print(f"SQLALCHEMY_DATABASE_URI: {SQLALCHEMY_DATABASE_URI}")

# Initialize SQLAlchemy engine and session factory
engine = create_engine(SQLALCHEMY_DATABASE_URI)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def get_db():
    """
    Database session dependency for FastAPI.
    Yields a database session and ensures proper cleanup.
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def get_supabase_client() -> Client:
    """
    Get Supabase client instance for additional storage capabilities.
    Returns:
        Client: Configured Supabase client
    """
    return create_client(
        supabase_url=SUPABASE_URL,
        supabase_key=os.getenv("SERVICE_ROLE_KEY")
    )


================================================================================

Filename: configs/jwt_config.py
Content:
# Standard library imports
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# JWT Configuration
# These settings control token generation and validation
SECRET_KEY = os.getenv("SECRET_KEY", "your_default_secret_key")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))
REFRESH_TOKEN_EXPIRE_DAYS = int(os.getenv("REFRESH_TOKEN_EXPIRE_DAYS", 7))

# Zoho OAuth Configuration
# Required for authentication integration with Zoho services
ZOHO_CLIENT_ID = os.getenv("ZOHO_CLIENT_ID")
ZOHO_CLIENT_SECRET = os.getenv("ZOHO_CLIENT_SECRET")
ZOHO_REDIRECT_URI = os.getenv("ZOHO_REDIRECT_URI")
ZOHO_TOKEN_URL = os.getenv("ZOHO_TOKEN_URL")


================================================================================

Filename: configs/logging_config.py
Content:
# Standard library imports for logging and request handling
import logging
import os
import time
import uuid
from logging.handlers import RotatingFileHandler
from fastapi import Request
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware

# Configure logging directory
LOGS_DIRECTORY = "logs"
if not os.path.exists(LOGS_DIRECTORY):
    os.makedirs(LOGS_DIRECTORY)


def get_logger(namespace: str, route: str) -> logging.Logger:
    """
    Creates or retrieves a logger instance for a specific namespace and route.
    Implements rotating file handlers for log management.
    
    Args:
        namespace (str): The logging namespace (e.g., 'auth', 'email')
        route (str): The specific route within the namespace
    
    Returns:
        logging.Logger: Configured logger instance
    """
    global LOGS_DIRECTORY
    logger_name = f"{namespace}.{route}"
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.INFO)

    if not logger.hasHandlers():
        namespace_dir = os.path.join(LOGS_DIRECTORY, namespace)
        if not os.path.exists(namespace_dir):
            os.makedirs(namespace_dir)

        # Configure rotating file handler with 10MB size limit
        handler = RotatingFileHandler(
            os.path.join(namespace_dir, f"{route}.log"),
            maxBytes=10485760,  # 10MB
            backupCount=10,
        )
        handler.setLevel(logging.INFO)

        # Set standard logging format
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    return logger


class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Middleware for logging HTTP requests and responses.
    Tracks request timing and handles error logging.
    """
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        request_id = str(uuid.uuid4())

        # Extract namespace and route from request path
        path_parts = request.url.path.strip("/").split("/")
        namespace = path_parts[0] if path_parts else "main"
        route = path_parts[1] if len(path_parts) > 1 else "index"
        logger = get_logger(namespace, route)

        # Log request start
        logger.info(
            f"Request started: {request.method} {request.url.path} - ID: {request_id}"
        )

        try:
            response = await call_next(request)
        except Exception as e:
            # Log and handle errors
            logger.exception(f"An error occurred: {str(e)} - ID: {request_id}")
            return JSONResponse(content={'message': 'An internal error occurred'}, status_code=500)

        # Log request completion with timing
        diff = time.time() - start_time
        logger.info(
            f"Request completed: {response.status_code} in {diff:.2f}s - ID: {request_id}"
        )

        return response


================================================================================

Filename: configs/modes_config.json
Content:
{
  "enterprise": {
    "enable_docs": true,
    "fastapi_title": "alert.ventory (Enterprise)",
    "fastapi_description": "A microservice to handle alert system of Ventory - Full Access",
    "fastapi_version": "0.1.3",
    "active_routers": [
      "auth",
      "users",
      "emails",
      "storage",
      "rbac",
      "email_groups",
      "workflows",
      "tabs",
      "scheduled_emails",
      "recurring_emails"
    ]
  },
  "product": {
    "enable_docs": true,
    "fastapi_title": "alert.ventory (Product)",
    "fastapi_description": "Ventory Alerting Service",
    "fastapi_version": "0.1.3",
    "active_routers": [
      "auth",
      "users",
      "emails"
    ]
  }
}

================================================================================

Filename: configs/modes_config.py
Content:
import json
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# --- Read the run mode ---
APP_RUN_MODE = os.getenv("APP_RUN_MODE", "enterprise")  # Default to enterprise if not set

# --- Load the JSON configuration ---
CONFIG_FILE_PATH = Path(__file__).parent / "modes_config.json"

if not CONFIG_FILE_PATH.exists():
    raise RuntimeError(f"Mode configuration file not found: {CONFIG_FILE_PATH}")

try:
    with open(CONFIG_FILE_PATH, "r") as f:
        ALL_MODES_CONFIG = json.load(f)
except json.JSONDecodeError as e:
    raise RuntimeError(f"Error decoding {CONFIG_FILE_PATH}: {e}")

if APP_RUN_MODE not in ALL_MODES_CONFIG:
    raise ValueError(
        f"Invalid APP_RUN_MODE: '{APP_RUN_MODE}'. "
        f"Must be one of {list(ALL_MODES_CONFIG.keys())} as defined in {CONFIG_FILE_PATH}"
    )

# --- Expose current mode's settings ---
CURRENT_MODE_CONFIG = ALL_MODES_CONFIG[APP_RUN_MODE]

ENABLE_DOCS = CURRENT_MODE_CONFIG.get("enable_docs", False)
ACTIVE_ROUTER_KEYS = CURRENT_MODE_CONFIG.get("active_routers", [])
FASTAPI_TITLE = CURRENT_MODE_CONFIG.get("fastapi_title", "alert.ventory")
FASTAPI_DESCRIPTION = CURRENT_MODE_CONFIG.get("fastapi_description", "Ventory Alerting Service")
FASTAPI_VERSION = CURRENT_MODE_CONFIG.get("fastapi_version", "0.1.3")


# You can also add a function if you need more complex logic later
def get_current_mode_details():
    return {
        "run_mode": APP_RUN_MODE,
        "enable_docs": ENABLE_DOCS,
        "active_routers": ACTIVE_ROUTER_KEYS,
        "title": FASTAPI_TITLE,
        "description": FASTAPI_DESCRIPTION,
        "version": FASTAPI_VERSION
    }


if __name__ == "__main__":
    # For testing this module directly
    print(f"Application Run Mode: {APP_RUN_MODE}")
    print(f"Docs Enabled: {ENABLE_DOCS}")
    print(f"Active Router Keys: {ACTIVE_ROUTER_KEYS}")
    print(f"FastAPI Title: {FASTAPI_TITLE}")
    print(f"FastAPI Description: {FASTAPI_DESCRIPTION}")


================================================================================

Filename: configs/roles_permissions.json
Content:
{
  "roles": [
    {
      "name": "admin",
      "description": "Administrator role with all permissions",
      "permissions": [
        "basic_access",
        "users:create", "users:view", "users:edit", "users:delete",
        "roles:create", "roles:view", "roles:edit", "roles:delete",
        "permissions:create", "permissions:view", "permissions:edit", "permissions:delete",
        "email:send", "email:verify",
        "storage:write",
        "storage:read",
        "storage:delete",
        "workflow:view",
        "workflow:terminate",
        "tabs:create", "tabs:view", "tabs:edit", "tabs:delete"
      ]
    },
    {
      "name": "viewer",
      "description": "Viewer role with permissions to view users and permissions",
      "permissions": [
        "basic_access",
        "users:view",
        "roles:view",
        "permissions:view",
        "storage:read",
        "tabs:create", "tabs:view"
      ]
    },
    {
      "name": "manager",
      "description": "Manager role with permissions to manage users and roles",
      "permissions": [
        "basic_access",
        "users:create", "users:view", "users:edit", "users:delete",
        "roles:create", "roles:view", "roles:edit", "roles:delete",
        "tabs:create", "tabs:view", "tabs:edit", "tabs:delete"
      ]
    },
    {
      "name": "editor",
      "description": "Editor role with permissions to edit users, roles, and permissions",
      "permissions": [
        "basic_access",
        "users:edit",
        "roles:edit",
        "permissions:edit",
        "tabs:create", "tabs:view", "tabs:edit", "tabs:delete"
      ]
    },
    {
      "name": "email_manager",
      "description": "Email manager role with permissions to send and verify emails",
      "permissions": ["basic_access",
        "email:send",
        "email:verify",
        "tabs:create", "tabs:view", "tabs:edit", "tabs:delete"
      ]
    },
    {
      "name": "storage_manager",
      "description": "Storage manager role with permissions to manage storage",
      "permissions": ["basic_access",
        "storage:write",
        "storage:read",
        "storage:delete"
      ]
    },
    {
      "name": "storage_viewer",
      "description": "Storage viewer role with permissions to read storage",
      "permissions": ["basic_access",
        "storage:read"
      ]
    },
    {
      "name": "email_viewer",
      "description": "Email viewer role with permissions to view email-related data",
      "permissions": ["basic_access",
        "email:send",
        "tabs:create", "tabs:view", "tabs:edit", "tabs:delete"
      ]
    },
    {
      "name": "email_group_manager",
      "description": "Role for managing email groups",
      "permissions": [
        "basic_access",
        "email_groups:create",
        "email_groups:edit",
        "email_groups:delete",
        "email_groups:view"
      ]
    }
  ],
  "permissions": [
    {
        "name": "basic_access",
        "description": "Basic Access to the portal"
    },
    {
      "name": "users:create",
      "description": "Permission to create users"
    },
    {
      "name": "users:view",
      "description": "Permission to view users"
    },
    {
      "name": "users:edit",
      "description": "Permission to edit users"
    },
    {
      "name": "users:delete",
      "description": "Permission to delete users"
    },
    {
      "name": "roles:create",
      "description": "Permission to create roles"
    },
    {
      "name": "roles:view",
      "description": "Permission to view roles"
    },
    {
      "name": "roles:edit",
      "description": "Permission to edit roles"
    },
    {
      "name": "roles:delete",
      "description": "Permission to delete roles"
    },
    {
      "name": "permissions:create",
      "description": "Permission to create permissions"
    },
    {
      "name": "permissions:view",
      "description": "Permission to view permissions"
    },
    {
      "name": "permissions:edit",
      "description": "Permission to edit permissions"
    },
    {
      "name": "permissions:delete",
      "description": "Permission to delete permissions"
    },
    {
      "name": "email:send",
      "description": "Permission to send emails"
    },
    {
      "name": "email:verify",
      "description": "Permission to verify emails"
    },
    {
      "name": "storage:write",
      "description": "Permission to write to storage"
    },
    {
      "name": "storage:read",
      "description": "Permission to read from storage"
    },
    {
      "name": "storage:delete",
      "description": "Permission to delete from storage"
    },
    {
      "name": "email:manage",
      "description": "Permission to manage email groups"
    },
    {
      "name": "email:view",
      "description": "Permission to view email groups"
    },
    {
      "name": "email_groups:create",
      "description": "Permission to create email groups"
    },
    {
      "name": "email_groups:edit",
      "description": "Permission to edit email groups"
    },
    {
      "name": "email_groups:delete",
      "description": "Permission to delete email groups"
    },
    {
      "name": "email_groups:view",
      "description": "Permission to view email groups"
    },
    {
      "name": "workflow:view",
      "description": "Permission to view workflows"
    },
    {
      "name": "workflow:terminate",
      "description": "Permission to terminate workflows"
    },
    {
      "name": "tabs:create",
      "description": "Permission to create tabs"
    },
    {
      "name": "tabs:view",
      "description": "Permission to view tabs"
    },
    {
      "name": "tabs:edit",
      "description": "Permission to edit tabs"
    },
    {
      "name": "tabs:delete",
      "description": "Permission to delete tabs"
    }
  ]
}

================================================================================

Filename: configs/smtp_config.py
Content:
# Standard library imports
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


class SMTPConfig:
    """
    SMTP Configuration class for email services.
    Manages email server connection settings.
    """
    def __init__(self):
        """Initialize SMTP configuration with environment variables"""
        self.SMTP_SERVER = os.getenv("SMTP_HOST", "smtp.zeptomail.in")
        self.PORT = int(os.getenv("SMTP_PORT", "587"))
        self.USERNAME = os.getenv("SMTP_USERNAME", "emailapikey")
        self.PASSWORD = os.getenv("SMTP_PASSWORD", "")

    def __str__(self):
        """
        String representation of SMTP configuration.
        Useful for debugging and logging.
        """
        return (
            f"SMTP Configuration:\n"
            f"Server: {self.SMTP_SERVER}\n"
            f"Port: {self.PORT}\n"
            f"Username: {self.USERNAME}\n"
            f"Password: {self.PASSWORD}"
        )


# Create global SMTP configuration instance
smtp_config = SMTPConfig()

# Debug configuration when run directly
if __name__ == "__main__":
    print(smtp_config)


================================================================================

Filename: configs/temporal_config.py
Content:
# Standard library imports
import asyncio
import os
from typing import Any

# Third-party imports
import dotenv
from temporalio.client import Client, TLSConfig, WorkflowHandle

# Load environment variables
dotenv.load_dotenv()

# Email configuration
EMAIL_FROM = os.getenv("EMAIL_FROM", "noreply@ventory.in")

# Temporal service configuration
TEMPORAL_HOST = os.getenv("TEMPORAL_HOST", "temporal")
TEMPORAL_PORT = os.getenv("TEMPORAL_PORT", "80")
TEMPORAL_URL = f"{TEMPORAL_HOST}"

# Client connection caching
_client_cache = None
_client_lock = asyncio.Lock()


async def get_temporal_client() -> Client:
    """
    Get or create a Temporal client connection.
    Implements connection caching with thread-safe access.
    
    Returns:
        Client: Connected Temporal client instance
    """
    global _client_cache

    if _client_cache is not None:
        return _client_cache

    async with _client_lock:
        # Double-check pattern for thread safety
        if _client_cache is not None:
            return _client_cache

        # Create new client connection with TLS
        _client_cache = await Client.connect(TEMPORAL_URL, tls=TLSConfig())
        return _client_cache


async def get_workflow_handle(workflowId: str) -> WorkflowHandle[Any, Any]:
    """
    Get the workflow handle for the Temporal client.
    
    Args:
        workflowId (str): The ID of the workflow.
    Returns:
        WorkflowHandle: The handle for the specified workflow.
    """
    client = await get_temporal_client()
    return client.get_workflow_handle(workflowId)


================================================================================

Filename: controllers/__init__.py
Content:


================================================================================

Filename: controllers/auth_controller.py
Content:
"""
Authentication Controller Module

Handles all authentication-related operations including:
- Zoho OAuth2 authentication flow
- Token generation and validation
- User session management
- Permission verification

This module integrates with Zoho's OAuth service for secure user authentication
and manages JWT token generation for maintaining user sessions.
"""

# Standard library imports
import requests
from datetime import timedelta

# FastAPI and database imports
from fastapi import HTTPException
from sqlalchemy.orm import Session
from starlette import status

# Configuration and utility imports
from configs.jwt_config import (
    ACCESS_TOKEN_EXPIRE_MINUTES, 
    ZOHO_CLIENT_ID, 
    ZOHO_CLIENT_SECRET, 
    ZOHO_REDIRECT_URI, 
    ZOHO_TOKEN_URL
)
from models.users_model import User
from utils.jwt import create_access_token, validate_token, create_refresh_token, TokenType
from utils.zoho_token import decode_zoho_token


async def get_zoho_tokens(auth_code: str):
    """
    Exchange authorization code for Zoho access tokens.
    
    Args:
        auth_code (str): The authorization code received from Zoho OAuth
    
    Returns:
        dict: Contains access_token, refresh_token, and id_token
    
    Raises:
        HTTPException: If token exchange fails
    """
    # Prepare token exchange request data
    data = {
        "client_id": ZOHO_CLIENT_ID,
        "client_secret": ZOHO_CLIENT_SECRET,
        "grant_type": "authorization_code",
        "redirect_uri": ZOHO_REDIRECT_URI,
        "code": auth_code
    }

    try:
        # Make token exchange request to Zoho
        response = requests.post(ZOHO_TOKEN_URL, data=data) 
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Error fetching Zoho tokens: {str(e)}"
        )


async def authenticate_user(auth_code: str, db: Session):
    """
    Authenticate user using Zoho OAuth and create session tokens.
    
    Args:
        auth_code (str): Authorization code from Zoho OAuth flow
        db (Session): Database session for user lookup
    
    Returns:
        dict: Contains access_token, refresh_token, and user information
    
    Raises:
        HTTPException: For various authentication failures
    """
    # Exchange auth code for Zoho tokens
    zoho_tokens = await get_zoho_tokens(auth_code)
    
    # Validate ID token presence
    id_token = zoho_tokens.get("id_token")
    if not id_token:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ID token not found in Zoho response"
        )

    # Extract user information from ID token
    decoded_token = decode_zoho_token(id_token)
    email = decoded_token.get("email")

    if not email:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email not found in Zoho token"
        )

    # Verify user exists in database
    user = db.query(User).filter(User.email == email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Email not found"
        )

    # Verify user has basic access permission
    if not user.has_permission("basic_access"):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied"
        )

    # Generate access and refresh tokens
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={
            "sub": email,
            "permissions": list(user.get_all_permissions()),
            "roles": [role.name for role in user.roles]
        },
        expires_delta=access_token_expires
    )

    refresh_token = create_refresh_token(
        data={"sub": email}
    )

    # Return authentication response
    return {
        "access_token": access_token,
        "refresh_token": refresh_token,
        "token_type": "bearer",
        "permissions": list(user.get_all_permissions()),
        "roles": [role.name for role in user.roles]
    }

async def refresh_token(refresh_token: str, db: Session):
    try:
        # Validate the refresh token specifically
        payload = await validate_token(refresh_token, required_type=TokenType.REFRESH)
        
        # Extract user email from token
        email = payload.get("sub")
        if not email:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token"
            )

        # Get user from database
        user = db.query(User).filter(User.email == email).first()
        if not user:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="User not found"
            )

        # Check if user has basic access permission
        if not user.has_permission("basic_access"):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied"
            )

        # Create new access token
        access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        new_access_token = create_access_token(
            data={
                "sub": email,
                "permissions": list(user.get_all_permissions()),
                "roles": [role.name for role in user.roles]
            },
            expires_delta=access_token_expires
        )

        return {
            "access_token": new_access_token,
            "token_type": "bearer",
            "permissions": list(user.get_all_permissions()),
            "roles": [role.name for role in user.roles]
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate refresh token"
        )

================================================================================

Filename: controllers/email_controller.py
Content:
"""
Email Controller Module

Manages email operations including:
- Email sending with priority queues
- Email group resolution
- Permission verification
- Workflow management for email processing

This module integrates with Temporal for reliable email processing
and implements role-based access control for email operations.
"""

# Standard library imports
import json
import time

# FastAPI and database imports
from fastapi import HTTPException
from sqlalchemy.orm import Session
from temporalio.client import Client
from temporalio.service import TLSConfig

# Application imports
from configs.temporal_config import get_workflow_handle, get_temporal_client, EMAIL_FROM
from models.rbac_model import EmailGroup
from models.users_model import User
from models.workflow_model import Workflow
from schemas.email_schema import EmailCreate, EmailConfirmation
from workflows.email_workflow import EmailWorkflow


class EmailController:
    """
    Handles all email-related operations including sending, verification,
    and permission checking.
    """

    @staticmethod
    def _check_email_permissions(user: User, email_data: EmailCreate):
        """
        Verify user has required permissions for email operations.
        
        Args:
            user (User): The user attempting the operation
            email_data (EmailCreate): Email data containing permissions requirements
        
        Raises:
            HTTPException: If user lacks required permissions
        """
        # Check basic email sending permission
        if not user.has_permission("email:send"):
            raise HTTPException(
                status_code=403,
                detail="Insufficient permissions to send emails"
            )

        # For verification emails, check additional permission
        if email_data.requires_verification and not user.has_permission("email:verify"):
            raise HTTPException(
                status_code=403,
                detail="Insufficient permissions to send verification emails"
            )

        # Check permissions only for email groups, not individual emails
        for email in email_data.to_emails:
            # Only check permissions if it's an email group (not an email address)
            if not '@' in str(email) and not user.can_send_to_group(str(email)):
                raise HTTPException(
                    status_code=403,
                    detail=f"Not authorized to send emails to group: {email}"
                )

    @staticmethod
    def _resolve_email_groups(db: Session, emails: list[str]) -> list[str]:
        """
        Convert email group names to individual email addresses.
        
        Args:
            db (Session): Database session
            emails (list[str]): List of email addresses and group names
        
        Returns:
            list[str]: List of resolved individual email addresses
        """
        if not emails:
            return []

        resolved_emails = set()

        for email in emails:
            # Check if this is an email group
            group = db.query(EmailGroup).filter(EmailGroup.name == email).first()
            if group:
                # Add all emails from the group
                resolved_emails.update(group.get_email_addresses())
            else:
                # It's a direct email address
                resolved_emails.add(email)

        return list(resolved_emails)

    @staticmethod
    async def handle_send_email(
            email_data: EmailCreate,
            current_user: User,
            db: Session
    ):
        """
        Process and send an email through the workflow system.
        
        Args:
            email_data (EmailCreate): Email content and recipients
            current_user (User): User initiating the send
            db (Session): Database session
        
        Returns:
            dict: Workflow execution details
        
        Raises:
            HTTPException: For various failure conditions
        """
        try:
            # Check permissions
            EmailController._check_email_permissions(current_user, email_data)

            # Resolve email groups to actual email addresses for all recipient types
            resolved_to_emails = EmailController._resolve_email_groups(db, email_data.to_emails or [])
            resolved_cc = EmailController._resolve_email_groups(db, email_data.cc or [])
            resolved_bcc = EmailController._resolve_email_groups(db, email_data.bcc or [])

            # Ensure at least one recipient is specified
            if not (resolved_to_emails or resolved_cc or resolved_bcc):
                raise HTTPException(
                    status_code=400,
                    detail="At least one recipient (To, CC, or BCC) must be specified"
                )

            client = await get_temporal_client()

            args = {
                "from_email": EMAIL_FROM,
                "to_emails": resolved_to_emails,
                "cc_emails": resolved_cc,
                "bcc_emails": resolved_bcc,
                "subject": email_data.subject,
                "body": email_data.body,
                "test_email": current_user.email,
                "sender": current_user.email,
                "permissions": list(current_user.get_all_permissions())
            }

            args_json = json.dumps(args)
            workflow_id = f"email_{email_data.priority.value}_{int(time.time())}"

            # Create workflow record in database
            workflow = Workflow(workflow_id=workflow_id, uid=current_user.uid)
            db.add(workflow)
            db.commit()

            handle = await client.start_workflow(
                EmailWorkflow,
                args=[args_json, email_data.requires_verification],
                id=workflow_id,
                task_queue=f"email_{email_data.priority.value}_priority"
            )

            return {
                "message": "Email workflow started",
                "workflow_id": workflow_id,
                "requires_verification": email_data.requires_verification,
                "status": "Test email sent" if email_data.requires_verification else "Email sent",
                "sender": current_user.email,
                "resolved_recipients": {
                    "to": resolved_to_emails,
                    "cc": resolved_cc,
                    "bcc": resolved_bcc
                }
            }

        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start email workflow: {str(e)}"
            )

    @staticmethod
    async def handle_confirm_email(
            confirmation: EmailConfirmation,
            current_user: User
    ):
        """
        Handle email confirmation or cancellation in the verification workflow.
        
        Args:
            confirmation (EmailConfirmation): Contains workflow_id and confirmation status
            current_user (User): User performing the confirmation action
        
        Returns:
            dict: Confirmation status including:
                - message: Action result message
                - workflow_id: ID of the affected workflow
                - verified_by: Email of the confirming user
        
        Raises:
            HTTPException: 
                - 403: If user lacks verification permissions
                - 500: If confirmation process fails
        """
        try:
            # Verify user has permission to confirm emails
            if not current_user.has_permission("email:verify"):
                raise HTTPException(
                    status_code=403,
                    detail="Insufficient permissions to verify emails"
                )

            # Get workflow handle and send verification signal
            handle = await get_workflow_handle(confirmation.workflow_id)
            await handle.signal(
                "verify_email",  
                confirmation.confirm
            )

            return {
                "message": "Email confirmed" if confirmation.confirm else "Email cancelled",
                "workflow_id": confirmation.workflow_id,
                "verified_by": current_user.email
            }

        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to confirm email: {str(e)}"
            )

    @staticmethod
    async def handle_check_workflow_status(
            workflow_id: str,
            current_user: User
    ):
        """
        Check the status and details of an email workflow.
        
        Args:
            workflow_id (str): ID of the workflow to check
            current_user (User): User requesting the status check
        
        Returns:
            dict: Comprehensive workflow status including:
                - Basic workflow information (ID, run ID)
                - Current status (RUNNING, COMPLETED, etc.)
                - Timing information (start, close, execution time)
                - Workflow details (type, queue, metrics)
                - History events and counts
                - Priority and timestamp information
        
        Raises:
            HTTPException:
                - 403: If user lacks view permissions
                - 404: If workflow not found
                - 500: For general workflow operation failures
        """
        try:
            # Check basic email permission
            if not current_user.has_permission("email:view"):
                raise HTTPException(
                    status_code=403,
                    detail="Insufficient permissions to view email workflow status"
                )

            handle = await get_workflow_handle(workflow_id)

            try:
                # Get workflow description
                desc = await handle.describe()

                # Get workflow history
                history = await handle.fetch_history()
                history_dict = history.to_json_dict()
                history_events = history_dict.get('events', [])

                # Determine workflow status
                status = "RUNNING"
                if desc.status.name == "COMPLETED":
                    status = "COMPLETED"
                elif desc.status.name == "FAILED":
                    status = "FAILED"
                elif desc.status.name == "CANCELED":
                    status = "CANCELED"
                elif desc.status.name == "TERMINATED":
                    status = "TERMINATED"
                elif desc.status.name == "TIMED_OUT":
                    status = "TIMED_OUT"

                # Extract additional workflow details
                workflow_details = {
                    "workflow_type": desc.workflow_type,
                    "task_queue": desc.task_queue,
                    "execution_duration": getattr(desc.raw_description, "execution_duration", None),
                    "history_size_bytes": getattr(desc.raw_description, "history_size_bytes", None),
                    "state_transition_count": getattr(desc.raw_description, "state_transition_count", None),
                }

                # Parse workflow ID to extract additional information
                priority = None
                timestamp = None
                try:
                    # Format is typically "email_{priority}_{timestamp}"
                    parts = workflow_id.split('_')
                    if len(parts) >= 3 and parts[0] == "email":
                        priority = parts[1]
                        timestamp = int(parts[2])
                except Exception:
                    pass

                return {
                    "workflow_id": workflow_id,
                    "run_id": desc.run_id,
                    "status": status,
                    "start_time": desc.start_time.isoformat() if desc.start_time else None,
                    "close_time": desc.close_time.isoformat() if desc.close_time else None,
                    "execution_time": (
                                desc.close_time - desc.start_time).total_seconds() if desc.close_time and desc.start_time else None,
                    "history_length": desc.history_length,
                    "checked_by": current_user.email,
                    "workflow_details": workflow_details,
                    "priority": priority,
                    "timestamp": timestamp,
                    "history": {
                        "events": history_events,
                        "total_events": len(history_events)
                    }
                }

            except Exception as e:
                # If workflow not found or other error
                raise HTTPException(
                    status_code=404,
                    detail=f"Workflow not found or error retrieving status: {str(e)}"
                )

        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to check workflow status: {str(e)}"
            )


================================================================================

Filename: controllers/storage_controller.py
Content:
"""
Storage Controller Module

Manages file storage operations including:
- File upload and download
- Storage bucket management
- Permission verification
- File metadata handling

Integrates with Supabase for secure file storage and implements
role-based access control for storage operations.
"""

# Standard library imports
import os
import uuid
from typing import List, Optional

# FastAPI and storage imports
from fastapi import HTTPException, UploadFile
from supabase import Client

# Application imports
from configs.database import get_supabase_client
from models.users_model import User


class StorageController:
    """
    Handles all storage-related operations including file management
    and permission verification.
    """

    def __init__(self, bucket_name: str = "default"):
        """
        Initialize storage controller with specified bucket.
        
        Args:
            bucket_name (str): Name of the storage bucket
        """
        self.bucket_name = bucket_name
        self.supabase: Client = get_supabase_client()
        self._ensure_bucket_exists()

    def _ensure_bucket_exists(self):
        """
        Verify storage bucket exists, create if missing.
        
        Raises:
            HTTPException: If bucket creation fails
        """
        try:
            buckets = self.supabase.storage.list_buckets()
            bucket_exists = any(bucket.name == self.bucket_name for bucket in buckets)

            if not bucket_exists:
                self.supabase.storage.create_bucket(self.bucket_name, options={'public': True})
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to initialize bucket: {str(e)}")

    @staticmethod
    def _get_file_extension(filename: str) -> str:
        """
        Extract file extension from filename.
        
        Args:
            filename (str): Original filename
        
        Returns:
            str: File extension including dot
        """
        return os.path.splitext(filename)[1] if filename else ""

    @staticmethod
    def _check_storage_permission(user: User, required_permission: str):
        if not user.has_permission(required_permission):
            raise HTTPException(
                status_code=403,
                detail=f"Insufficient permissions: {required_permission} required"
            )

    async def upload_file(self, file: UploadFile, current_user: User) -> dict:
        """Upload a file to Supabase Storage"""
        self._check_storage_permission(current_user, "storage:write")
        try:
            file_extension = self._get_file_extension(file.filename)
            unique_filename = f"{uuid.uuid4()}{file_extension}"

            file_content = await file.read()

            result = self.supabase.storage.from_(self.bucket_name).upload(
                unique_filename,
                file_content
            )
            file_url = self.supabase.storage.from_(self.bucket_name).get_public_url(unique_filename)

            return {
                "filename": unique_filename,
                "original_filename": file.filename,
                "url": file_url,
                "size": len(file_content),
                "uploaded_by": current_user.email
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to upload file: {str(e)}")

    async def get_file(self, filename: str, current_user: User) -> bytes:
        """Get file content from Supabase Storage"""
        self._check_storage_permission(current_user, "storage:read")
        try:
            return self.supabase.storage.from_(self.bucket_name).download(filename)
        except Exception as e:
            raise HTTPException(status_code=404, detail=f"File not found: {str(e)}")

    async def list_files(self, path: Optional[str] = None, current_user: User = None) -> List[dict]:
        """List files in storage"""
        if current_user is None:
            raise HTTPException(status_code=401, detail="Authentication required")
        self._check_storage_permission(current_user, "storage:read")
        try:
            return self.supabase.storage.from_(self.bucket_name).list(path)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to list files: {str(e)}")

    async def delete_file(self, filename: str, current_user: User) -> dict:
        """Delete a file from storage"""
        self._check_storage_permission(current_user, "storage:delete")
        try:
            self.supabase.storage.from_(self.bucket_name).remove([filename])
            return {
                "message": f"File {filename} deleted successfully",
                "deleted_by": current_user.email
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")

    async def get_image_url(
            self, filename: str, current_user: User,
            width: Optional[int] = None, height: Optional[int] = None,
            ext: Optional[str] = "", quality: Optional[int] = 100
    ) -> str:
        """Get transformed image URL"""
        self._check_storage_permission(current_user, "storage:read")
        try:
            transform = {
                "resize": "cover",
                **({"width": width} if width else {}),
                **({"height": height} if height else {}),
                **({"format": ext} if ext else {}),
                **({"quality": quality} if quality else {})
            }

            return self.supabase.storage.from_(self.bucket_name).get_public_url(
                filename,
                {"transform": transform}
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to get image URL: {str(e)}")


================================================================================

Filename: controllers/tab_controller.py
Content:
import uuid
from fastapi import HTTPException, status
from sqlalchemy.orm import Session
from models.tab_model import Tab
from models.users_model import User
from schemas.tab_schema import TabCreate, TabUpdate


async def create_tab(tab_data: TabCreate, db: Session, current_user: User):
    """Create a new tab associated with the current user."""
    new_tab = Tab(**tab_data.model_dump(), uid=current_user.uid)
    db.add(new_tab)
    db.commit()
    db.refresh(new_tab)
    return new_tab


async def get_tabs(db: Session, current_user: User, skip: int = 0, limit: int = 100):
    """Retrieve tabs associated with the current user."""
    return db.query(Tab).filter(Tab.uid == current_user.uid).offset(skip).limit(limit).all()


async def get_tab(tab_id: uuid.UUID, db: Session, current_user: User):
    """Retrieve a specific tab by its ID, checking ownership."""
    tab = db.query(Tab).filter(Tab.tab_id == tab_id).first()
    if not tab:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Tab not found"
        )
    if tab.uid != current_user.uid:
        # Optionally, check for admin/view-all permissions here if needed
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to access this tab"
        )
    return tab


async def update_tab(tab_id: uuid.UUID, tab_data: TabUpdate, db: Session, current_user: User):
    """Update a specific tab, checking ownership."""
    tab = await get_tab(tab_id, db, current_user) # Reuse get_tab for fetch and ownership check
    
    update_data = tab_data.model_dump(exclude_unset=True)
    for key, value in update_data.items():
        setattr(tab, key, value)
        
    db.commit()
    db.refresh(tab)
    return tab


async def delete_tab(tab_id: uuid.UUID, db: Session, current_user: User):
    """Delete a specific tab, checking ownership."""
    tab = await get_tab(tab_id, db, current_user) # Reuse get_tab for fetch and ownership check
    
    db.delete(tab)
    db.commit()
    return {"message": "Tab deleted successfully"} 

================================================================================

Filename: controllers/user_controller.py
Content:
"""
User Controller Module

Manages user-related operations including:
- User creation and updates
- Role assignment
- User lookup and listing
- Permission management

Implements user management with role-based access control
and integrates with the authentication system.
"""

# Standard library imports
import uuid

# FastAPI and database imports
from fastapi import HTTPException
from sqlalchemy.orm import Session
from starlette import status

# Application imports
from models.users_model import User
from models.rbac_model import Role
from schemas.user_schema import UserCreate, UserUpdate, UserRoleUpdate


async def create_user(user_data: UserCreate, db: Session):
    """
    Create a new user with specified roles.
    
    Args:
        user_data (UserCreate): User creation data
        db (Session): Database session
    
    Returns:
        dict: Created user information
    
    Raises:
        HTTPException: If user exists or role assignment fails
    """
    db_user = db.query(User).filter(User.email == user_data.email).first()
    if db_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email already registered"
        )

    new_user = User(
        uid=str(uuid.uuid4()),
        email=str(user_data.email),
        UserName=user_data.UserName,
        profile_image_url=user_data.profile_image_url
    )

    # Assign roles
    if user_data.role_ids:
        roles = db.query(Role).filter(Role.id.in_(user_data.role_ids)).all()
        if len(roles) != len(user_data.role_ids):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="One or more role IDs are invalid"
            )
        new_user.roles = roles
    else:
        # Assign default role
        default_role = db.query(Role).filter(Role.name == user_data.default_role).first()
        if not default_role:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Default role not found"
            )
        new_user.roles = [default_role]

    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    return new_user.to_response()


async def get_users(db: Session, skip: int = 0, limit: int = 100):
    return db.query(User).offset(skip).limit(limit).all()


async def get_user(email: str, db: Session):
    user = db.query(User).filter(User.email == email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    return user.to_response()


async def update_user(email: str, user_data: UserUpdate, db: Session):
    user = db.query(User).filter(User.email == email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )

    update_data = user_data.model_dump(exclude_unset=True)
    for key, value in update_data.items():
        if key != 'role_ids':  # Handle roles separately
            setattr(user, key, value)

    if user_data.role_ids is not None:
        roles = db.query(Role).filter(Role.id.in_(user_data.role_ids)).all()
        if len(roles) != len(user_data.role_ids):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="One or more role IDs are invalid"
            )
        user.roles = roles

    db.commit()
    db.refresh(user)
    return user.to_response()


async def update_user_roles(db: Session, role_update: UserRoleUpdate, email: str):
    user = db.query(User).filter(User.email == email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )

    # Add new roles
    if role_update.add_roles:
        new_roles = db.query(Role).filter(Role.name.in_(role_update.add_roles)).all()
        if len(new_roles) != len(role_update.add_roles):
            invalid_roles = set(role_update.add_roles) - {role.name for role in new_roles}
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid role names: {', '.join(invalid_roles)}"
            )
        user.roles.extend([role for role in new_roles if role not in user.roles])

    # Remove roles
    if role_update.remove_roles:
        user.roles = [role for role in user.roles if role.name not in role_update.remove_roles]

    db.commit()
    db.refresh(user)
    return user.to_response()


async def delete_user(email: str, db: Session):
    user = db.query(User).filter(User.email == email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )

    db.delete(user)
    db.commit()
    return {"message": "User deleted successfully"}

================================================================================

Filename: controllers/workflow_controller.py
Content:
"""
Workflow Controller Module

Manages workflow operations including:
- Workflow status checking
- Workflow termination
- Workflow history tracking
- User-specific workflow management

Integrates with Temporal for workflow orchestration and implements
role-based access control for workflow operations.
"""

# FastAPI and database imports
from fastapi import HTTPException
from sqlalchemy.orm import Session

# Application imports
from configs.temporal_config import get_workflow_handle
from models.users_model import User
from models.workflow_model import Workflow


class WorkflowController:
    """
    Handles all workflow-related operations including status checking
    and workflow management.
    """

    @staticmethod
    async def get_workflows(db: Session, user: User):
        """
        Retrieve all workflows for a specific user.
        
        Args:
            db (Session): Database session
            user (User): User whose workflows to retrieve
        
        Returns:
            list[Workflow]: List of user's workflows
        """
        return db.query(Workflow).filter(Workflow.uid == user.uid).all()

    @staticmethod
    async def terminate_workflow(workflow_id: str, user: User, db: Session):
        """
        Terminate a running workflow and clean up associated database records.
        
        Args:
            workflow_id (str): Unique identifier of the workflow to terminate
            user (User): User requesting the termination
            db (Session): Database session for workflow record management
        
        Returns:
            dict: Confirmation message with workflow ID
            
        Raises:
            HTTPException: 
                - 404: If workflow not found or user lacks access
                - 500: If termination operation fails
        """
        # Verify workflow exists and belongs to user
        workflow = db.query(Workflow).filter(
            Workflow.workflow_id == workflow_id,
            Workflow.uid == user.uid
        ).first()

        if not workflow:
            raise HTTPException(
                status_code=404,
                detail="Workflow not found or access denied"
            )

        try:
            # Get workflow handle and terminate execution
            handle = await get_workflow_handle(workflow_id)
            await handle.terminate("Terminated by user request")

            # Clean up the workflow record
            db.delete(workflow)
            db.commit()

            return {"message": f"Workflow {workflow_id} terminated successfully"}
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to terminate workflow: {str(e)}"
            )

    @staticmethod
    async def check_workflow_status(workflow_id: str, current_user: User):
        """
        Retrieve detailed status and history of a specific workflow.
        
        Args:
            workflow_id (str): Unique identifier of the workflow to check
            current_user (User): User requesting the status check
        
        Returns:
            dict: Comprehensive workflow status including:
                - Basic workflow information (ID, run ID)
                - Temporal status (RUNNING, COMPLETED, etc.)
                - Timing information (start, close, execution time)
                - Workflow details (type, queue, metrics)
                - History events
                - Priority and timestamp (for email workflows)
        
        Raises:
            HTTPException:
                - 404: If workflow not found or status retrieval fails
                - 500: For general workflow operation failures
        """
        try:
            # Get workflow handle from Temporal
            handle = await get_workflow_handle(workflow_id)

            try:
                # Retrieve workflow description and history
                desc = await handle.describe()
                history = await handle.fetch_history()
                history_dict = history.to_json_dict()
                history_events = history_dict.get('events', [])

                # Map Temporal workflow status to application status
                status = "RUNNING"
                if desc.status.name == "COMPLETED":
                    status = "COMPLETED"
                elif desc.status.name == "FAILED":
                    status = "FAILED"
                elif desc.status.name == "CANCELED":
                    status = "CANCELED"
                elif desc.status.name == "TERMINATED":
                    status = "TERMINATED"
                elif desc.status.name == "TIMED_OUT":
                    status = "TIMED_OUT"

                # Collect detailed workflow metrics and information
                workflow_details = {
                    "workflow_type": desc.workflow_type,
                    "task_queue": desc.task_queue,
                    "execution_duration": getattr(desc.raw_description, "execution_duration", None),
                    "history_size_bytes": getattr(desc.raw_description, "history_size_bytes", None),
                    "state_transition_count": getattr(desc.raw_description, "state_transition_count", None),
                }

                # Extract priority and timestamp from workflow ID
                priority = None
                timestamp = None
                try:
                    # Parse workflow ID format: "email_{priority}_{timestamp}"
                    parts = workflow_id.split('_')
                    if len(parts) >= 3 and parts[0] == "email":
                        priority = parts[1]
                        timestamp = int(parts[2])

                except Exception:
                    # Silently handle parsing errors
                    pass

                # Compile comprehensive workflow status response
                return {
                    "workflow_id": workflow_id,
                    "run_id": desc.run_id,
                    "status": status,
                    "start_time": desc.start_time.isoformat() if desc.start_time else None,
                    "close_time": desc.close_time.isoformat() if desc.close_time else None,
                    "execution_time": (
                                desc.close_time - desc.start_time).total_seconds() if desc.close_time and desc.start_time else None,
                    "history_length": desc.history_length,
                    "checked_by": current_user.email,
                    "workflow_details": workflow_details,
                    "priority": priority,
                    "timestamp": timestamp,
                    "history": {
                        "events": history_events,
                        "total_events": len(history_events)
                    }
                }

            except Exception as e:
                raise HTTPException(
                    status_code=404,
                    detail=f"Workflow not found or error retrieving status: {str(e)}"
                )
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to check workflow status: {str(e)}"
            )


================================================================================

Filename: docker/.env.example
Content:
############
# Secrets
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=your-super-secret-jwt-token-with-at-least-32-characters-long
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q
DASHBOARD_USERNAME=supabase
DASHBOARD_PASSWORD=this_password_is_insecure_and_should_be_updated

############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_PORT=5432
# default user is postgres

############
# Supavisor -- Database pooler
############
POOLER_PROXY_PORT_TRANSACTION=6543
POOLER_DEFAULT_POOL_SIZE=20
POOLER_MAX_CLIENT_CONN=100
POOLER_TENANT_ID=your-tenant-id


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION="/auth/v1/verify"
MAILER_URLPATHS_INVITE="/auth/v1/verify"
MAILER_URLPATHS_RECOVERY="/auth/v1/verify"
MAILER_URLPATHS_EMAIL_CHANGE="/auth/v1/verify"

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=supabase-mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender
ENABLE_ANONYMOUS_USERS=false

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_DEFAULT_ORGANIZATION=Default Organization
STUDIO_DEFAULT_PROJECT=Default Project

STUDIO_PORT=3000
# replace if you intend to use Studio outside of localhost
SUPABASE_PUBLIC_URL=http://localhost:8000

# Enable webp support
IMGPROXY_ENABLE_WEBP_DETECTION=true

# Add your OpenAI API key to enable SQL Editor Assistant
OPENAI_API_KEY=

############
# Functions - Configuration for Functions
############
# NOTE: VERIFY_JWT applies to all functions. Per-function VERIFY_JWT is not supported yet.
FUNCTIONS_VERIFY_JWT=false

############
# Logs - Configuration for Logflare
# Please refer to https://supabase.com/docs/reference/self-hosting-analytics/introduction
############

LOGFLARE_LOGGER_BACKEND_API_KEY=your-super-secret-and-long-logflare-key

# Change vector.toml sinks to reflect this change
LOGFLARE_API_KEY=your-super-secret-and-long-logflare-key

# Docker socket location - this value will differ depending on your OS
DOCKER_SOCKET_LOCATION=/var/run/docker.sock

# Google Cloud Project details
GOOGLE_PROJECT_ID=GOOGLE_PROJECT_ID
GOOGLE_PROJECT_NUMBER=GOOGLE_PROJECT_NUMBER




############################################
# Application - configs for application
# These values should also be copied to .env in the root of the project for the application to work
# This is because the application uses these values to connect to the database
# Also copy the ANON_KEY and SERVICE_ROLE_KEY to the .env file in the root of the project
############################################
# These values should be set same to the respective values above
DB_USER=postgres
DB_PASSWORD=your-super-secret-and-long-postgres-password
# This is the port can be 5432
DB_PORT=6543
DB_HOST=supavisor
DB_NAME=postgres
TENANT_ID=your-tenant-id
SECRET_KEY=super_secret_key
SMTP_USERNAME=fake_mail_user
SMTP_PASSWORD=fake_mail_password
SMTP_PORT=587
SMTP_HOST=fake.smtp.com
# Zoho OAuth Configuration
ZOHO_CLIENT_ID=your_zoho_client_id
ZOHO_CLIENT_SECRET=your_zoho_client_secret
ZOHO_REDIRECT_URI=your_redirect_uri
ZOHO_TOKEN_URL=https://accounts.zoho.in/oauth/v2/token


================================================================================

Filename: docker/.gitignore
Content:
volumes/db/data
volumes/storage
.env
test.http
docker-compose.override.yml

================================================================================

Filename: docker/Dockerfile
Content:
# Base Image
FROM python:3.12-slim

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the project files
COPY . .

# Expose application port
EXPOSE 8123

# Command to run the application
CMD ["uvicorn", "run_api:app", "--host", "0.0.0.0", "--port", "8123"]


================================================================================

Filename: docker/Dockerfile_worker
Content:
# Base Image
FROM python:3.12-slim

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the project files
COPY . .
#

# Command to run the worker
CMD ["python", "run_worker.py"]

================================================================================

Filename: docker/[old]docker-compose-temporal.yml
Content:
services:
  temporal-postgres:
    image: postgres:13
    container_name: temporal-postgres
    environment:
      POSTGRES_USER: temporal
      POSTGRES_PASSWORD: temporal
    ports:
      - ":5432"
    networks:
      - temporal-network
    volumes:
      - temporal-postgres-data:/var/lib/postgres/data

  temporal:
    image: temporalio/auto-setup:latest
    container_name: temporal
    ports:
      - "7233:7233"
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-postgres
      - TEMPORAL_CLI_ADDRESS=temporal:7233
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development-sql.yaml
    depends_on:
      - temporal-postgres
    networks:
      - temporal-network
    volumes:
      - ./dynamicconfig:/etc/temporal/config/dynamicconfig

  temporal-admin-tools:
    image: temporalio/admin-tools:latest
    container_name: temporal-admin-tools
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
    depends_on:
      - temporal
    networks:
      - temporal-network
    stdin_open: true
    tty: true

  temporal-ui:
    image: temporalio/ui:latest
    container_name: temporal-ui
    ports:
      - "8080:8080" # Temporal Web UI
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    depends_on:
      - temporal
    networks:
      - temporal-network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090" # Prometheus Web UI
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - temporal-network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000" # Grafana Web UI
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - temporal-network

volumes:
  temporal-postgres-data:

networks:
  temporal-network:
    driver: bridge
    name: temporal-network


================================================================================

Filename: docker/docker-compose.yml
Content:
name: "alert.ventory"

services:
  api:
    container_name: fastapi-app
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8123:8123"
    environment:
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - TENANT_ID=${TENANT_ID}
      - SECRET_KEY=${SECRET_KEY}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_PORT=${SMTP_PORT}
      - SMTP_HOST=${SMTP_HOST}
      - ZOHO_CLIENT_ID=${ZOHO_CLIENT_ID}
      - ZOHO_CLIENT_SECRET=${ZOHO_CLIENT_SECRET}
      - ZOHO_REDIRECT_URI=${ZOHO_REDIRECT_URI}
      - ZOHO_TOKEN_URL=${ZOHO_TOKEN_URL}

    depends_on:
      db:
        condition: service_healthy
      supavisor:
        condition: service_healthy
      temporal:
        condition: service_started
      kong:
        condition: service_started
      storage:
        condition: service_healthy
    networks:
      - supabase-network
      - temporal-network

  worker:
    container_name: temporal-worker
    build:
      context: ..
      dockerfile: docker/Dockerfile_worker
    environment:
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - TENANT_ID=${TENANT_ID}
      - SECRET_KEY=${SECRET_KEY}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_PORT=${SMTP_PORT}
      - SMTP_HOST=${SMTP_HOST}
      - ZOHO_CLIENT_ID=${ZOHO_CLIENT_ID}
      - ZOHO_CLIENT_SECRET=${ZOHO_CLIENT_SECRET}
      - ZOHO_REDIRECT_URI=${ZOHO_REDIRECT_URI}
      - ZOHO_TOKEN_URL=${ZOHO_TOKEN_URL}
    depends_on:
      db:
        condition: service_healthy
      supavisor:
        condition: service_healthy
      temporal:
        condition: service_started
      kong:
        condition: service_started
      storage:
        condition: service_healthy
    networks:
      - supabase-network
      - temporal-network



  #supabase:
  studio:
    container_name: supabase-studio
    image: supabase/studio:20241202-71e5240
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "fetch('http://studio:3000/api/profile').then((r) => {if (r.status !== 200) throw new Error(r.status)})"
        ]
      timeout: 10s
      interval: 5s
      retries: 3
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      STUDIO_PG_META_URL: http://meta:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}

      DEFAULT_ORGANIZATION_NAME: ${STUDIO_DEFAULT_ORGANIZATION}
      DEFAULT_PROJECT_NAME: ${STUDIO_DEFAULT_PROJECT}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      SUPABASE_URL: http://kong:8000
      SUPABASE_PUBLIC_URL: ${SUPABASE_PUBLIC_URL}
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      AUTH_JWT_SECRET: ${JWT_SECRET}

      LOGFLARE_API_KEY: ${LOGFLARE_API_KEY}
      LOGFLARE_URL: http://analytics:4000
      NEXT_PUBLIC_ENABLE_LOGS: true
      # Comment to use Big Query backend for analytics
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
      # Uncomment to use Big Query backend for analytics
      # NEXT_ANALYTICS_BACKEND_PROVIDER: bigquery
    networks:
      - supabase-network

  kong:
    container_name: supabase-kong
    image: kong:2.8.1
    restart: unless-stopped
    # https://unix.stackexchange.com/a/294837
    entrypoint: bash -c 'eval "echo \"$$(cat ~/temp.yml)\"" > ~/kong.yml && /docker-entrypoint.sh kong docker-start'
    ports:
      - ${KONG_HTTP_PORT}:8000/tcp
      - ${KONG_HTTPS_PORT}:8443/tcp
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      # https://github.com/supabase/cli/issues/14
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      DASHBOARD_USERNAME: ${DASHBOARD_USERNAME}
      DASHBOARD_PASSWORD: ${DASHBOARD_PASSWORD}
    volumes:
      # https://github.com/supabase/supabase/issues/12661
      - ./volumes/api/kong.yml:/home/kong/temp.yml:ro
    networks:
      - supabase-network

  auth:
    container_name: supabase-auth
    image: supabase/gotrue:v2.164.0
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    restart: unless-stopped
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: ${API_EXTERNAL_URL}

      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

      GOTRUE_SITE_URL: ${SITE_URL}
      GOTRUE_URI_ALLOW_LIST: ${ADDITIONAL_REDIRECT_URLS}
      GOTRUE_DISABLE_SIGNUP: ${DISABLE_SIGNUP}

      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: ${JWT_EXPIRY}
      GOTRUE_JWT_SECRET: ${JWT_SECRET}

      GOTRUE_EXTERNAL_EMAIL_ENABLED: ${ENABLE_EMAIL_SIGNUP}
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: ${ENABLE_ANONYMOUS_USERS}
      GOTRUE_MAILER_AUTOCONFIRM: ${ENABLE_EMAIL_AUTOCONFIRM}

      # Uncomment to bypass nonce check in ID Token flow. Commonly set to true when using Google Sign In on mobile.
      # GOTRUE_EXTERNAL_SKIP_NONCE_CHECK: true

      # GOTRUE_MAILER_SECURE_EMAIL_CHANGE_ENABLED: true
      # GOTRUE_SMTP_MAX_FREQUENCY: 1s
      GOTRUE_SMTP_ADMIN_EMAIL: ${SMTP_ADMIN_EMAIL}
      GOTRUE_SMTP_HOST: ${SMTP_HOST}
      GOTRUE_SMTP_PORT: ${SMTP_PORT}
      GOTRUE_SMTP_USER: ${SMTP_USER}
      GOTRUE_SMTP_PASS: ${SMTP_PASS}
      GOTRUE_SMTP_SENDER_NAME: ${SMTP_SENDER_NAME}
      GOTRUE_MAILER_URLPATHS_INVITE: ${MAILER_URLPATHS_INVITE}
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: ${MAILER_URLPATHS_CONFIRMATION}
      GOTRUE_MAILER_URLPATHS_RECOVERY: ${MAILER_URLPATHS_RECOVERY}
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: ${MAILER_URLPATHS_EMAIL_CHANGE}

      GOTRUE_EXTERNAL_PHONE_ENABLED: ${ENABLE_PHONE_SIGNUP}
      GOTRUE_SMS_AUTOCONFIRM: ${ENABLE_PHONE_AUTOCONFIRM}
      # Uncomment to enable custom access token hook. Please see: https://supabase.com/docs/guides/auth/auth-hooks for full list of hooks and additional details about custom_access_token_hook

      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_ENABLED: "true"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_SECRETS: "<standard-base64-secret>"

      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/mfa_verification_attempt"

      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/password_verification_attempt"

      # GOTRUE_HOOK_SEND_SMS_ENABLED: "false"
      # GOTRUE_HOOK_SEND_SMS_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_SEND_SMS_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"

      # GOTRUE_HOOK_SEND_EMAIL_ENABLED: "false"
      # GOTRUE_HOOK_SEND_EMAIL_URI: "http://host.docker.internal:54321/functions/v1/email_sender"
      # GOTRUE_HOOK_SEND_EMAIL_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"
    networks:
      - supabase-network
  rest:
    container_name: supabase-rest
    image: postgrest/postgrest:v12.2.0
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    restart: unless-stopped
    environment:
      PGRST_DB_URI: postgres://authenticator:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      PGRST_DB_SCHEMAS: ${PGRST_DB_SCHEMAS}
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${JWT_SECRET}
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_APP_SETTINGS_JWT_SECRET: ${JWT_SECRET}
      PGRST_APP_SETTINGS_JWT_EXP: ${JWT_EXPIRY}
    command: "postgrest"
    networks:
      - supabase-network
  realtime:
    # This container name looks inconsistent but is correct because realtime constructs tenant id by parsing the subdomain
    container_name: realtime-dev.supabase-realtime
    image: supabase/realtime:v2.33.70
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "-H",
          "Authorization: Bearer ${ANON_KEY}",
          "http://localhost:4000/api/tenants/realtime-dev/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    restart: unless-stopped
    environment:
      PORT: 4000
      DB_HOST: ${POSTGRES_HOST}
      DB_PORT: ${POSTGRES_PORT}
      DB_USER: supabase_admin
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: ${POSTGRES_DB}
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      API_JWT_SECRET: ${JWT_SECRET}
      SECRET_KEY_BASE: UpNVntn3cDxHJpq99YMc1T1AQgQpc8kfYTuRgBiYa15BLrx8etQoXz3gZv1/u2oq
      ERL_AFLAGS: -proto_dist inet_tcp
      DNS_NODES: "''"
      RLIMIT_NOFILE: "10000"
      APP_NAME: realtime
      SEED_SELF_HOST: true
      RUN_JANITOR: true
    networks:
      - supabase-network
  # To use S3 backed storage: docker compose -f docker-compose.yml -f docker-compose.s3.yml up
  storage:
    container_name: supabase-storage
    image: supabase/storage-api:v1.11.13
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      rest:
        condition: service_started
      imgproxy:
        condition: service_started
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://storage:5000/status"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    restart: unless-stopped
    environment:
      ANON_KEY: ${ANON_KEY}
      SERVICE_KEY: ${SERVICE_ROLE_KEY}
      POSTGREST_URL: http://rest:3000
      PGRST_JWT_SECRET: ${JWT_SECRET}
      DATABASE_URL: postgres://supabase_storage_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub

      REGION: stub
      GLOBAL_S3_BUCKET: stub
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy:5001
    volumes:
      - ./volumes/storage:/var/lib/storage:z
    networks:
      - supabase-network
  imgproxy:
    container_name: supabase-imgproxy
    image: darthsim/imgproxy:v3.8.0
    healthcheck:
      test: [ "CMD", "imgproxy", "health" ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: ${IMGPROXY_ENABLE_WEBP_DETECTION}
    volumes:
      - ./volumes/storage:/var/lib/storage:z
    networks:
      - supabase-network
  meta:
    container_name: supabase-meta
    image: supabase/postgres-meta:v0.84.2
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    restart: unless-stopped
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: ${POSTGRES_HOST}
      PG_META_DB_PORT: ${POSTGRES_PORT}
      PG_META_DB_NAME: ${POSTGRES_DB}
      PG_META_DB_USER: supabase_admin
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}
    networks:
      - supabase-network
  functions:
    container_name: supabase-edge-functions
    image: supabase/edge-runtime:v1.65.3
    restart: unless-stopped
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      JWT_SECRET: ${JWT_SECRET}
      SUPABASE_URL: http://kong:8000
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_ROLE_KEY: ${SERVICE_ROLE_KEY}
      SUPABASE_DB_URL: postgresql://postgres:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      VERIFY_JWT: "${FUNCTIONS_VERIFY_JWT}"
    volumes:
      - ./volumes/functions:/home/deno/functions:Z
    command:
      - start
      - --main-service
      - /home/deno/functions/main
    networks:
      - supabase-network
  analytics:
    container_name: supabase-analytics
    image: supabase/logflare:1.4.0
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:4000/health" ]
      timeout: 5s
      interval: 5s
      retries: 10
    restart: unless-stopped
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
    # Uncomment to use Big Query backend for analytics
    # volumes:
    #   - type: bind
    #     source: ${PWD}/gcloud.json
    #     target: /opt/app/rel/logflare/bin/gcloud.json
    #     read_only: true
    environment:
      LOGFLARE_NODE_HOST: 127.0.0.1
      DB_USERNAME: supabase_admin
      DB_DATABASE: _supabase
      DB_HOSTNAME: ${POSTGRES_HOST}
      DB_PORT: ${POSTGRES_PORT}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_SCHEMA: _analytics
      LOGFLARE_API_KEY: ${LOGFLARE_API_KEY}
      LOGFLARE_SINGLE_TENANT: true
      LOGFLARE_SUPABASE_MODE: true
      LOGFLARE_MIN_CLUSTER_SIZE: 1

      # Comment variables to use Big Query backend for analytics
      POSTGRES_BACKEND_URL: postgresql://supabase_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/_supabase
      POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
      # Uncomment to use Big Query backend for analytics
      # GOOGLE_PROJECT_ID: ${GOOGLE_PROJECT_ID}
      # GOOGLE_PROJECT_NUMBER: ${GOOGLE_PROJECT_NUMBER}
    ports:
      - 4000:4000
    networks:
      - supabase-network
  # Comment out everything below this point if you are using an external Postgres database
  db:
    container_name: supabase-db
    image: supabase/postgres:15.6.1.146
    healthcheck:
      test: pg_isready -U postgres -h localhost
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      vector:
        condition: service_healthy
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=fatal # prevents Realtime polling queries from appearing in logs
    restart: unless-stopped
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXP: ${JWT_EXPIRY}
    volumes:
      - ./volumes/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      # Must be superuser to create event trigger
      - ./volumes/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      # Must be superuser to alter reserved role
      - ./volumes/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # Initialize the database settings with JWT_SECRET and JWT_EXP
      - ./volumes/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      # PGDATA directory is persisted between restarts
      - ./volumes/db/data:/var/lib/postgresql/data:Z
      # Changes required for internal supabase data such as _analytics
      - ./volumes/db/_supabase.sql:/docker-entrypoint-initdb.d/migrations/97-_supabase.sql:Z
      # Changes required for Analytics support
      - ./volumes/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      # Changes required for Pooler support
      - ./volumes/db/pooler.sql:/docker-entrypoint-initdb.d/migrations/99-pooler.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - db-config:/etc/postgresql-custom
    networks:
      - supabase-network
  vector:
    container_name: supabase-vector
    image: timberio/vector:0.28.1-alpine
    healthcheck:
      test:
        [

          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://vector:9001/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    volumes:
      - ./volumes/logs/vector.yml:/etc/vector/vector.yml:ro
      - ${DOCKER_SOCKET_LOCATION}:/var/run/docker.sock:ro
    environment:
      LOGFLARE_API_KEY: ${LOGFLARE_API_KEY}
    command: [ "--config", "/etc/vector/vector.yml" ]
    networks:
      - supabase-network
  # Update the DATABASE_URL if you are using an external Postgres database
  supavisor:
    container_name: supabase-pooler
    image: supabase/supavisor:1.1.56
    healthcheck:
      test: curl -sSfL --head -o /dev/null "http://127.0.0.1:4000/api/health"
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    command:
      - /bin/sh
      - -c
      - /app/bin/migrate && /app/bin/supavisor eval "$$(cat /etc/pooler/pooler.exs)" && /app/bin/server
    restart: unless-stopped
    ports:
      - ${POSTGRES_PORT}:5432
      - ${POOLER_PROXY_PORT_TRANSACTION}:6543
    environment:
      - PORT=4000
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - DATABASE_URL=ecto://supabase_admin:${POSTGRES_PASSWORD}@db:${POSTGRES_PORT}/_supabase
      - CLUSTER_POSTGRES=true
      - SECRET_KEY_BASE=UpNVntn3cDxHJpq99YMc1T1AQgQpc8kfYTuRgBiYa15BLrx8etQoXz3gZv1/u2oq
      - VAULT_ENC_KEY=your-encryption-key-32-chars-min
      - API_JWT_SECRET=${JWT_SECRET}
      - METRICS_JWT_SECRET=${JWT_SECRET}
      - REGION=local
      - ERL_AFLAGS=-proto_dist inet_tcp
      - POOLER_TENANT_ID=${POOLER_TENANT_ID}
      - POOLER_DEFAULT_POOL_SIZE=${POOLER_DEFAULT_POOL_SIZE}
      - POOLER_MAX_CLIENT_CONN=${POOLER_MAX_CLIENT_CONN}
      - POOLER_POOL_MODE=transaction
    volumes:
      - ./volumes/pooler/pooler.exs:/etc/pooler/pooler.exs:ro
    networks:
      - supabase-network
  # temporal:
  temporal-postgres:
    image: postgres:13
    container_name: temporal-postgres
    environment:
      POSTGRES_USER: temporal
      POSTGRES_PASSWORD: temporal
    restart: unless-stopped
    ports:
      - "5433:5432"  # Changed from 5432 to avoid conflict with Supabase Postgres
    networks:
      - temporal-network
    volumes:
      - temporal-postgres-data:/var/lib/postgresql/data

  temporal:
    image: temporalio/auto-setup:latest
    container_name: temporal
    ports:
      - "7233:7233"
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-postgres
      - TEMPORAL_CLI_ADDRESS=temporal:7233
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development-sql.yaml
    restart: unless-stopped
    depends_on:
      - temporal-postgres
    networks:
      - temporal-network
    volumes:
      - ./dynamicconfig:/etc/temporal/config/dynamicconfig

  temporal-admin-tools:
    image: temporalio/admin-tools:latest
    container_name: temporal-admin-tools
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
    restart: unless-stopped
    depends_on:
      - temporal
    networks:
      - temporal-network
    stdin_open: true
    tty: true

  temporal-ui:
    image: temporalio/ui:latest
    container_name: temporal-ui
    ports:
      - "8088:8080"  # Changed from 8080 to avoid potential conflicts
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    restart: unless-stopped
    depends_on:
      - temporal
    networks:
      - temporal-network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    restart: unless-stopped
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - temporal-network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"  # Changed from 3000 to avoid conflicts
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - temporal-network



volumes:
  db-config:
  temporal-postgres-data:

networks:
  supabase-network:
    driver: bridge
  temporal-network:
    driver: bridge



================================================================================

Filename: docker/dynamicconfig/README.md
Content:
Use `docker.yaml` file to override the default dynamic config value (they are specified
when creating the service config).

Each key can have zero or more values and each value can have zero or more
constraints. There are only three types of constraint:
1. `namespace`: `string`
2. `taskQueueName`: `string`
3. `taskType`: `int` (`1`:`Workflow`, `2`:`Activity`)
A value will be selected and returned if all its has exactly the same constraints
as the ones specified in query filters (including the number of constraints).

Please use the following format:
```
testGetBoolPropertyKey:
  - value: false
  - value: true
    constraints:
      namespace: "global-samples-namespace"
  - value: false
    constraints:
      namespace: "samples-namespace"
testGetDurationPropertyKey:
  - value: "1m"
    constraints:
      namespace: "samples-namespace"
      taskQueueName: "longIdleTimeTaskqueue"
testGetFloat64PropertyKey:
  - value: 12.0
    constraints:
      namespace: "samples-namespace"
testGetMapPropertyKey:
  - value:
      key1: 1
      key2: "value 2"
      key3:
        - false
        - key4: true
          key5: 2.0
```

================================================================================

Filename: docker/dynamicconfig/development-cass.yaml
Content:
system.forceSearchAttributesCacheRefreshOnRead:
  - value: true # Dev setup only. Please don't turn this on in production.
    constraints: {}

================================================================================

Filename: docker/dynamicconfig/development-sql.yaml
Content:
limit.maxIDLength:
  - value: 255
    constraints: {}
system.forceSearchAttributesCacheRefreshOnRead:
  - value: true # Dev setup only. Please don't turn this on in production.
    constraints: {}

================================================================================

Filename: docker/dynamicconfig/docker.yaml
Content:


================================================================================

Filename: docker/prometheus/prometheus.yml
Content:
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "temporal"
    static_configs:
      - targets: ["temporal:7233"]
  - job_name: "temporal-ui"
    static_configs:
      - targets: ["temporal-web:8080"]


================================================================================

Filename: docker/readme.md
Content:
# Docker Configuration

This folder contains Docker configurations for running the Alert.ventory application stack, which includes Supabase, Temporal, and custom services.

## Structure

```
docker/
âââ .env                # Env file
âââ docker-compose.yml    # Main compose file for all services
âââ Dockerfile           # Custom application Dockerfile
âââ prometheus/         # Prometheus configuration
â   âââ prometheus.yml
âââ dynamicconfig/      # Temporal dynamic config
âââ volumes/           # Persistent data volumes
    âââ db/           # Database initialization scripts
    âââ storage/      # Storage service data
    âââ functions/    # Edge functions
    âââ pooler/       # Connection pooler config
    âââ logs/         # Logging configuration
```

## Services

### Core Application
- **API Service**: FastAPI application running on port 8123
- **Database**: PostgreSQL database managed by Supabase
- **Temporal Workflow Engine**: For managing distributed workflows and tasks

### Supabase Services
- **Studio**: Supabase admin interface
- **Kong**: API Gateway
- **Auth**: Authentication service
- **REST**: PostgREST API
- **Realtime**: Real-time subscriptions
- **Storage**: File storage service
- **Meta**: Database management
- **Edge Functions**: Serverless functions
- **Analytics**: Log management
- **Connection Pooler**: Database connection pooling

### Temporal Stack
- **Temporal Server**: Main workflow engine
- **Temporal UI**: Web interface (port 8088)
- **Temporal PostgreSQL**: Dedicated database (port 5433)
- **Admin Tools**: CLI tools for Temporal management

### Monitoring
- **Prometheus**: Metrics collection (port 9090)
- **Grafana**: Metrics visualization (port 3001)

## Configuration

### Environment Variables

Create a `.env` file in the root directory with the necessary variables:


## Usage

### Starting the Stack
Start all services:
```bash
docker compose up -d --build
```

### Stopping the Stack

```bash
docker compose down
```

To remove volumes:
```bash
docker compose down -v
```

### Viewing Logs

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f api
```

### Service Access

- FastAPI Application: http://localhost:8123
- Supabase Studio: http://localhost:3000
- Temporal UI: http://localhost:8088
- Grafana: http://localhost:3001
- Prometheus: http://localhost:9090

## Maintenance

### Database Backups

The PostgreSQL data is persisted in `./volumes/db/data`. Regular backups can be configured based on requirements.

### Updating Services

1. Pull the latest images:
```bash
docker compose pull
```

2. Rebuild custom services:
```bash
docker compose build --no-cache api
```

3. Restart the stack:
```bash
docker compose down && docker compose up -d
```

## Troubleshooting

### Common Issues

1. **Database Connection Issues**
   - Verify PostgreSQL is healthy: `docker compose ps db`
   - Check database logs: `docker compose logs db`
   - Ensure correct environment variables in `.env`

2. **Service Dependencies**
   - Services are configured with health checks and proper startup order
   - If services fail to start, check the logs for dependency issues

3. **Port Conflicts**
   - Ensure no other services are using the configured ports
   - Ports can be modified in docker-compose.yml if needed

### Health Checks

Monitor service health:
```bash
docker compose ps
```

## Development

### Local Development

1. Create a development environment file:
```bash
cp .env.example .env
```

2. Modify environment variables as needed

3. Start required services:
```bash
docker compose up -d 
```

### Building Custom Images

```bash
docker compose build api --no-cache
```

## Security Notes

1. Never commit `.env` files to version control
2. Regularly update service images for security patches
3. Use strong passwords for all services
4. Review and restrict network access as needed
5. Monitor service logs for security events

================================================================================

Filename: docker/volumes/api/kong.yml
Content:
_format_version: '2.1'
_transform: true

###
### Consumers / Users
###
consumers:
  - username: DASHBOARD
  - username: anon
    keyauth_credentials:
      - key: $SUPABASE_ANON_KEY
  - username: service_role
    keyauth_credentials:
      - key: $SUPABASE_SERVICE_KEY

###
### Access Control List
###
acls:
  - consumer: anon
    group: anon
  - consumer: service_role
    group: admin

###
### Dashboard credentials
###
basicauth_credentials:
  - consumer: DASHBOARD
    username: $DASHBOARD_USERNAME
    password: $DASHBOARD_PASSWORD

###
### API Routes
###
services:
  ## Open Auth routes
  - name: auth-v1-open
    url: http://auth:9999/verify
    routes:
      - name: auth-v1-open
        strip_path: true
        paths:
          - /auth/v1/verify
    plugins:
      - name: cors
  - name: auth-v1-open-callback
    url: http://auth:9999/callback
    routes:
      - name: auth-v1-open-callback
        strip_path: true
        paths:
          - /auth/v1/callback
    plugins:
      - name: cors
  - name: auth-v1-open-authorize
    url: http://auth:9999/authorize
    routes:
      - name: auth-v1-open-authorize
        strip_path: true
        paths:
          - /auth/v1/authorize
    plugins:
      - name: cors

  ## Secure Auth routes
  - name: auth-v1
    _comment: 'GoTrue: /auth/v1/* -> http://auth:9999/*'
    url: http://auth:9999/
    routes:
      - name: auth-v1-all
        strip_path: true
        paths:
          - /auth/v1/
    plugins:
      - name: cors
      - name: key-auth
        config:
          hide_credentials: false
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin
            - anon

  ## Secure REST routes
  - name: rest-v1
    _comment: 'PostgREST: /rest/v1/* -> http://rest:3000/*'
    url: http://rest:3000/
    routes:
      - name: rest-v1-all
        strip_path: true
        paths:
          - /rest/v1/
    plugins:
      - name: cors
      - name: key-auth
        config:
          hide_credentials: true
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin
            - anon

  ## Secure GraphQL routes
  - name: graphql-v1
    _comment: 'PostgREST: /graphql/v1/* -> http://rest:3000/rpc/graphql'
    url: http://rest:3000/rpc/graphql
    routes:
      - name: graphql-v1-all
        strip_path: true
        paths:
          - /graphql/v1
    plugins:
      - name: cors
      - name: key-auth
        config:
          hide_credentials: true
      - name: request-transformer
        config:
          add:
            headers:
              - Content-Profile:graphql_public
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin
            - anon

  ## Secure Realtime routes
  - name: realtime-v1-ws
    _comment: 'Realtime: /realtime/v1/* -> ws://realtime:4000/socket/*'
    url: http://realtime-dev.supabase-realtime:4000/socket
    protocol: ws
    routes:
      - name: realtime-v1-ws
        strip_path: true
        paths:
          - /realtime/v1/
    plugins:
      - name: cors
      - name: key-auth
        config:
          hide_credentials: false
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin
            - anon
  - name: realtime-v1-rest
    _comment: 'Realtime: /realtime/v1/* -> ws://realtime:4000/socket/*'
    url: http://realtime-dev.supabase-realtime:4000/api
    protocol: http
    routes:
      - name: realtime-v1-rest
        strip_path: true
        paths:
          - /realtime/v1/api
    plugins:
      - name: cors
      - name: key-auth
        config:
          hide_credentials: false
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin
            - anon
  ## Storage routes: the storage server manages its own auth
  - name: storage-v1
    _comment: 'Storage: /storage/v1/* -> http://storage:5000/*'
    url: http://storage:5000/
    routes:
      - name: storage-v1-all
        strip_path: true
        paths:
          - /storage/v1/
    plugins:
      - name: cors

  ## Edge Functions routes
  - name: functions-v1
    _comment: 'Edge Functions: /functions/v1/* -> http://functions:9000/*'
    url: http://functions:9000/
    routes:
      - name: functions-v1-all
        strip_path: true
        paths:
          - /functions/v1/
    plugins:
      - name: cors

  ## Analytics routes
  - name: analytics-v1
    _comment: 'Analytics: /analytics/v1/* -> http://logflare:4000/*'
    url: http://analytics:4000/
    routes:
      - name: analytics-v1-all
        strip_path: true
        paths:
          - /analytics/v1/

  ## Secure Database routes
  - name: meta
    _comment: 'pg-meta: /pg/* -> http://pg-meta:8080/*'
    url: http://meta:8080/
    routes:
      - name: meta-all
        strip_path: true
        paths:
          - /pg/
    plugins:
      - name: key-auth
        config:
          hide_credentials: false
      - name: acl
        config:
          hide_groups_header: true
          allow:
            - admin

  ## Protected Dashboard - catch all remaining routes
  - name: dashboard
    _comment: 'Studio: /* -> http://studio:3000/*'
    url: http://studio:3000/
    routes:
      - name: dashboard-all
        strip_path: true
        paths:
          - /
    plugins:
      - name: cors
      - name: basic-auth
        config:
          hide_credentials: true


================================================================================

Filename: docker/volumes/db/_supabase.sql
Content:
\set pguser `echo "$POSTGRES_USER"`

CREATE DATABASE _supabase WITH OWNER :pguser;


================================================================================

Filename: docker/volumes/db/init/data.sql
Content:


================================================================================

Filename: docker/volumes/db/jwt.sql
Content:
\set jwt_secret `echo "$JWT_SECRET"`
\set jwt_exp `echo "$JWT_EXP"`

ALTER DATABASE postgres SET "app.settings.jwt_secret" TO :'jwt_secret';
ALTER DATABASE postgres SET "app.settings.jwt_exp" TO :'jwt_exp';


================================================================================

Filename: docker/volumes/db/logs.sql
Content:
\set pguser `echo "$POSTGRES_USER"`

\c _supabase
create schema if not exists _analytics;
alter schema _analytics owner to :pguser;
\c postgres


================================================================================

Filename: docker/volumes/db/pooler.sql
Content:
\set pguser `echo "$POSTGRES_USER"`

\c _supabase
create schema if not exists _supavisor;
alter schema _supavisor owner to :pguser;
\c postgres


================================================================================

Filename: docker/volumes/db/realtime.sql
Content:
\set pguser `echo "$POSTGRES_USER"`

create schema if not exists _realtime;
alter schema _realtime owner to :pguser;


================================================================================

Filename: docker/volumes/db/roles.sql
Content:
-- NOTE: change to your own passwords for production environments
\set pgpass `echo "$POSTGRES_PASSWORD"`

ALTER USER authenticator WITH PASSWORD :'pgpass';
ALTER USER pgbouncer WITH PASSWORD :'pgpass';
ALTER USER supabase_auth_admin WITH PASSWORD :'pgpass';
ALTER USER supabase_functions_admin WITH PASSWORD :'pgpass';
ALTER USER supabase_storage_admin WITH PASSWORD :'pgpass';


================================================================================

Filename: docker/volumes/db/webhooks.sql
Content:
BEGIN;
  -- Create pg_net extension
  CREATE EXTENSION IF NOT EXISTS pg_net SCHEMA extensions;
  -- Create supabase_functions schema
  CREATE SCHEMA supabase_functions AUTHORIZATION supabase_admin;
  GRANT USAGE ON SCHEMA supabase_functions TO postgres, anon, authenticated, service_role;
  ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON TABLES TO postgres, anon, authenticated, service_role;
  ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON FUNCTIONS TO postgres, anon, authenticated, service_role;
  ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON SEQUENCES TO postgres, anon, authenticated, service_role;
  -- supabase_functions.migrations definition
  CREATE TABLE supabase_functions.migrations (
    version text PRIMARY KEY,
    inserted_at timestamptz NOT NULL DEFAULT NOW()
  );
  -- Initial supabase_functions migration
  INSERT INTO supabase_functions.migrations (version) VALUES ('initial');
  -- supabase_functions.hooks definition
  CREATE TABLE supabase_functions.hooks (
    id bigserial PRIMARY KEY,
    hook_table_id integer NOT NULL,
    hook_name text NOT NULL,
    created_at timestamptz NOT NULL DEFAULT NOW(),
    request_id bigint
  );
  CREATE INDEX supabase_functions_hooks_request_id_idx ON supabase_functions.hooks USING btree (request_id);
  CREATE INDEX supabase_functions_hooks_h_table_id_h_name_idx ON supabase_functions.hooks USING btree (hook_table_id, hook_name);
  COMMENT ON TABLE supabase_functions.hooks IS 'Supabase Functions Hooks: Audit trail for triggered hooks.';
  CREATE FUNCTION supabase_functions.http_request()
    RETURNS trigger
    LANGUAGE plpgsql
    AS $function$
    DECLARE
      request_id bigint;
      payload jsonb;
      url text := TG_ARGV[0]::text;
      method text := TG_ARGV[1]::text;
      headers jsonb DEFAULT '{}'::jsonb;
      params jsonb DEFAULT '{}'::jsonb;
      timeout_ms integer DEFAULT 1000;
    BEGIN
      IF url IS NULL OR url = 'null' THEN
        RAISE EXCEPTION 'url argument is missing';
      END IF;

      IF method IS NULL OR method = 'null' THEN
        RAISE EXCEPTION 'method argument is missing';
      END IF;

      IF TG_ARGV[2] IS NULL OR TG_ARGV[2] = 'null' THEN
        headers = '{"Content-Type": "application/json"}'::jsonb;
      ELSE
        headers = TG_ARGV[2]::jsonb;
      END IF;

      IF TG_ARGV[3] IS NULL OR TG_ARGV[3] = 'null' THEN
        params = '{}'::jsonb;
      ELSE
        params = TG_ARGV[3]::jsonb;
      END IF;

      IF TG_ARGV[4] IS NULL OR TG_ARGV[4] = 'null' THEN
        timeout_ms = 1000;
      ELSE
        timeout_ms = TG_ARGV[4]::integer;
      END IF;

      CASE
        WHEN method = 'GET' THEN
          SELECT http_get INTO request_id FROM net.http_get(
            url,
            params,
            headers,
            timeout_ms
          );
        WHEN method = 'POST' THEN
          payload = jsonb_build_object(
            'old_record', OLD,
            'record', NEW,
            'type', TG_OP,
            'table', TG_TABLE_NAME,
            'schema', TG_TABLE_SCHEMA
          );

          SELECT http_post INTO request_id FROM net.http_post(
            url,
            payload,
            params,
            headers,
            timeout_ms
          );
        ELSE
          RAISE EXCEPTION 'method argument % is invalid', method;
      END CASE;

      INSERT INTO supabase_functions.hooks
        (hook_table_id, hook_name, request_id)
      VALUES
        (TG_RELID, TG_NAME, request_id);

      RETURN NEW;
    END
  $function$;
  -- Supabase super admin
  DO
  $$
  BEGIN
    IF NOT EXISTS (
      SELECT 1
      FROM pg_roles
      WHERE rolname = 'supabase_functions_admin'
    )
    THEN
      CREATE USER supabase_functions_admin NOINHERIT CREATEROLE LOGIN NOREPLICATION;
    END IF;
  END
  $$;
  GRANT ALL PRIVILEGES ON SCHEMA supabase_functions TO supabase_functions_admin;
  GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA supabase_functions TO supabase_functions_admin;
  GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA supabase_functions TO supabase_functions_admin;
  ALTER USER supabase_functions_admin SET search_path = "supabase_functions";
  ALTER table "supabase_functions".migrations OWNER TO supabase_functions_admin;
  ALTER table "supabase_functions".hooks OWNER TO supabase_functions_admin;
  ALTER function "supabase_functions".http_request() OWNER TO supabase_functions_admin;
  GRANT supabase_functions_admin TO postgres;
  -- Remove unused supabase_pg_net_admin role
  DO
  $$
  BEGIN
    IF EXISTS (
      SELECT 1
      FROM pg_roles
      WHERE rolname = 'supabase_pg_net_admin'
    )
    THEN
      REASSIGN OWNED BY supabase_pg_net_admin TO supabase_admin;
      DROP OWNED BY supabase_pg_net_admin;
      DROP ROLE supabase_pg_net_admin;
    END IF;
  END
  $$;
  -- pg_net grants when extension is already enabled
  DO
  $$
  BEGIN
    IF EXISTS (
      SELECT 1
      FROM pg_extension
      WHERE extname = 'pg_net'
    )
    THEN
      GRANT USAGE ON SCHEMA net TO supabase_functions_admin, postgres, anon, authenticated, service_role;
      ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
      ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
      ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
      ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
      REVOKE ALL ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
      REVOKE ALL ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
      GRANT EXECUTE ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
      GRANT EXECUTE ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
    END IF;
  END
  $$;
  -- Event trigger for pg_net
  CREATE OR REPLACE FUNCTION extensions.grant_pg_net_access()
  RETURNS event_trigger
  LANGUAGE plpgsql
  AS $$
  BEGIN
    IF EXISTS (
      SELECT 1
      FROM pg_event_trigger_ddl_commands() AS ev
      JOIN pg_extension AS ext
      ON ev.objid = ext.oid
      WHERE ext.extname = 'pg_net'
    )
    THEN
      GRANT USAGE ON SCHEMA net TO supabase_functions_admin, postgres, anon, authenticated, service_role;
      ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
      ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
      ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
      ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
      REVOKE ALL ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
      REVOKE ALL ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
      GRANT EXECUTE ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
      GRANT EXECUTE ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
    END IF;
  END;
  $$;
  COMMENT ON FUNCTION extensions.grant_pg_net_access IS 'Grants access to pg_net';
  DO
  $$
  BEGIN
    IF NOT EXISTS (
      SELECT 1
      FROM pg_event_trigger
      WHERE evtname = 'issue_pg_net_access'
    ) THEN
      CREATE EVENT TRIGGER issue_pg_net_access ON ddl_command_end WHEN TAG IN ('CREATE EXTENSION')
      EXECUTE PROCEDURE extensions.grant_pg_net_access();
    END IF;
  END
  $$;
  INSERT INTO supabase_functions.migrations (version) VALUES ('20210809183423_update_grants');
  ALTER function supabase_functions.http_request() SECURITY DEFINER;
  ALTER function supabase_functions.http_request() SET search_path = supabase_functions;
  REVOKE ALL ON FUNCTION supabase_functions.http_request() FROM PUBLIC;
  GRANT EXECUTE ON FUNCTION supabase_functions.http_request() TO postgres, anon, authenticated, service_role;
COMMIT;


================================================================================

Filename: docker/volumes/functions/hello/index.ts
Content:
// Follow this setup guide to integrate the Deno language server with your editor:
// https://deno.land/manual/getting_started/setup_your_environment
// This enables autocomplete, go to definition, etc.

import { serve } from "https://deno.land/std@0.177.1/http/server.ts"

serve(async () => {
  return new Response(
    `"Hello from Edge Functions!"`,
    { headers: { "Content-Type": "application/json" } },
  )
})

// To invoke:
// curl 'http://localhost:<KONG_HTTP_PORT>/functions/v1/hello' \
//   --header 'Authorization: Bearer <anon/service_role API key>'


================================================================================

Filename: docker/volumes/functions/main/index.ts
Content:
import { serve } from 'https://deno.land/std@0.131.0/http/server.ts'
import * as jose from 'https://deno.land/x/jose@v4.14.4/index.ts'

console.log('main function started')

const JWT_SECRET = Deno.env.get('JWT_SECRET')
const VERIFY_JWT = Deno.env.get('VERIFY_JWT') === 'true'

function getAuthToken(req: Request) {
  const authHeader = req.headers.get('authorization')
  if (!authHeader) {
    throw new Error('Missing authorization header')
  }
  const [bearer, token] = authHeader.split(' ')
  if (bearer !== 'Bearer') {
    throw new Error(`Auth header is not 'Bearer {token}'`)
  }
  return token
}

async function verifyJWT(jwt: string): Promise<boolean> {
  const encoder = new TextEncoder()
  const secretKey = encoder.encode(JWT_SECRET)
  try {
    await jose.jwtVerify(jwt, secretKey)
  } catch (err) {
    console.error(err)
    return false
  }
  return true
}

serve(async (req: Request) => {
  if (req.method !== 'OPTIONS' && VERIFY_JWT) {
    try {
      const token = getAuthToken(req)
      const isValidJWT = await verifyJWT(token)

      if (!isValidJWT) {
        return new Response(JSON.stringify({ msg: 'Invalid JWT' }), {
          status: 401,
          headers: { 'Content-Type': 'application/json' },
        })
      }
    } catch (e) {
      console.error(e)
      return new Response(JSON.stringify({ msg: e.toString() }), {
        status: 401,
        headers: { 'Content-Type': 'application/json' },
      })
    }
  }

  const url = new URL(req.url)
  const { pathname } = url
  const path_parts = pathname.split('/')
  const service_name = path_parts[1]

  if (!service_name || service_name === '') {
    const error = { msg: 'missing function name in request' }
    return new Response(JSON.stringify(error), {
      status: 400,
      headers: { 'Content-Type': 'application/json' },
    })
  }

  const servicePath = `/home/deno/functions/${service_name}`
  console.error(`serving the request with ${servicePath}`)

  const memoryLimitMb = 150
  const workerTimeoutMs = 1 * 60 * 1000
  const noModuleCache = false
  const importMapPath = null
  const envVarsObj = Deno.env.toObject()
  const envVars = Object.keys(envVarsObj).map((k) => [k, envVarsObj[k]])

  try {
    const worker = await EdgeRuntime.userWorkers.create({
      servicePath,
      memoryLimitMb,
      workerTimeoutMs,
      noModuleCache,
      importMapPath,
      envVars,
    })
    return await worker.fetch(req)
  } catch (e) {
    const error = { msg: e.toString() }
    return new Response(JSON.stringify(error), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    })
  }
})


================================================================================

Filename: docker/volumes/logs/vector.yml
Content:
api:
  enabled: true
  address: 0.0.0.0:9001

sources:
  docker_host:
    type: docker_logs
    exclude_containers:
      - supabase-vector

transforms:
  project_logs:
    type: remap
    inputs:
      - docker_host
    source: |-
      .project = "default"
      .event_message = del(.message)
      .appname = del(.container_name)
      del(.container_created_at)
      del(.container_id)
      del(.source_type)
      del(.stream)
      del(.label)
      del(.image)
      del(.host)
      del(.stream)
  router:
    type: route
    inputs:
      - project_logs
    route:
      kong: '.appname == "supabase-kong"'
      auth: '.appname == "supabase-auth"'
      rest: '.appname == "supabase-rest"'
      realtime: '.appname == "supabase-realtime"'
      storage: '.appname == "supabase-storage"'
      functions: '.appname == "supabase-functions"'
      db: '.appname == "supabase-db"'
  # Ignores non nginx errors since they are related with kong booting up
  kong_logs:
    type: remap
    inputs:
      - router.kong
    source: |-
      req, err = parse_nginx_log(.event_message, "combined")
      if err == null {
          .timestamp = req.timestamp
          .metadata.request.headers.referer = req.referer
          .metadata.request.headers.user_agent = req.agent
          .metadata.request.headers.cf_connecting_ip = req.client
          .metadata.request.method = req.method
          .metadata.request.path = req.path
          .metadata.request.protocol = req.protocol
          .metadata.response.status_code = req.status
      }
      if err != null {
        abort
      }
  # Ignores non nginx errors since they are related with kong booting up
  kong_err:
    type: remap
    inputs:
      - router.kong
    source: |-
      .metadata.request.method = "GET"
      .metadata.response.status_code = 200
      parsed, err = parse_nginx_log(.event_message, "error")
      if err == null {
          .timestamp = parsed.timestamp
          .severity = parsed.severity
          .metadata.request.host = parsed.host
          .metadata.request.headers.cf_connecting_ip = parsed.client
          url, err = split(parsed.request, " ")
          if err == null {
              .metadata.request.method = url[0]
              .metadata.request.path = url[1]
              .metadata.request.protocol = url[2]
          }
      }
      if err != null {
        abort
      }
  # Gotrue logs are structured json strings which frontend parses directly. But we keep metadata for consistency.
  auth_logs:
    type: remap
    inputs:
      - router.auth
    source: |-
      parsed, err = parse_json(.event_message)
      if err == null {
          .metadata.timestamp = parsed.time
          .metadata = merge!(.metadata, parsed)
      }
  # PostgREST logs are structured so we separate timestamp from message using regex
  rest_logs:
    type: remap
    inputs:
      - router.rest
    source: |-
      parsed, err = parse_regex(.event_message, r'^(?P<time>.*): (?P<msg>.*)$')
      if err == null {
          .event_message = parsed.msg
          .timestamp = to_timestamp!(parsed.time)
          .metadata.host = .project
      }
  # Realtime logs are structured so we parse the severity level using regex (ignore time because it has no date)
  realtime_logs:
    type: remap
    inputs:
      - router.realtime
    source: |-
      .metadata.project = del(.project)
      .metadata.external_id = .metadata.project
      parsed, err = parse_regex(.event_message, r'^(?P<time>\d+:\d+:\d+\.\d+) \[(?P<level>\w+)\] (?P<msg>.*)$')
      if err == null {
          .event_message = parsed.msg
          .metadata.level = parsed.level
      }
  # Storage logs may contain json objects so we parse them for completeness
  storage_logs:
    type: remap
    inputs:
      - router.storage
    source: |-
      .metadata.project = del(.project)
      .metadata.tenantId = .metadata.project
      parsed, err = parse_json(.event_message)
      if err == null {
          .event_message = parsed.msg
          .metadata.level = parsed.level
          .metadata.timestamp = parsed.time
          .metadata.context[0].host = parsed.hostname
          .metadata.context[0].pid = parsed.pid
      }
  # Postgres logs some messages to stderr which we map to warning severity level
  db_logs:
    type: remap
    inputs:
      - router.db
    source: |-
      .metadata.host = "db-default"
      .metadata.parsed.timestamp = .timestamp

      parsed, err = parse_regex(.event_message, r'.*(?P<level>INFO|NOTICE|WARNING|ERROR|LOG|FATAL|PANIC?):.*', numeric_groups: true)

      if err != null || parsed == null {
        .metadata.parsed.error_severity = "info"
      }
      if parsed != null {
       .metadata.parsed.error_severity = parsed.level
      }
      if .metadata.parsed.error_severity == "info" {
          .metadata.parsed.error_severity = "log"
      }
      .metadata.parsed.error_severity = upcase!(.metadata.parsed.error_severity)

sinks:
  logflare_auth:
    type: 'http'
    inputs:
      - auth_logs
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=gotrue.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_realtime:
    type: 'http'
    inputs:
      - realtime_logs
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=realtime.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_rest:
    type: 'http'
    inputs:
      - rest_logs
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=postgREST.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_db:
    type: 'http'
    inputs:
      - db_logs
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    # We must route the sink through kong because ingesting logs before logflare is fully initialised will
    # lead to broken queries from studio. This works by the assumption that containers are started in the
    # following order: vector > db > logflare > kong
    uri: 'http://kong:8000/analytics/v1/api/logs?source_name=postgres.logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_functions:
    type: 'http'
    inputs:
      - router.functions
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=deno-relay-logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_storage:
    type: 'http'
    inputs:
      - storage_logs
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=storage.logs.prod.2&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
  logflare_kong:
    type: 'http'
    inputs:
      - kong_logs
      - kong_err
    encoding:
      codec: 'json'
    method: 'post'
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=cloudflare.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'


================================================================================

Filename: docker/volumes/pooler/pooler.exs
Content:
{:ok, _} = Application.ensure_all_started(:supavisor)

{:ok, version} =
  case Supavisor.Repo.query!("select version()") do
    %{rows: [[ver]]} -> Supavisor.Helpers.parse_pg_version(ver)
    _ -> nil
  end

params = %{
  "external_id" => System.get_env("POOLER_TENANT_ID"),
  "db_host" => "db",
  "db_port" => System.get_env("POSTGRES_PORT"),
  "db_database" => System.get_env("POSTGRES_DB"),
  "require_user" => false,
  "auth_query" => "SELECT * FROM pgbouncer.get_auth($1)",
  "default_max_clients" => System.get_env("POOLER_MAX_CLIENT_CONN"),
  "default_pool_size" => System.get_env("POOLER_DEFAULT_POOL_SIZE"),
  "default_parameter_status" => %{"server_version" => version},
  "users" => [%{
    "db_user" => "pgbouncer",
    "db_password" => System.get_env("POSTGRES_PASSWORD"),
    "mode_type" => System.get_env("POOLER_POOL_MODE"),
    "pool_size" => System.get_env("POOLER_DEFAULT_POOL_SIZE"),
    "is_manager" => true
  }]
}

if !Supavisor.Tenants.get_tenant_by_external_id(params["external_id"]) do
  {:ok, _} = Supavisor.Tenants.create_tenant(params)
end


================================================================================

Filename: entrypoint.sh
Content:
#!/bin/bash

# Set default service type if not provided through environment
# This ensures the container has a valid running mode
SERVICE_TYPE=${SERVICE_TYPE:-"api"}

# Log the service type for debugging purposes
echo "Starting service: $SERVICE_TYPE"

# Handle different service types
if [ "$SERVICE_TYPE" = "worker" ]; then
    # Start the background worker process for handling async tasks
    echo "Starting worker process..."
    python run_worker.py
elif [ "$SERVICE_TYPE" = "api" ]; then
    # Start the FastAPI server with configurable host and port
    # HOST and PORT can be overridden through environment variables
    echo "Starting API server..."
    uvicorn run_api:app --host ${HOST:-"0.0.0.0"} --port ${PORT:-"8123"}
else
    # Exit with error if an invalid service type is specified
    echo "Error: Unknown SERVICE_TYPE. Must be 'api' or 'worker'"
    exit 1
fi

================================================================================

Filename: handlers/__init__.py
Content:


================================================================================

Filename: handlers/exception.py
Content:
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse


async def exception_handler(request: Request, exc: HTTPException):
    """Handle global exceptions and return JSON response."""
    path_parts = request.url.path.strip('/').split('/')
    namespace = path_parts[0] if path_parts else 'main'
    route = path_parts[1] if len(path_parts) > 1 else 'index'
    request.state.logger.exception(f"An error occurred: {str(exc.detail)}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"message": exc.detail}
    )


================================================================================

Filename: logs/readme.md
Content:
# Logs Directory
___
This directory will contain logs created at runtime.

================================================================================

Filename: models/__init__.py
Content:
from configs.database import engine, SessionLocal
from .base import Base, schema_name
from .rbac_model import *
from .users_model import *
from .tab_model import *
from .workflow_model import *
import json
import uuid

def init_db():
    # Create tables
    Base.metadata.create_all(bind=engine)

    # Initialize roles and permissions
    with open("configs/roles_permissions.json") as f:
        roles_permissions_data = json.load(f)

    with open("configs/admin.json") as f:
        admin_users = json.load(f)

    db = SessionLocal()
    try:
        # Create roles and store mapping
        role_mapping = {}  # Store role objects by name
        for role_data in roles_permissions_data["roles"]:
            role = db.query(Role).filter_by(name=role_data["name"]).first()
            if not role:
                role = Role(name=role_data["name"], description=role_data["description"])
                db.add(role)
            role_mapping[role.name] = role

        # Create permissions and store mapping
        permission_mapping = {}  # Store permission objects by name
        for permission_data in roles_permissions_data["permissions"]:
            permission = db.query(Permission).filter_by(name=permission_data["name"]).first()
            if not permission:
                permission = Permission(
                    name=permission_data["name"],
                    description=permission_data["description"]
                )
                db.add(permission)
            permission_mapping[permission.name] = permission

        # Assign permissions to roles based on the JSON configuration
        for role_data in roles_permissions_data["roles"]:
            role_name = role_data["name"]
            role = role_mapping.get(role_name)
            if role:
                assigned_permissions = []
                for permission_name in role_data.get("permissions", []):
                    permission = permission_mapping.get(permission_name)
                    if permission:
                        assigned_permissions.append(permission)
                    else:
                        # Optional: Add logging or warning for missing permissions defined in JSON
                        print(f"Warning: Permission '{permission_name}' defined for role '{role_name}' not found in permissions list.")
                role.permissions = assigned_permissions

        # Create admin users with admin role
        admin_role = role_mapping.get("admin")
        if admin_role:
            for email in admin_users:
                user = db.query(User).filter_by(email=email).first()
                if not user:
                    # Convert email name part to proper formatted username
                    name_part = email.split('@')[0].replace('.', ' ').title()
                    user = User(
                        uid=str(uuid.uuid4()),
                        email=email,
                        UserName=name_part
                    )
                    user.roles = [admin_role]
                    db.add(user)

        db.commit()
    except Exception as e:
        db.rollback()
        raise e
    finally:
        db.close()


================================================================================

Filename: models/base.py
Content:
from sqlalchemy import MetaData
from sqlalchemy.ext.declarative import declarative_base

schema_name = "public"

metadata = MetaData(schema=schema_name)

Base = declarative_base(metadata=metadata)



================================================================================

Filename: models/rbac_model.py
Content:
from sqlalchemy import Column, String, Integer, ForeignKey, Table
from sqlalchemy.orm import relationship

from .base import Base, schema_name

# Association table for user-roles relationship
user_roles = Table(
    'user_roles',
    Base.metadata,
    Column('uid', String, ForeignKey(f'{schema_name}.users.uid')),
    Column('role_id', Integer, ForeignKey(f'{schema_name}.roles.id')),
    schema=schema_name
)

# Association table for role-permissions relationship
role_permissions = Table(
    'role_permissions',
    Base.metadata,
    Column('role_id', Integer, ForeignKey('roles.id')),
    Column('permission_id', Integer, ForeignKey('permissions.id')),
    schema=schema_name
)

# Association table for role-email_groups relationship
role_email_groups = Table(
    'role_email_groups',
    Base.metadata,
    Column('role_id', Integer, ForeignKey('roles.id')),
    Column('email_group_id', Integer, ForeignKey('email_groups.id')),
    schema=schema_name
)


class EmailAddress(Base):
    __tablename__ = 'email_addresses'
    __table_args__ = {'schema': schema_name}

    id = Column(Integer, primary_key=True)
    email = Column(String, nullable=False)
    group_id = Column(Integer, ForeignKey(f'{schema_name}.email_groups.id', ondelete='CASCADE'))

    # Relationship with email group
    group = relationship('EmailGroup', back_populates='email_addresses')


class Permission(Base):
    __tablename__ = 'permissions'
    __table_args__ = {'schema': schema_name}

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)
    description = Column(String)

    # Relationship
    roles = relationship('Role', secondary=role_permissions, back_populates='permissions')

    def to_response(self) -> dict:
        return {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "roles": [role.name for role in self.roles]
        }


class Role(Base):
    __tablename__ = 'roles'
    __table_args__ = {'schema': schema_name}

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)
    description = Column(String)

    # Relationships
    users = relationship('User', secondary=user_roles, back_populates='roles')
    permissions = relationship('Permission', secondary=role_permissions, back_populates='roles')
    email_groups = relationship('EmailGroup', secondary=role_email_groups, back_populates='roles')

    def to_response(self) -> dict:
        return {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "permissions": [perm.name for perm in self.permissions],
            "email_groups": [group.name for group in self.email_groups]
        }


class EmailGroup(Base):
    __tablename__ = 'email_groups'
    __table_args__ = {'schema': schema_name}

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)
    description = Column(String)

    # Relationships
    email_addresses = relationship('EmailAddress', back_populates='group', cascade='all, delete-orphan')
    roles = relationship('Role', secondary=role_email_groups, back_populates='email_groups')

    def get_email_addresses(self) -> list[str]:
        """Get all email addresses in this group."""
        return [addr.email for addr in self.email_addresses]

    def to_response(self) -> dict:
        return {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "email_addresses": self.get_email_addresses(),
            "roles": [role.name for role in self.roles]
        }


================================================================================

Filename: models/tab_model.py
Content:
import uuid
from sqlalchemy import Column, String, DateTime, func, ForeignKey, Text
from sqlalchemy.dialects.postgresql import UUID  
from sqlalchemy.orm import relationship
from sqlalchemy.types import JSON 

from .base import Base, schema_name
from .users_model import User  # Import User for relationship typing


class Tab(Base):
    __tablename__ = "tabs"
    __table_args__ = {"schema": schema_name}

    tab_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    uid = Column(String, ForeignKey(f'{schema_name}.users.uid'), nullable=False, index=True)
    subject = Column(String, nullable=True)
    from_address = Column(String, nullable=True)
    to_address = Column(JSON, nullable=True)  # Storing as JSON list
    cc_address = Column(JSON, nullable=True)  # Storing as JSON list
    bcc_address = Column(JSON, nullable=True) # Storing as JSON list
    body = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), default=func.now())

    # Relationship to User
    user = relationship("User", back_populates="tabs")

    def to_response(self) -> dict:
        """Convert tab model to a dictionary suitable for API response."""
        return {
            "tab_id": self.tab_id,
            "uid": self.uid,
            "subject": self.subject,
            "from_address": self.from_address,
            "to_address": self.to_address,
            "cc_address": self.cc_address,
            "bcc_address": self.bcc_address,
            "body": self.body,
            "created_at": self.created_at
        }

================================================================================

Filename: models/users_model.py
Content:
from sqlalchemy import Column, String, DateTime, func, event
from sqlalchemy.orm import relationship
from .rbac_model import user_roles
from .base import Base, schema_name
from typing import List
import uuid


class User(Base):
    __tablename__ = "users"
    __table_args__ = {"schema": schema_name}

    uid = Column(String, unique=True, nullable=False, primary_key=True)
    email = Column(String)
    UserName = Column(String, nullable=False)
    created_at = Column(DateTime, default=func.now())
    profile_image_url = Column(String, nullable=True)

    # Workflow relationship
    workflows = relationship('Workflow', back_populates='user', cascade='all, delete-orphan')

    # Tab storage relationship
    tabs = relationship("Tab", back_populates="user", cascade="all, delete-orphan")

    # Relationships
    roles = relationship('Role', secondary=user_roles, back_populates='users', lazy='joined')

    def has_permission(self, permission_name: str) -> bool:
        """Check if user has a specific permission through any of their roles."""
        return any(
            any(p.name == permission_name for p in role.permissions)
            for role in self.roles
        )

    def can_send_to_group(self, group_name: str) -> bool:
        """Check if user can send alerts to a specific email group through their roles."""
        return any(
            any(group.name == group_name for group in role.email_groups)
            for role in self.roles
        )

    def get_all_permissions(self) -> set:
        """Get all permissions user has through their roles."""
        return {
            permission.name
            for role in self.roles
            for permission in role.permissions
        }

    def get_authorized_groups(self) -> set:
        """Get all email groups user can send to through their roles."""
        return {
            group.name
            for role in self.roles
            for group in role.email_groups
        }

    def get_role_names(self) -> List[str]:
        """Get list of role names."""
        return [role.name for role in self.roles]

    def to_response(self) -> dict:
        """Convert user to response format."""
        return {
            "email": self.email,
            "UserName": self.UserName,
            "roles": self.get_role_names(),
            "permissions": list(self.get_all_permissions()),
            "authorized_groups": list(self.get_authorized_groups()),
            "profile_image_url": self.profile_image_url
        }

    

================================================================================

Filename: models/workflow_model.py
Content:
from sqlalchemy import Column, String, Integer, ForeignKey, DateTime, func
from sqlalchemy.orm import relationship
from .base import Base, schema_name


class Workflow(Base):
    __tablename__ = "workflows"
    __table_args__ = {"schema": schema_name}

    workflow_id = Column(String, primary_key=True)
    uid = Column(String, ForeignKey(f"{schema_name}.users.uid"))
    created_at = Column(DateTime, default=func.now())

    # Relationship
    user = relationship('User', back_populates='workflows')

    def to_response(self) -> dict:
        """Convert workflow to response format."""
        return {
            "workflow_id": self.workflow_id,
            "uid": self.uid,
            "created_at": self.created_at
        }

================================================================================

Filename: pytest.ini
Content:
[pytest]
pythonpath = .
testpaths = tests

================================================================================

Filename: requirements.in
Content:
annotated-types==0.7.0
anyio==4.7.0
click==8.1.7
colorama==0.4.6
exceptiongroup==1.2.2
fastapi==0.115.6
h11==0.14.0
idna==3.10
protobuf==5.29.2
pydantic==2.10.3
pydantic_core==2.27.1
python-dateutil==2.9.0.post0
six==1.17.0
sniffio==1.3.1
starlette==0.41.3
temporalio==1.10.0
types-protobuf==5.29.1.20241207
typing_extensions==4.12.2
uvicorn==0.32.1
pydantic[email]
SQLAlchemy~=2.0.36
python-dotenv~=1.0.1
requests~=2.32.3
psycopg2-binary
PyJWT~=2.10.1
pydantic-settings~=2.7.0
supabase~=2.11.0
python-multipart
alembic
safety


================================================================================

Filename: requirements.txt
Content:
#
# This file is autogenerated by pip-compile with Python 3.12
# by the following command:
#
#    pip-compile
#
aiohappyeyeballs==2.6.1
    # via aiohttp
aiohttp==3.11.16
    # via realtime
aiosignal==1.3.2
    # via aiohttp
alembic==1.15.2
    # via -r requirements.in
annotated-types==0.7.0
    # via
    #   -r requirements.in
    #   pydantic
anyio==4.7.0
    # via
    #   -r requirements.in
    #   httpx
    #   starlette
attrs==25.3.0
    # via aiohttp
authlib==1.5.2
    # via safety
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
cffi==1.17.1
    # via cryptography
charset-normalizer==3.4.1
    # via requests
click==8.1.7
    # via
    #   -r requirements.in
    #   safety
    #   typer
    #   uvicorn
colorama==0.4.6
    # via
    #   -r requirements.in
    #   click
    #   pytest
cryptography==44.0.2
    # via authlib
deprecation==2.1.0
    # via postgrest
dnspython==2.7.0
    # via email-validator
dparse==0.6.4
    # via
    #   safety
    #   safety-schemas
email-validator==2.2.0
    # via pydantic
exceptiongroup==1.2.2
    # via -r requirements.in
fastapi==0.115.6
    # via -r requirements.in
filelock==3.12.4
    # via safety
frozenlist==1.6.0
    # via
    #   aiohttp
    #   aiosignal
gotrue==2.12.0
    # via supabase
greenlet==3.2.0
    # via sqlalchemy
h11==0.14.0
    # via
    #   -r requirements.in
    #   httpcore
    #   uvicorn
h2==4.2.0
    # via httpx
hpack==4.1.0
    # via h2
httpcore==1.0.8
    # via httpx
httpx[http2]==0.28.1
    # via
    #   gotrue
    #   postgrest
    #   storage3
    #   supabase
    #   supafunc
hyperframe==6.1.0
    # via h2
idna==3.10
    # via
    #   -r requirements.in
    #   anyio
    #   email-validator
    #   httpx
    #   requests
    #   yarl
iniconfig==2.1.0
    # via pytest
jinja2==3.1.6
    # via safety
mako==1.3.10
    # via alembic
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via
    #   jinja2
    #   mako
marshmallow==4.0.0
    # via safety
mdurl==0.1.2
    # via markdown-it-py
multidict==6.4.3
    # via
    #   aiohttp
    #   yarl
packaging==24.2
    # via
    #   deprecation
    #   dparse
    #   pytest
    #   safety
    #   safety-schemas
pluggy==1.5.0
    # via pytest
postgrest==0.19.3
    # via supabase
propcache==0.3.1
    # via
    #   aiohttp
    #   yarl
protobuf==5.29.2
    # via
    #   -r requirements.in
    #   temporalio
psutil==6.0.0
    # via safety
psycopg2-binary==2.9.10
    # via -r requirements.in
pycparser==2.22
    # via cffi
pydantic[email]==2.10.3
    # via
    #   -r requirements.in
    #   fastapi
    #   gotrue
    #   postgrest
    #   pydantic-settings
    #   safety
    #   safety-schemas
pydantic-core==2.27.1
    # via
    #   -r requirements.in
    #   pydantic
pydantic-settings==2.7.1
    # via -r requirements.in
pygments==2.19.1
    # via rich
pyjwt==2.10.1
    # via
    #   -r requirements.in
    #   gotrue
pytest==8.3.5
    # via pytest-mock
pytest-mock==3.14.0
    # via gotrue
python-dateutil==2.9.0.post0
    # via
    #   -r requirements.in
    #   realtime
    #   storage3
python-dotenv==1.0.1
    # via
    #   -r requirements.in
    #   pydantic-settings
python-multipart==0.0.20
    # via -r requirements.in
realtime==2.4.2
    # via supabase
requests==2.32.3
    # via
    #   -r requirements.in
    #   safety
rich==14.0.0
    # via
    #   safety
    #   typer
ruamel-yaml==0.18.10
    # via
    #   safety
    #   safety-schemas
ruamel-yaml-clib==0.2.12
    # via ruamel-yaml
safety==3.2.9
    # via -r requirements.in
safety-schemas==0.0.5
    # via safety
shellingham==1.5.4
    # via typer
six==1.17.0
    # via
    #   -r requirements.in
    #   python-dateutil
sniffio==1.3.1
    # via
    #   -r requirements.in
    #   anyio
sqlalchemy==2.0.40
    # via
    #   -r requirements.in
    #   alembic
starlette==0.41.3
    # via
    #   -r requirements.in
    #   fastapi
storage3==0.11.3
    # via supabase
strenum==0.4.15
    # via supafunc
supabase==2.11.0
    # via -r requirements.in
supafunc==0.9.4
    # via supabase
temporalio==1.10.0
    # via -r requirements.in
typer==0.15.2
    # via safety
types-protobuf==5.29.1.20241207
    # via
    #   -r requirements.in
    #   temporalio
typing-extensions==4.12.2
    # via
    #   -r requirements.in
    #   alembic
    #   anyio
    #   fastapi
    #   pydantic
    #   pydantic-core
    #   realtime
    #   safety
    #   safety-schemas
    #   sqlalchemy
    #   temporalio
    #   typer
urllib3==2.4.0
    # via
    #   requests
    #   safety
uvicorn==0.32.1
    # via -r requirements.in
websockets==14.2
    # via realtime
yarl==1.20.0
    # via aiohttp

# The following packages are considered to be unsafe in a requirements file:
# setuptools


================================================================================

Filename: results_cov.txt
Content:
============================= test session starts =============================
platform win32 -- Python 3.12.5, pytest-8.3.5, pluggy-1.5.0 -- C:\PyCharmProjects\Ventory\alert.ventory\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\PyCharmProjects\Ventory\alert.ventory
configfile: ov
plugins: anyio-4.7.0, asyncio-0.26.0, cov-6.1.1
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 13 items

tests/test_auth.py::test_create_access_token PASSED                      [  7%]
tests/test_auth.py::test_create_refresh_token PASSED                     [ 15%]
tests/test_auth.py::test_login_endpoint PASSED                           [ 23%]
tests/test_auth.py::test_refresh_token_endpoint PASSED                   [ 30%]
tests/test_auth.py::test_invalid_refresh_token PASSED                    [ 38%]
tests/test_auth.py::test_expired_tokens PASSED                           [ 46%]
tests/test_users_model.py::test_user_creation PASSED                     [ 53%]
tests/test_users_model.py::test_has_permission PASSED                    [ 61%]
tests/test_users_model.py::test_can_send_to_group PASSED                 [ 69%]
tests/test_users_model.py::test_get_all_permissions PASSED               [ 76%]
tests/test_users_model.py::test_get_authorized_groups PASSED             [ 84%]
tests/test_users_model.py::test_get_role_names PASSED                    [ 92%]
tests/test_users_model.py::test_to_response PASSED                       [100%]

============================== warnings summary ===============================
models\base.py:8
  C:\PyCharmProjects\Ventory\alert.ventory\models\base.py:8: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base(metadata=metadata)

tests/test_auth.py::test_refresh_token_endpoint
tests/test_auth.py::test_expired_tokens
  C:\PyCharmProjects\Ventory\alert.ventory\.venv\Lib\site-packages\jwt\api_jwt.py:125: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    now = timegm(datetime.utcnow().utctimetuple())

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================= 13 passed, 3 warnings in 3.16s ========================


================================================================================

Filename: routes/__init__.py
Content:
from .auth import router as auth_router
from .users import router as users_router
from .email import router as email_router
from .storage import router as storage_router
from .rbac import router as rbac_router
from .email_groups import router as email_groups_router
from .workflow import router as workflow_router
from .tab import router as tab_router


================================================================================

Filename: routes/auth.py
Content:
"""
Authentication Router Module

This module handles authentication-related routes including:
- User login with Zoho OAuth2 integration
- Token refresh for maintaining user sessions

The module uses JWT (JSON Web Tokens) for secure authentication:
- Access tokens for API authorization
- Refresh tokens for obtaining new access tokens

Routes:
- POST /token: Login endpoint for obtaining initial access and refresh tokens
- POST /refresh: Endpoint for refreshing expired access tokens
"""

from fastapi import APIRouter, Depends
from fastapi.security import HTTPBearer
from sqlalchemy.orm import Session

from configs.database import get_db
from controllers.auth_controller import authenticate_user, refresh_token
from schemas.auth_schema import AuthRequest, RefreshRequest

router = APIRouter()

# Create security scheme for JWT tokens
security = HTTPBearer()


@router.post("/token")
async def login_for_access_token(auth_request: AuthRequest, db: Session = Depends(get_db)):
    """
    Authenticate user and generate access tokens.

    This endpoint:
    1. Validates the Zoho OAuth authorization code
    2. Retrieves user information
    3. Generates both access and refresh tokens

    Args:
        auth_request (AuthRequest): Contains the Zoho OAuth authorization code
        db (Session): Database session dependency

    Returns:
        dict: Contains:
            - access_token: JWT access token
            - refresh_token: JWT refresh token
            - token_type: Token type (bearer)
            - permissions: List of user permissions
            - roles: List of user roles

    Raises:
        HTTPException: 
            - 400: Invalid authorization code
            - 401: Authentication failed
            - 403: Insufficient permissions
            - 404: User not found
    """
    return await authenticate_user(auth_request.auth_code, db)


@router.post("/refresh")
async def refresh_access_token(
        refresh_request: RefreshRequest,
        db: Session = Depends(get_db)
):
    """
    Refresh an expired access token.

    This endpoint:
    1. Validates the provided refresh token
    2. Generates a new access token if refresh token is valid
    3. Returns updated token information

    Args:
        refresh_request (RefreshRequest): Contains the refresh token
        db (Session): Database session dependency

    Returns:
        dict: Contains:
            - access_token: New JWT access token
            - token_type: Token type (bearer)
            - permissions: List of user permissions
            - roles: List of user roles

    Raises:
        HTTPException:
            - 401: Invalid or expired refresh token
            - 403: Insufficient permissions
            - 404: User not found
    """
    return await refresh_token(refresh_request.refresh_token, db)


================================================================================

Filename: routes/email.py
Content:
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from configs.database import get_db
from schemas.email_schema import EmailCreate, EmailConfirmation
from controllers.email_controller import EmailController
from models.users_model import User
from utils.jwt import get_current_user

router = APIRouter()


@router.post("/send")
async def send_email(
    email_data: EmailCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    return await EmailController.handle_send_email(email_data, current_user, db)


@router.post("/confirm")
async def confirm_email(
    confirmation: EmailConfirmation,
    current_user: User = Depends(get_current_user)
):
    return await EmailController.handle_confirm_email(confirmation, current_user)


================================================================================

Filename: routes/email_groups.py
Content:
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List

from configs.database import get_db
from models.users_model import User
from models.rbac_model import EmailGroup, Role, EmailAddress
from schemas.rbac_schema import EmailGroupCreate, EmailGroupUpdate, EmailGroup as EmailGroupResponse
from utils.jwt import get_current_user

router = APIRouter()

@router.get("", response_model=List[EmailGroupResponse])
async def list_email_groups(
    skip: int = 0,
    limit: int = 100,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """List all email groups"""
    if not current_user.has_permission("email_groups:view"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    groups = db.query(EmailGroup).offset(skip).limit(limit).all()
    return [group.to_response() for group in groups]

@router.get("/{group_id}", response_model=EmailGroupResponse)
async def get_email_group(
    group_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get a specific email group by ID"""
    if not current_user.has_permission("email_groups:view"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    group = db.query(EmailGroup).filter(EmailGroup.id == group_id).first()
    if not group:
        raise HTTPException(status_code=404, detail="Email group not found")
    return group.to_response()

@router.post("", response_model=EmailGroupResponse)
async def create_email_group(
    group_data: EmailGroupCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Create a new email group"""
    if not current_user.has_permission("email_groups:create"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    # Check if group with same name exists
    existing = db.query(EmailGroup).filter(EmailGroup.name == group_data.name).first()
    if existing:
        raise HTTPException(status_code=400, detail="Email group with this name already exists")
    
    # Create new group
    new_group = EmailGroup(
        name=group_data.name,
        description=group_data.description,
    )
    
    # Add email addresses
    for email in group_data.email_addresses:
        new_group.email_addresses.append(EmailAddress(email=str(email)))
    
    db.add(new_group)
    db.commit()
    db.refresh(new_group)
    return new_group.to_response()

@router.put("/{group_id}", response_model=EmailGroupResponse)
async def update_email_group(
    group_id: int,
    group_data: EmailGroupUpdate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Update an existing email group"""
    if not current_user.has_permission("email_groups:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    group = db.query(EmailGroup).filter(EmailGroup.id == group_id).first()
    if not group:
        raise HTTPException(status_code=404, detail="Email group not found")
    
    # Update basic fields
    group.name = group_data.name
    group.description = group_data.description
    
    # Update email addresses
    # Remove existing addresses
    for addr in group.email_addresses:
        db.delete(addr)
    
    # Add new addresses
    for email in group_data.email_addresses:
        group.email_addresses.append(EmailAddress(email=str(email)))
    
    db.commit()
    db.refresh(group)
    return group.to_response()

@router.delete("/{group_id}")
async def delete_email_group(
    group_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Delete an email group"""
    if not current_user.has_permission("email_groups:delete"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    group = db.query(EmailGroup).filter(EmailGroup.id == group_id).first()
    if not group:
        raise HTTPException(status_code=404, detail="Email group not found")
    
    db.delete(group)
    db.commit()
    return {"message": f"Email group '{group.name}' deleted successfully"}

@router.post("/{group_id}/roles")
async def assign_roles_to_group(
    group_id: int,
    role_ids: List[int],
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Assign roles to an email group"""
    if not current_user.has_permission("email_groups:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    group = db.query(EmailGroup).filter(EmailGroup.id == group_id).first()
    if not group:
        raise HTTPException(status_code=404, detail="Email group not found")
    
    roles = db.query(Role).filter(Role.id.in_(role_ids)).all()
    if len(roles) != len(role_ids):
        raise HTTPException(status_code=400, detail="One or more invalid role IDs")
    
    group.roles = roles
    db.commit()
    return {"message": "Roles assigned successfully"} 

================================================================================

Filename: routes/rbac.py
Content:
from typing import List

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from configs.database import get_db
from models.rbac_model import Role, Permission
from models.users_model import User
from schemas.rbac_schema import (
    RoleCreate, RoleUpdate, RoleResponse,
    PermissionCreate, PermissionUpdate, PermissionResponse, RolePermissionUpdate
)
from utils.jwt import get_current_user

router = APIRouter()


# Role routes
@router.get("/roles", response_model=List[RoleResponse])
async def list_roles(
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("roles:view"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return [role.to_response() for role in db.query(Role).all()]


@router.post("/roles", response_model=RoleResponse)
async def create_role(
        role_data: RoleCreate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("roles:create"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")

    existing_role = db.query(Role).filter(Role.name == role_data.name).first()
    if existing_role:
        raise HTTPException(status_code=400, detail="Role already exists")

    new_role = Role(**role_data.model_dump())
    db.add(new_role)
    db.commit()
    db.refresh(new_role)
    return new_role.to_response()


@router.put("/roles/{role_name}", response_model=RoleResponse)
async def update_role(
        role_name: str,
        role_data: RoleUpdate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("roles:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")

    role = db.query(Role).filter(Role.name == role_name).first()
    if not role:
        raise HTTPException(status_code=404, detail="Role not found")

    for key, value in role_data.model_dump(exclude_unset=True).items():
        setattr(role, key, value)

    db.commit()
    db.refresh(role)
    return role.to_response()


# Permission routes
@router.get("/permissions", response_model=List[PermissionResponse])
async def list_permissions(
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("permissions:view"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return [perm.to_response() for perm in db.query(Permission).all()]


@router.post("/permissions", response_model=PermissionResponse)
async def create_permission(
        permission_data: PermissionCreate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("permissions:create"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")

    existing_permission = db.query(Permission).filter(Permission.name == permission_data.name).first()
    if existing_permission:
        raise HTTPException(status_code=400, detail="Permission already exists")

    new_permission = Permission(**permission_data.model_dump())
    db.add(new_permission)
    db.commit()
    db.refresh(new_permission)
    return new_permission.to_response()


@router.put("/permissions/{permission_name}", response_model=PermissionResponse)
async def update_permission(
        permission_name: str,
        permission_data: PermissionUpdate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("permissions:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")

    permission = db.query(Permission).filter(Permission.name == permission_name).first()
    if not permission:
        raise HTTPException(status_code=404, detail="Permission not found")

    for key, value in permission_data.model_dump(exclude_unset=True).items():
        setattr(permission, key, value)

    db.commit()
    db.refresh(permission)
    return permission.to_response()


@router.put("/roles/{role_name}/permissions", response_model=RoleResponse)
async def update_role_permissions(
        role_name: str,
        permission_update: RolePermissionUpdate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    """Update permissions for a specific role"""
    if not current_user.has_permission("roles:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")

    # Get the role
    role = db.query(Role).filter(Role.name == role_name).first()
    if not role:
        raise HTTPException(status_code=404, detail="Role not found")

    # Add new permissions
    if permission_update.add_permissions:
        permissions_to_add = db.query(Permission).filter(
            Permission.name.in_(permission_update.add_permissions)
        ).all()

        if len(permissions_to_add) != len(permission_update.add_permissions):
            raise HTTPException(
                status_code=400,
                detail="One or more permission names are invalid"
            )

        for permission in permissions_to_add:
            if permission not in role.permissions:
                role.permissions.append(permission)

    # Remove permissions
    if permission_update.remove_permissions:
        permissions_to_remove = db.query(Permission).filter(
            Permission.name.in_(permission_update.remove_permissions)
        ).all()

        for permission in permissions_to_remove:
            if permission in role.permissions:
                role.permissions.remove(permission)

    db.commit()
    db.refresh(role)
    return role.to_response()


================================================================================

Filename: routes/storage.py
Content:
import io
from typing import Optional, List
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from fastapi.responses import StreamingResponse
from controllers.storage_controller import StorageController
from models.users_model import User
from schemas.storage_schema import FileResponse, FileListResponse, DeleteResponse, ImageTransformParams
from utils.jwt import get_current_user


# Create router instance with prefix and tags
router = APIRouter()


# Dependency injection for StorageController
def get_storage_controller():
    return StorageController("alert.ventory")


@router.post("/upload/",
             response_model=FileResponse,
             summary="Upload a file",
             description="Upload a file to Supabase storage"
             )
async def upload_file(
        file: UploadFile = File(...),
        storage: StorageController = Depends(get_storage_controller),
        current_user: User = Depends(get_current_user)
):
    if not current_user.has_permission("storage:write"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    """
    Upload a file to storage:
    - Accepts any file type
    - Returns file information including public URL
    """
    return await storage.upload_file(file, current_user)


@router.get("/files/",
            response_model=List[FileListResponse],
            summary="List files",
            description="List all files in storage"
            )
async def list_files(
        path: Optional[str] = None,
        storage: StorageController = Depends(get_storage_controller),
        current_user: User = Depends(get_current_user)
):
    if not current_user.has_permission("storage:read"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    """
    List all files in storage:
    - Optional path parameter for specific directories
    - Returns list of file information
    """
    files = await storage.list_files(path=path, current_user=current_user)
    return [FileListResponse.from_supabase_response(file) for file in files]


@router.get("/files/{filename}",
            summary="Download file",
            description="Download a specific file"
            )
async def download_file(
        filename: str,
        storage: StorageController = Depends(get_storage_controller),
        current_user: User = Depends(get_current_user)
):
    """
    Download a specific file:
    - Streams file content
    - Sets proper content disposition for download
    """
    file_content = await storage.get_file(filename)
    return StreamingResponse(
        io.BytesIO(file_content),
        media_type="application/octet-stream",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


@router.delete("/files/{filename}",
               response_model=DeleteResponse,
               summary="Delete file",
               description="Delete a specific file"
               )
async def delete_file(
        filename: str,
        storage: StorageController = Depends(get_storage_controller),
        current_user: User = Depends(get_current_user)
):
    if not current_user.has_permission("storage:delete"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    """
    Delete a specific file:
    - Returns success message if file is deleted
    - Raises 404 if file not found
    """
    return await storage.delete_file(filename)


@router.get("/images/{filename}",
            summary="Get transformed image",
            description="Get an image with optional transformations"
            )
async def get_transformed_image(
        filename: str,
        params: ImageTransformParams = Depends(),
        storage: StorageController = Depends(get_storage_controller),
        current_user: User = Depends(get_current_user)
):
    """
    Get an image with transformations:
    - Optional width and height parameters
    - Returns transformed image URL
    """
    url = await storage.get_image_url(filename, params.width, params.height, params.format, params.quality)
    return {"url": url}


================================================================================

Filename: routes/tab.py
Content:
import uuid
from typing import List
from fastapi import APIRouter, Depends, HTTPException, status, Response
from sqlalchemy.orm import Session

from configs.database import get_db
from utils.jwt import get_current_user
from models.users_model import User
from models.tab_model import Tab
from schemas.tab_schema import TabCreate, TabUpdate, TabResponse
from controllers import tab_controller

router = APIRouter()


@router.post("/", response_model=TabResponse, status_code=status.HTTP_201_CREATED)
async def create_new_tab(
        tab_data: TabCreate,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    """Create a new tab entry."""
    if not current_user.has_permission("tabs:create"):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions to create tabs")
    return await tab_controller.create_tab(tab_data=tab_data, db=db, current_user=current_user)


@router.get("/", response_model=List[TabResponse])
async def list_tabs(
        skip: int = 0,
        limit: int = 100,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    """Retrieve a list of tabs for the current user."""
    if not current_user.has_permission("tabs:view"):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions to view tabs")
    tabs = await tab_controller.get_tabs(db=db, current_user=current_user, skip=skip, limit=limit)
    return tabs  # Controller returns list of Tab models, FastAPI handles conversion via response_model


@router.get("/{tab_id}", response_model=TabResponse)
async def read_tab(
        tab_id: uuid.UUID,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    """Retrieve a specific tab by its ID."""
    if not current_user.has_permission("tabs:view"):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions to view tabs")
    # Ownership is checked within the controller function get_tab
    tab = await tab_controller.get_tab(tab_id=tab_id, db=db, current_user=current_user)
    return tab


@router.put("/{tab_id}", response_model=TabResponse)
async def update_existing_tab(
        tab_id: uuid.UUID,
        tab_data: TabUpdate,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    """Update a specific tab by its ID."""
    if not current_user.has_permission("tabs:edit"):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions to edit tabs")
    # Ownership is checked within the controller function update_tab
    updated_tab = await tab_controller.update_tab(tab_id=tab_id, tab_data=tab_data, db=db, current_user=current_user)
    return updated_tab


@router.delete("/{tab_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_existing_tab(
        tab_id: uuid.UUID,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    """Delete a specific tab by its ID."""
    if not current_user.has_permission("tabs:delete"):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions to delete tabs")
    # Ownership is checked within the controller function delete_tab
    await tab_controller.delete_tab(tab_id=tab_id, db=db, current_user=current_user)
    return Response(status_code=status.HTTP_204_NO_CONTENT)  # Return explicit No Content response


================================================================================

Filename: routes/users.py
Content:
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from configs.database import get_db
from controllers.user_controller import (
    create_user, get_users, get_user,
    update_user, delete_user, update_user_roles
)
from models.users_model import User
from schemas.user_schema import UserCreate, UserUpdate, UserResponse, UserRoleUpdate
from utils.jwt import get_current_user

router = APIRouter()


@router.post("", response_model=UserResponse)
async def create_new_user(
        user_data: UserCreate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("users:create"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return await create_user(user_data, db)


@router.get("", response_model=List[UserResponse])
async def list_users(
        skip: int = 0,
        limit: int = 100,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("users:view"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    users = await get_users(db, skip, limit)
    return [user.to_response() for user in users]


@router.get("/{email}", response_model=UserResponse)
async def get_user_by_email(
        email: str,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not (current_user.has_permission("users:view") or current_user.email == email):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return await get_user(email, db)


@router.put("/{email}", response_model=UserResponse)
async def update_user_by_email(
        email: str,
        user_data: UserUpdate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not (current_user.has_permission("users:edit") or current_user.email == email):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return await update_user(email, user_data, db)


@router.put("/{email}/roles", response_model=UserResponse)
async def update_user_roles_by_email(
        email: str,
        role_update: UserRoleUpdate,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("roles:edit"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return await update_user_roles(db, role_update, email)


@router.delete("/{email}")
async def delete_user_by_email(
        email: str,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    if not current_user.has_permission("users:delete"):
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    return await delete_user(email, db)


================================================================================

Filename: routes/workflow.py
Content:
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from configs.database import get_db
from controllers.workflow_controller import WorkflowController
from models.users_model import User
from utils.jwt import get_current_user

router = APIRouter()


@router.get("/")
async def list_workflows(
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    """List all workflows for the current user"""
    if not current_user.has_permission("workflow:view"):
        raise HTTPException(
            status_code=403,
            detail="Insufficient permissions to view workflows"
        )

    workflows = await WorkflowController.get_workflows(db, current_user)
    return [workflow.to_response() for workflow in workflows]


@router.delete("/{workflow_id}")
async def terminate_workflow(
        workflow_id: str,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    """Terminate a specific workflow"""
    if not current_user.has_permission("workflow:terminate"):
        raise HTTPException(
            status_code=403,
            detail="Insufficient permissions to terminate workflows"
        )

    return await WorkflowController.terminate_workflow(workflow_id, current_user, db)


@router.get("/{workflow_id}/status")
async def check_workflow_status(
        workflow_id: str,
        current_user: User = Depends(get_current_user)
):
    """Check the status of a workflow by its ID"""
    if not current_user.has_permission("workflow:view"):
        raise HTTPException(
            status_code=403,
            detail="Insufficient permissions to view workflow status"
        )

    return await WorkflowController.check_workflow_status(workflow_id, current_user)


================================================================================

Filename: run_api.py
Content:
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from configs.logging_config import RequestLoggingMiddleware
from configs.modes_config import (
    APP_RUN_MODE,
    ENABLE_DOCS,
    ACTIVE_ROUTER_KEYS,
    FASTAPI_TITLE,
    FASTAPI_DESCRIPTION,
    FASTAPI_VERSION
)
from models import init_db
from routes import (
    auth_router,
    users_router,
    email_router,
    storage_router,
    rbac_router,
    email_groups_router,
    workflow_router,
    tab_router,
    scheduled_email_router,
    recurring_email_router
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()
    print(f"Application running in '{APP_RUN_MODE}' mode.")
    if ENABLE_DOCS:
        print(f"API docs available at /docs and /redoc")
    else:
        print(f"API docs are disabled.")
    print(f"Active router keys: {ACTIVE_ROUTER_KEYS}")
    yield


# Initialize FastAPI application with conditional docs
if ENABLE_DOCS:
    app = FastAPI(
        title=FASTAPI_TITLE,
        description=FASTAPI_DESCRIPTION,
        version=FASTAPI_VERSION,
        lifespan=lifespan
    )
else:
    app = FastAPI(
        title=FASTAPI_TITLE,
        description=FASTAPI_DESCRIPTION,
        version=FASTAPI_VERSION,
        lifespan=lifespan,
        docs_url=None,
        redoc_url=None
    )

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(RequestLoggingMiddleware)

# --- Router Mapping ---
ROUTER_MAPPING = {
    "auth": {"router": auth_router, "prefix": "/auth", "tags": ["auth"]},
    "users": {"router": users_router, "prefix": "/users", "tags": ["users"]},
    "emails": {"router": email_router, "prefix": "/emails", "tags": ["emails"]},
    "storage": {"router": storage_router, "prefix": "/storage", "tags": ["storage"],
                "responses": {404: {"description": "Not found"}}},
    "rbac": {"router": rbac_router, "prefix": "/rbac", "tags": ["rbac"]},
    "email_groups": {"router": email_groups_router, "prefix": "/email-groups", "tags": ["email-groups"]},
    "workflows": {"router": workflow_router, "prefix": "/workflows", "tags": ["workflows"]},
    "tabs": {"router": tab_router, "prefix": "/tabs", "tags": ["tabs"],
             "responses": {404: {"description": "Not found"}}},
    "scheduled_emails": {"router": scheduled_email_router, "prefix": "/scheduled-emails", "tags": ["Scheduled Emails"]},
    "recurring_emails": {"router": recurring_email_router, "prefix": "/recurring-emails", "tags": ["Recurring Emails"]},
}
# --- End Router Mapping ---

# --- Dynamically Include Routers ---
for router_key in ACTIVE_ROUTER_KEYS:
    if router_key in ROUTER_MAPPING:
        router_config = ROUTER_MAPPING[router_key]
        app.include_router(
            router=router_config["router"],
            prefix=router_config["prefix"],
            tags=router_config["tags"],
            responses=router_config.get("responses")
        )
        print(f"Included router: {router_key} at prefix {router_config['prefix']}")
    else:
        print(f"Warning: Router key '{router_key}' defined in modes_config.json but not found in ROUTER_MAPPING.")
# --- End Dynamically Include Routers ---

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8124)


================================================================================

Filename: run_worker.py
Content:
# Standard library imports
import asyncio

# Local worker imports
from workers.email_workers import start_email_workers

# Entry point for worker processes
if __name__ == "__main__":
    try:
        # Start the email worker processes using asyncio
        # This runs the worker in an event loop for handling async operations
        asyncio.run(start_email_workers())
    except KeyboardInterrupt:
        # Handle graceful shutdown on Ctrl+C
        print("\nShutting down email workers...")


================================================================================

Filename: schemas/__init__.py
Content:


================================================================================

Filename: schemas/auth_schema.py
Content:
from pydantic import BaseModel


class AuthRequest(BaseModel):
    auth_code: str


class RefreshRequest(BaseModel):
    refresh_token: str


================================================================================

Filename: schemas/email_schema.py
Content:
from pydantic import BaseModel, EmailStr
from enum import Enum
from typing import List, Optional, Union


class EmailPriority(str, Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class EmailCreate(BaseModel):
    subject: str
    body: str
    to_emails: Optional[List[Union[EmailStr, str]]] = []  # Can be either email addresses or group names
    priority: EmailPriority = EmailPriority.MEDIUM
    requires_verification: bool = False
    cc: Optional[List[Union[EmailStr, str]]] = []  # Can be either email addresses or group names
    bcc: Optional[List[Union[EmailStr, str]]] = []  # Can be either email addresses or group names


class EmailConfirmation(BaseModel):
    workflow_id: str
    confirm: bool

================================================================================

Filename: schemas/rbac_schema.py
Content:
from typing import List, Optional

from pydantic import BaseModel, EmailStr, ConfigDict, Field


class RoleBase(BaseModel):
    name: str
    description: Optional[str] = None


class RoleCreate(RoleBase):
    pass


class RoleUpdate(RoleBase):
    pass


class RoleResponse(RoleBase):
    id: int
    permissions: List[str]  # List of permission names

    model_config = ConfigDict(from_attributes=True)


class PermissionBase(BaseModel):
    name: str
    description: Optional[str] = None


class PermissionCreate(PermissionBase):
    pass


class PermissionUpdate(PermissionBase):
    pass


class PermissionResponse(PermissionBase):
    id: int

    model_config = ConfigDict(from_attributes=True)


class RolePermissionUpdate(BaseModel):
    add_permissions: Optional[List[str]] = None  # Permission names to add
    remove_permissions: Optional[List[str]] = None  # Permission names to remove


class EmailGroupBase(BaseModel):
    name: str
    email_addresses: List[EmailStr]
    description: Optional[str] = None


class EmailGroupCreate(EmailGroupBase):
    pass


class EmailGroupUpdate(EmailGroupBase):
    pass


class EmailGroup(BaseModel):
    id: int
    name: str = Field(..., pattern=r'^[a-zA-Z0-9\-]+$', description="Group name allowing only alphanumeric characters and hyphens")
    description: Optional[str] = None
    email_addresses: List[str]
    roles: Optional[List[str]] = None

    model_config = ConfigDict(from_attributes=True)


================================================================================

Filename: schemas/storage_schema.py
Content:
from typing import Optional
from pydantic import BaseModel


class FileResponse(BaseModel):
    filename: str
    original_filename: str
    url: str
    size: int


class ImageTransformParams(BaseModel):
    format: Optional[str] = "origin"
    quality: Optional[int] = 100
    width: Optional[int] = None
    height: Optional[int] = None
    resize: Optional[str] = "cover"


class FileListResponse(BaseModel):
    name: str
    size: int
    created_at: str
    updated_at: str

    # This method will help parse the `metadata`.
    @classmethod
    def from_supabase_response(cls, item: dict):
        # Extract the size from the metadata
        size = item['metadata']['size']
        return cls(
            name=item['name'],
            size=size,
            created_at=item['created_at'],
            updated_at=item['updated_at']
        )


class DeleteResponse(BaseModel):
    message: str


================================================================================

Filename: schemas/tab_schema.py
Content:
import uuid
from datetime import datetime
from typing import Optional, List, Any
from pydantic import BaseModel, Field, ConfigDict

# Shared properties
class TabBase(BaseModel):
    subject: Optional[str] = None
    from_address: Optional[str] = None
    to_address: Optional[List[Any]] = [] # Expecting a list for JSON
    cc_address: Optional[List[Any]] = [] # Expecting a list for JSON
    bcc_address: Optional[List[Any]] = [] # Expecting a list for JSON
    body: Optional[str] = None

# Properties to receive via API on creation
class TabCreate(TabBase):
    # uid will be taken from the current user in the controller
    pass

# Properties to receive via API on update
class TabUpdate(TabBase):
    # All fields are optional in TabBase by default
    pass

# Properties stored in DB
class TabInDBBase(TabBase):
    tab_id: uuid.UUID
    uid: str
    created_at: datetime

    model_config = ConfigDict(from_attributes=True)

# Properties to return to client
class TabResponse(TabInDBBase):
    # Inherits all necessary fields from TabInDBBase
    pass 

================================================================================

Filename: schemas/user_schema.py
Content:
from typing import Optional, List

from pydantic import BaseModel, EmailStr, ConfigDict


class UserBase(BaseModel):
    email: EmailStr
    UserName: str
    profile_image_url: Optional[str] = None  # Added field


class UserCreate(UserBase):
    role_ids: Optional[List[int]]  # List of role IDs to assign
    default_role: str = "viewer"  # Default role if no roles specified


class UserUpdate(BaseModel):
    UserName: Optional[str] = None
    role_ids: Optional[List[int]] = None  # Allow updating role assignments
    profile_image_url: Optional[str] = None  # Added field

    model_config = ConfigDict(extra='ignore')


class UserRole(BaseModel):
    role_id: int


class UserRoleUpdate(BaseModel):
    add_roles: Optional[List[str]]  # Role names to add
    remove_roles: Optional[List[str]]  # Role names to remove


class UserResponse(UserBase):
    roles: List[str]  # List of role names
    permissions: List[str]  # List of permission names
    authorized_groups: List[str]  # List of email group names user can access
    # profile_image_url is inherited from UserBase

    model_config = ConfigDict(from_attributes=True)


class UserRoleAssignment(BaseModel):
    user_email: EmailStr
    role_ids: List[int]


class BulkRoleUpdate(BaseModel):
    user_emails: List[EmailStr]
    role_ids: List[int]
    operation: str = "add"  # "add" or "remove"


================================================================================

Filename: schemas/workflow_schema.py
Content:
from pydantic import BaseModel
from datetime import datetime


class WorkflowResponse(BaseModel):
    workflow_id: str
    uid: str
    created_at: datetime

    class Config:
        from_attributes = True


================================================================================

Filename: tests/README.md
Content:
# tests/

- Contains unit tests for workflows and activities.
- Use Temporal's test framework or mock out Temporal for unit testing.

================================================================================

Filename: tests/conftest.py
Content:
import os
import sys
from pathlib import Path

# Get the project root directory
root_dir = Path(__file__).parent.parent

# Add the project root to Python path
sys.path.append(str(root_dir))

================================================================================

Filename: tests/requirements-test.txt
Content:
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.24.1
fastapi==0.104.1
python-jose[cryptography]==3.3.0
python-multipart==0.0.6
requests==2.31.0
SQLAlchemy==2.0.23
pytest-mock==3.12.0

================================================================================

Filename: tests/test_auth.py
Content:
import pytest
import uuid
from datetime import datetime, timedelta, timezone
from unittest.mock import Mock, patch
from fastapi.testclient import TestClient
from sqlalchemy.orm import Session
from models.users_model import User
from utils.jwt import create_access_token, create_refresh_token, TokenType
from configs.jwt_config import ACCESS_TOKEN_EXPIRE_MINUTES, REFRESH_TOKEN_EXPIRE_DAYS
from configs.database import get_db
from run_api import app

"""
Test suite for authentication-related functionality.
This module tests JWT token creation, validation, and authentication endpoints.
"""

@pytest.fixture
def test_client():
    """
    Creates a FastAPI TestClient instance for making HTTP requests during tests.
    Returns:
        TestClient: A configured test client for the application
    """
    # Create a test client that can make requests to our FastAPI application
    return TestClient(app)

@pytest.fixture
def mock_db():
    """
    Creates a mock database session for testing.
    Returns:
        Mock: A mock object that simulates SQLAlchemy Session behavior
    """
    # Create a mock that follows the SQLAlchemy Session interface
    db = Mock(spec=Session)
    return db

@pytest.fixture
def mock_user():
    """
    Creates a mock user with basic permissions for testing.
    The user is configured with:
    - A random UUID
    - Test email and username
    - Basic access permissions
    
    Returns:
        User: A configured mock user object
    """
    # Generate a unique identifier for the test user
    unique_id = str(uuid.uuid4())
    
    # Create a test user instance with basic information
    user = User(
        uid=unique_id,
        email="test@example.com",
        UserName="Test User",
        profile_image_url=None  # Added field explicitly as None for clarity
    )
    
    # Configure permission mocks for basic access control testing
    # Lambda function returns True only for "basic_access" permission
    user.has_permission = lambda x: True if x == "basic_access" else False
    # Lambda function returns a set containing only "basic_access"
    user.get_all_permissions = lambda: {"basic_access"}
    # Initialize with empty roles list for basic testing
    user.roles = []
    return user

@pytest.fixture
def mock_zoho_token():
    """
    Provides mock Zoho OAuth tokens for testing.
    Returns:
        dict: Mock token data including access token, refresh token, and expiry
    """
    # Return a dictionary mimicking Zoho OAuth response structure
    return {
        "access_token": "mock_access_token",  # Simulated access token
        "refresh_token": "mock_refresh_token",  # Simulated refresh token
        "expires_in": 3600  # Token validity period in seconds
    }

def test_create_access_token(mock_user):
    """
    Tests the creation of JWT access tokens.
    Verifies that:
    - Token creation succeeds
    - Returns a string token
    """
    # Prepare token data with user email as subject
    token_data = {"sub": mock_user.email}
    # Generate the actual access token
    token = create_access_token(token_data)
    # Verify token is a string (valid JWT format)
    assert isinstance(token, str)

def test_create_refresh_token(mock_user):
    """
    Tests the creation of JWT refresh tokens.
    Verifies that:
    - Token creation succeeds
    - Returns a string token
    """
    # Prepare token data with user email as subject
    token_data = {"sub": mock_user.email}
    # Generate the actual refresh token
    token = create_refresh_token(token_data)
    # Verify token is a string (valid JWT format)
    assert isinstance(token, str)

@pytest.mark.asyncio
async def test_login_endpoint(test_client, mock_db, mock_user, mock_zoho_token):
    """
    Tests the login endpoint with Zoho OAuth integration.
    
    Test flow:
    1. Mocks database and Zoho token dependencies
    2. Simulates successful authentication
    3. Verifies response format and content
    
    Expected response:
    - 200 status code
    - Contains access_token, refresh_token
    - Bearer token type
    """
    # Override the database dependency with our mock
    app.dependency_overrides[get_db] = lambda: mock_db

    # Simulate Zoho OAuth token generation
    with patch('controllers.auth_controller.get_zoho_tokens') as mock_get_tokens:
        # Configure mock to return predefined token response
        mock_get_tokens.return_value = {
            "access_token": "mock_access_token",
            "refresh_token": "mock_refresh_token",
            "id_token": "mock.id.token",  # JWT containing user info
            "expires_in": 3600  # Token validity in seconds
        }
        
        # Configure database mock to return our test user
        mock_db.query.return_value.filter.return_value.first.return_value = mock_user
        
        # Mock the Zoho token decoder
        with patch('controllers.auth_controller.decode_zoho_token') as mock_decode:
            # Configure decoder to return user email
            mock_decode.return_value = {"email": mock_user.email}
            
            # Simulate login request with auth code
            response = test_client.post(
                "/auth/token",
                json={"auth_code": "test_auth_code"}
            )
            
            # Verify successful response
            assert response.status_code == 200
            data = response.json()
            # Check all required tokens are present
            assert "access_token" in data
            assert "refresh_token" in data
            assert "token_type" in data
            assert data["token_type"] == "bearer"

@pytest.mark.asyncio
async def test_refresh_token_endpoint(test_client, mock_db, mock_user):
    """
    Tests the token refresh endpoint functionality.
    
    Test flow:
    1. Creates a valid refresh token
    2. Attempts to exchange it for a new access token
    3. Verifies the response format and content
    
    Expected response:
    - 200 status code
    - New access token
    - Bearer token type
    """
    # Override database dependency for testing
    app.dependency_overrides[get_db] = lambda: mock_db
    
    # Create a valid refresh token for the test user
    refresh_token = create_refresh_token({"sub": mock_user.email})
    
    # Configure mock database to return our test user when queried
    mock_db.query.return_value.filter.return_value.first.return_value = mock_user
    
    # Send request to refresh token endpoint
    response = test_client.post(
        "/auth/refresh",
        json={"refresh_token": refresh_token}
    )
    
    # Verify successful response code
    assert response.status_code == 200
    data = response.json()
    # Verify response contains required token information
    assert "access_token" in data
    assert "token_type" in data
    assert data["token_type"] == "bearer"

@pytest.mark.asyncio
async def test_invalid_refresh_token(test_client):
    """
    Tests the behavior when an invalid refresh token is provided.
    
    Expected response:
    - 401 Unauthorized status code
    - Error detail in response
    """
    # Attempt to refresh with an obviously invalid token
    response = test_client.post(
        "/auth/refresh",
        json={"refresh_token": "invalid_token"}
    )
    
    # Verify unauthorized response
    assert response.status_code == 401
    # Verify error details are included in response
    assert "detail" in response.json()

@pytest.mark.asyncio
async def test_expired_tokens():
    """
    Tests the handling of expired tokens.
    
    Test flow:
    1. Creates an intentionally expired token
    2. Attempts to validate it
    3. Verifies that appropriate exception is raised
    
    Expected outcome:
    - Token validation should fail with expiration error
    """
    # Prepare test data for token creation
    token_data = {"sub": "test@example.com"}
    # Create a timedelta for token that expired 1 minute ago
    expires_delta = timedelta(minutes=-1)
    
    # Generate an expired access token
    expired_token = create_access_token(token_data, expires_delta)
    
    # Attempt to validate the expired token and expect an exception
    with pytest.raises(Exception) as exc_info:
        from utils.jwt import validate_token
        # Validate token should fail due to expiration
        await validate_token(expired_token, TokenType.ACCESS)
    
    # Verify the error message indicates token expiration
    assert "Token has expired" in str(exc_info.value)

================================================================================

Filename: tests/test_users_model.py
Content:
import pytest
from models.users_model import User
from models.rbac_model import Role, Permission, EmailGroup

@pytest.fixture
def test_permission():
    return Permission(
        id=1,
        name="test_permission",
        description="Test permission"
    )

@pytest.fixture
def test_email_group():
    return EmailGroup(
        id=1,
        name="test_group",
        description="Test email group"
    )

@pytest.fixture
def test_role(test_permission, test_email_group):
    role = Role(
        id=1,
        name="test_role",
        description="Test role"
    )
    role.permissions = [test_permission]
    role.email_groups = [test_email_group]
    return role

@pytest.fixture
def test_user(test_role):
    user = User(
        uid="test-uid",
        email="test@example.com",
        UserName="Test User",
        profile_image_url="http://example.com/image.jpg"
    )
    user.roles = [test_role]
    return user

def test_user_creation(test_user):
    assert test_user.uid == "test-uid"
    assert test_user.email == "test@example.com"
    assert test_user.UserName == "Test User"
    assert test_user.profile_image_url == "http://example.com/image.jpg"

def test_has_permission(test_user, test_permission):
    assert test_user.has_permission(test_permission.name) is True
    assert test_user.has_permission("non_existent_permission") is False

def test_can_send_to_group(test_user, test_email_group):
    assert test_user.can_send_to_group(test_email_group.name) is True
    assert test_user.can_send_to_group("non_existent_group") is False

def test_get_all_permissions(test_user, test_permission):
    permissions = test_user.get_all_permissions()
    assert isinstance(permissions, set)
    assert test_permission.name in permissions
    assert len(permissions) == 1

def test_get_authorized_groups(test_user, test_email_group):
    groups = test_user.get_authorized_groups()
    assert isinstance(groups, set)
    assert test_email_group.name in groups
    assert len(groups) == 1

def test_get_role_names(test_user, test_role):
    role_names = test_user.get_role_names()
    assert isinstance(role_names, list)
    assert test_role.name in role_names
    assert len(role_names) == 1

def test_to_response(test_user, test_role, test_permission, test_email_group):
    response = test_user.to_response()
    assert isinstance(response, dict)
    assert response["email"] == test_user.email
    assert response["UserName"] == test_user.UserName
    assert response["roles"] == [test_role.name]
    assert response["permissions"] == [test_permission.name]
    assert response["authorized_groups"] == [test_email_group.name]
    assert response["profile_image_url"] == test_user.profile_image_url

================================================================================

Filename: tree.txt
Content:
.
|-- LICENSE
|-- README.md
|-- activities
|   |-- README.md
|   |-- email_activity.py
|   `-- print_hello.py
|-- configs
|   |-- README.md
|   |-- __init__.py
|   |-- admin.json
|   |-- database.py
|   |-- jwt_config.py
|   |-- logging_config.py
|   |-- roles_permissions.json
|   `-- smtp_config.py
|-- controllers
|   |-- __init__.py
|   |-- auth_controller.py
|   |-- email_controller.py
|   |-- storage_controller.py
|   `-- user_controller.py
|-- docker
|   |-- Dockerfile
|   |-- [old]docker-compose-temporal.yml
|   |-- docker-compose.yml
|   |-- dynamicconfig
|   |   |-- README.md
|   |   |-- development-cass.yaml
|   |   |-- development-sql.yaml
|   |   `-- docker.yaml
|   |-- prometheus
|   |   `-- prometheus.yml
|   |-- readme.md
|   `-- volumes
|       |-- api
|       |   `-- kong.yml
|       |-- db
|       |   |-- _supabase.sql
|       |   |-- data  [error opening dir]
|       |   |-- init
|       |   |   `-- data.sql
|       |   |-- jwt.sql
|       |   |-- logs.sql
|       |   |-- pooler.sql
|       |   |-- realtime.sql
|       |   |-- roles.sql
|       |   `-- webhooks.sql
|       |-- functions
|       |   |-- hello
|       |   |   `-- index.ts
|       |   `-- main
|       |       `-- index.ts
|       |-- logs
|       |   `-- vector.yml
|       |-- pooler
|       |   `-- pooler.exs
|       `-- storage
|           `-- stub
|               `-- stub
|                   |-- alert.ventory
|                   |   `-- 0f0830ac-6b16-46a0-8c04-e12bebbf4695.png
|                   |       `-- 3e460d4f-3390-4a5e-a7b8-e147c8e59444
|                   `-- test
|                       `-- 1734598824671\ (1)\ (Large).png
|                           |-- 6114eb74-0f9d-4a4d-8f4d-f1df7cf8634f
|                           `-- 6114eb74-0f9d-4a4d-8f4d-f1df7cf8634f.json
|-- docker.zip
|-- handlers
|   |-- __init__.py
|   `-- exception.py
|-- logs
|   |-- account
|   |   |-- email.log
|   |   `-- send-email.log
|   |-- auth
|   |   |-- refresh.log
|   |   `-- token.log
|   |-- docs
|   |   `-- index.log
|   |-- email-groups
|   |   |-- 1.log
|   |   `-- index.log
|   |-- emails
|   |   |-- confirm.log
|   |   `-- send.log
|   |-- favicon.ico
|   |   `-- index.log
|   |-- index.log
|   |-- openapi.json
|   |   `-- index.log
|   |-- rbac
|   |   |-- permissions.log
|   |   `-- roles.log
|   |-- readme.md
|   |-- storage
|   |   |-- files.log
|   |   |-- images.log
|   |   `-- upload.log
|   `-- users
|       |-- gautam.garg@grigtechnologies.com.log
|       |-- index.log
|       |-- rudr1a@gmail.com.log
|       `-- rudra@gmail.com.log
|-- main.py
|-- models
|   |-- __init__.py
|   |-- base.py
|   |-- rbac_model.py
|   `-- users_model.py
|-- requirements.txt
|-- routes
|   |-- __init__.py
|   |-- auth.py
|   |-- email.py
|   |-- email_groups.py
|   |-- rbac.py
|   |-- storage.py
|   `-- users.py
|-- schemas
|   |-- __init__.py
|   |-- auth_schema.py
|   |-- email_schema.py
|   |-- rbac_schema.py
|   |-- storage_schema.py
|   `-- user_schema.py
|-- tests
|   `-- README.md
|-- tree.txt
|-- utils
|   |-- __init__.py
|   |-- jwt.py
|   `-- zoho_token.py
|-- workers
|   |-- README.md
|   |-- email_workers.py
|   |-- hello_world_worker.py
|   `-- run_worker.py
`-- workflows
    |-- README.md
    |-- email_workflow.py
    `-- hello_world_workflow.py

42 directories, 99 files


================================================================================

Filename: utils/__init__.py
Content:


================================================================================

Filename: utils/jwt.py
Content:
from datetime import datetime, timedelta, timezone
import jwt
from fastapi import Depends, HTTPException, Security
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from sqlalchemy.orm import Session
from configs.database import get_db
from configs.jwt_config import SECRET_KEY, ALGORITHM, ACCESS_TOKEN_EXPIRE_MINUTES, REFRESH_TOKEN_EXPIRE_DAYS
from models.users_model import User
from enum import Enum
from typing import Optional, Type

# Initialize FastAPI's security scheme for Bearer token authentication
security = HTTPBearer()


class TokenType(Enum):
    """Enum defining the types of JWT tokens supported by the system"""
    ACCESS = "access"    # Short-lived token for API access
    REFRESH = "refresh"  # Long-lived token for obtaining new access tokens


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a new access token.
    Args:
        data (dict): The data to encode in the token
        expires_delta (timedelta, optional): Custom expiration time
    Returns:
        str: The encoded JWT token
    """
    # Create a copy to avoid modifying the original data
    to_encode = data.copy()
    # Add token type identifier for validation
    to_encode["token_type"] = TokenType.ACCESS.value
    
    # Calculate token expiration time
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        # Use default expiration time from config
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    
    # Add expiration timestamp to token payload
    to_encode.update({"exp": int(expire.timestamp())})
    
    # Encode and return the JWT token
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    # Handle both string and bytes return types from different jwt versions
    return encoded_jwt.decode() if isinstance(encoded_jwt, bytes) else encoded_jwt


def create_refresh_token(data: dict) -> str:
    """
    Create a new refresh token.
    Args:
        data (dict): The data to encode in the token
    Returns:
        str: The encoded JWT refresh token
    """
    # Create a copy of the input data
    to_encode = data.copy()
    # Mark as refresh token
    to_encode["token_type"] = TokenType.REFRESH.value
    # Set refresh token expiration (longer than access token)
    refresh_expire = datetime.now(timezone.utc) + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    # Add expiration timestamp
    to_encode.update({"exp": int(refresh_expire.timestamp())})
    # Create and return the JWT token
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt.decode() if isinstance(encoded_jwt, bytes) else encoded_jwt


async def get_current_user(
        token: HTTPAuthorizationCredentials = Security(security),
        db: Session = Depends(get_db)
) -> Type[User]:
    """
    Validate the access token and return the current user.
    Args:
        token (HTTPAuthorizationCredentials): The Bearer token credentials
        db (Session): The database session
    Returns:
        User: The current authenticated user
    Raises:
        HTTPException: If token is invalid, expired, or user has no access
    """
    # Define standard authentication error
    credentials_exception = HTTPException(
        status_code=401,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    try:
        # Validate token and get payload
        payload = await validate_token(token.credentials, required_type=TokenType.ACCESS)
        # Extract user email from token subject
        email = payload.get("sub")
        if email is None:
            raise credentials_exception

        # Lookup user in database
        user = db.query(User).filter(User.email == email).first()
        if user is None:
            raise credentials_exception

        return user
    except HTTPException:
        # Wrap all authentication errors in standard exception
        raise credentials_exception


async def validate_token(token: str, required_type: TokenType = TokenType.ACCESS) -> dict:
    """
    Validate a token without checking user access.
    Args:
        token (str): The JWT token to validate
        required_type (TokenType): The required token type
    Returns:
        dict: The decoded token payload
    Raises:
        HTTPException: If token is invalid or expired
    """
    try:
        # Attempt to decode and verify the token
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])

        # Verify the token is of the correct type
        token_type = payload.get("token_type")
        if token_type != required_type.value:
            raise HTTPException(
                status_code=401,
                detail=f"Invalid token type. Expected {required_type.value} token."
            )

        # Check token expiration
        exp = payload.get("exp")
        if exp:
            token_expire = datetime.fromtimestamp(exp, tz=timezone.utc)
            current_time = datetime.now(timezone.utc)

            if current_time >= token_expire:
                raise HTTPException(
                    status_code=401,
                    detail="Token has expired"
                )

        return payload

    except jwt.ExpiredSignatureError:
        # Handle expired tokens
        raise HTTPException(
            status_code=401,
            detail="Token has expired"
        )
    except jwt.InvalidTokenError:
        # Handle invalid tokens (malformed, wrong signature, etc.)
        raise HTTPException(
            status_code=401,
            detail="Invalid token"
        )


================================================================================

Filename: utils/zoho_token.py
Content:
import base64
import json

from fastapi import HTTPException, status


def decode_zoho_token(token: str):
    """
    Decodes and validates a Zoho JWT token.
    
    Args:
        token (str): The JWT token string from Zoho OAuth
    
    Returns:
        dict: Decoded token payload containing user information
    
    Raises:
        HTTPException: When token is invalid or malformed
    """
    try:
        # Split the JWT token into header, payload, and signature
        # JWT format: header.payload.signature
        token_parts = token.split('.')
        
        # Validate token structure (must have 3 parts)
        if len(token_parts) != 3:
            raise ValueError("Invalid token format")

        # Extract and decode the payload (middle part)
        payload_encoded = token_parts[1]
        # Add padding ('==') for proper base64 decoding
        payload_decoded = base64.urlsafe_b64decode(payload_encoded + '==')
        # Parse the JSON payload into a Python dictionary
        decoded_token = json.loads(payload_decoded)

        return decoded_token
        
    except (ValueError, json.JSONDecodeError):
        # Handle both structural and decoding errors
        # ValueError: when token parts are invalid
        # JSONDecodeError: when payload is not valid JSON
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid Zoho token"
        )


================================================================================

Filename: workers/README.md
Content:
# workers/
 
- Worker modules dedicated to specific task queues.
- Ensures scalability as new workers can be added for different workloads.
- Allows you to deploy workers independently.

================================================================================

Filename: workers/email_workers.py
Content:
# Standard library imports for async operations and logging
import asyncio
import logging

# Environment configuration
import dotenv
from temporalio.client import Client
from temporalio.worker import Worker

# Local application imports
from activities.email_activity import send_email_activity, send_test_email_activity
from configs.temporal_config import get_temporal_client
from workflows.email_workflow import EmailWorkflow

# Load environment variables from docker configuration
dotenv.load_dotenv(dotenv_path='docker/.env')

# Configure logging for Temporal operations
# Format: timestamp level message
logging.basicConfig(
    format='%(asctime)s %(levelname)s %(message)s',
    level=logging.INFO
)


async def create_priority_worker(
        client: Client,
        priority: str,
        workflows: list,
        activities: list
):
    """
    Create a worker instance for handling emails of specific priority.
    
    Args:
        client (Client): Temporal client instance
        priority (str): Priority level (high/medium/low)
        workflows (list): List of workflow classes to register
        activities (list): List of activity functions to register
    """
    # Create unique task queue name for this priority
    task_queue = f"email_{priority}_priority"
    
    # Initialize worker with specified workflows and activities
    worker = Worker(
        client,
        task_queue=task_queue,
        workflows=workflows,
        activities=activities
    )
    
    print(f"Starting {priority} priority email worker on queue: {task_queue}")
    
    # Run worker indefinitely
    async with worker:
        # Keep worker alive until external shutdown
        await asyncio.Event().wait()


async def start_email_workers():
    """
    Initialize and start email workers for all priority levels.
    Sets up concurrent workers for high, medium, and low priority queues.
    """
    # Establish connection to Temporal server
    client = await get_temporal_client()

    # Register workflow and activity implementations
    workflows = [EmailWorkflow]
    activities = [send_email_activity, send_test_email_activity]
    # TODO: Make worker queue type .env based
    # Create separate workers for each priority level
    worker_tasks = [
        create_priority_worker(client, "high", workflows, activities),
        create_priority_worker(client, "medium", workflows, activities),
        create_priority_worker(client, "low", workflows, activities),
    ]
   
    print("Starting all email workers...")

    # Launch all workers concurrently
    await asyncio.gather(*worker_tasks)


================================================================================

Filename: workflows/README.md
Content:
# workflows/

- Contains all workflow definitions.
- Each workflow is its own module for better isolation and reusability.
- Keep workflows focused and small to facilitate scaling.

================================================================================

Filename: workflows/email_workflow.py
Content:
# Standard library imports
import json
from datetime import timedelta
import logging

# Temporal workflow imports
from temporalio import workflow
from temporalio.common import RetryPolicy

# Local activity imports
from activities.email_activity import send_email_activity, send_test_email_activity


@workflow.defn
class EmailWorkflow:
    """
    Temporal workflow for handling email sending process.
    Supports verification workflow and direct sending.
    """
    def __init__(self):
        # Initialize workflow state
        self._verified = False        # Track if email is verified
        self._cancelled = False       # Track if email is cancelled
        self._signal_received = False # Track if verification signal received

    @workflow.signal
    def verify_email(self, confirm: bool):
        """
        Signal handler for email verification.
        
        Args:
            confirm (bool): Whether to proceed with sending the email
        """
        workflow.logger.info(f"Received verification signal with confirm={confirm}")
        self._verified = confirm
        self._cancelled = not confirm  # Cancel if not confirmed
        self._signal_received = True   # Mark signal as received
        workflow.logger.info(f"Signal processed - verified: {self._verified}, cancelled: {self._cancelled}")

    def _check_verification_status(self) -> bool:
        """
        Check if verification process is complete.
        Returns True if either verified or cancelled.
        """
        status = self._signal_received and (self._verified or self._cancelled)
        workflow.logger.info(f"Checking verification status: {status}")
        return status

    @workflow.run
    async def run(self, args_json: str, requires_verification: bool = False) -> bool:
        """
        Main workflow execution logic.
        
        Args:
            args_json (str): JSON string containing email parameters
            requires_verification (bool): Whether to require manual verification
        
        Returns:
            bool: True if email was sent successfully
        """
        workflow.logger.info(f"Starting email workflow with args: {args_json}")

        # Parse email parameters
        args = json.loads(args_json)
        from_email = args["from_email"]
        to_emails = args["to_emails"]
        subject = args["subject"]
        body = args["body"]
        test_email = args["test_email"]

        # Configure retry policy for activities
        retry_policy = RetryPolicy(
            initial_interval=timedelta(seconds=1),
            maximum_interval=timedelta(seconds=10),
            maximum_attempts=3,
        )

        # Handle verification workflow if required
        if requires_verification:
            # Prepare test email parameters
            test_args = {
                "from_email": from_email,
                "to_emails": [test_email],
                "cc_emails": [],  # Empty for test email
                "bcc_emails": [],  # Empty for test email
                "subject": subject,
                "body": body
            }

            # Send test email and verify success
            test_result = await workflow.execute_activity(
                send_test_email_activity,
                json.dumps(test_args),
                schedule_to_close_timeout=timedelta(seconds=30),
                retry_policy=retry_policy
            )

            if not test_result:
                workflow.logger.error("Test email failed to send")
                return False

            workflow.logger.info("Waiting for verification signal...")

            # Wait for verification with enhanced logging
            try:
                workflow.logger.info("Current state before wait - "
                                   f"signal_received: {self._signal_received}, "
                                   f"verified: {self._verified}, "
                                   f"cancelled: {self._cancelled}")

                # Wait for verification signal with 24-hour timeout
                await workflow.wait_condition(
                    self._check_verification_status,
                    timeout=timedelta(hours=24)
                )

                workflow.logger.info("Wait condition satisfied - "
                                   f"signal_received: {self._signal_received}, "
                                   f"verified: {self._verified}, "
                                   f"cancelled: {self._cancelled}")

            except TimeoutError:
                workflow.logger.error("Verification timeout - cancelling email")
                return False

            # Handle verification results
            if self._cancelled:
                workflow.logger.info("Email cancelled by user")
                return False

            if not self._verified:
                workflow.logger.info("Email not verified")
                return False

        workflow.logger.info("Proceeding to send actual email")

        # Prepare final email parameters
        actual_args = {
            "from_email": from_email,
            "to_emails": to_emails,
            "cc_emails": args.get("cc_emails", []),
            "bcc_emails": args.get("bcc_emails", []),
            "subject": subject,
            "body": body
        }

        # Send the actual email
        result = await workflow.execute_activity(
            send_email_activity,
            json.dumps(actual_args),
            schedule_to_close_timeout=timedelta(seconds=30),
            retry_policy=retry_policy
        )

        workflow.logger.info(f"Email send result: {result}")
        return result

================================================================================
