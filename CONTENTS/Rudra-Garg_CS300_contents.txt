
Filename: .env
Content:
# .env file for Fog/Edge Simulation Latency Configuration

# --- Master Switch ---
# Set to "true" to enable latency simulation, "false" to disable.
ENABLE_LATENCY=true

# --- Latency Values ---
# Define latency for different network links.
# Format: <value>ms (e.g., 50ms, 10ms)
# These values represent ONE-WAY delay added by the sender.

# Delay added by Mobile device when sending to its Gateway
LATENCY_MOBILE_TO_GATEWAY=50ms

# Delay added by Gateway device when sending to the Proxy
LATENCY_GATEWAY_TO_PROXY=100ms

# Delay added by Proxy device when sending to the Cloud
LATENCY_PROXY_TO_CLOUD=300ms

# --- Optional: Jitter ---
# Add random variation to the delay (e.g., 5ms means delay +/- 5ms)
# Uncomment and set values if needed. Keep them commented out for simple delay.
JITTER_MOBILE_TO_GATEWAY=5ms
JITTER_GATEWAY_TO_PROXY=10ms
JITTER_PROXY_TO_CLOUD=30ms

# --- Optional: Packet Loss ---
# Add random packet loss percentage. Use with caution.
# Uncomment and set values if needed.
# LOSS_MOBILE_TO_GATEWAY=5%
# LOSS_GATEWAY_TO_PROXY=5%
# LOSS_PROXY_TO_CLOUD=2%

# --- Module Placement Levels ---
# Defines the *highest* module level to execute on this tier.
# 0 = Passthrough (Gateway/Proxy only)
# 1 = Client (Mobile only - always runs)
# 2 = Calculator
# 3 = Connector
# Example: Mobile=1, Gateway=2, Proxy=3 -> Client on Mobile, Calc on Gateway, Connector on Proxy
# Example: Mobile=2, Gateway=0, Proxy=3 -> Client+Calc on Mobile, Gateway Passthrough, Connector on Proxy
# Example: Mobile=3, Gateway=0, Proxy=0 -> Client+Calc+Connector on Mobile, Gateway/Proxy Passthrough
MOBILE_PROCESSING_LEVEL=1
GATEWAY_PROCESSING_LEVEL=2
PROXY_PROCESSING_LEVEL=3
CLOUD_PROCESSING_LEVEL=3

CLOUD_URL=http://cloud_py:8000/


================================================================================

Filename: .gitignore
Content:
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc
extract.py
repo_contents.txt


================================================================================

Filename: .idea/.gitignore
Content:
# Default ignored files
/shelf/
/workspace.xml
# Editor-based HTTP Client requests
/httpRequests/
# Datasource local storage ignored files
/dataSources/
/dataSources.local.xml


================================================================================

Filename: .idea/CS300.iml
Content:
<?xml version="1.0" encoding="UTF-8"?>
<module type="PYTHON_MODULE" version="4">
  <component name="NewModuleRootManager">
    <content url="file://$MODULE_DIR$">
      <excludeFolder url="file://$MODULE_DIR$/.venv" />
    </content>
    <orderEntry type="jdk" jdkName="Python 3.12 (CS300)" jdkType="Python SDK" />
    <orderEntry type="sourceFolder" forTests="false" />
  </component>
</module>

================================================================================

Filename: .idea/inspectionProfiles/Project_Default.xml
Content:
<component name="InspectionProjectProfileManager">
  <profile version="1.0">
    <option name="myName" value="Project Default" />
    <inspection_tool class="DuplicatedCode" enabled="true" level="WEAK WARNING" enabled_by_default="true">
      <Languages>
        <language minSize="51" name="Python" />
      </Languages>
    </inspection_tool>
    <inspection_tool class="Eslint" enabled="true" level="WARNING" enabled_by_default="true" />
    <inspection_tool class="JsonSchemaCompliance" enabled="false" level="WARNING" enabled_by_default="false" />
    <inspection_tool class="PyPackageRequirementsInspection" enabled="true" level="WARNING" enabled_by_default="true">
      <option name="ignoredPackages">
        <value>
          <list size="11">
            <item index="0" class="java.lang.String" itemvalue="typing_extensions" />
            <item index="1" class="java.lang.String" itemvalue="grpcio" />
            <item index="2" class="java.lang.String" itemvalue="Werkzeug" />
            <item index="3" class="java.lang.String" itemvalue="cryptography" />
            <item index="4" class="java.lang.String" itemvalue="Jinja2" />
            <item index="5" class="java.lang.String" itemvalue="idna" />
            <item index="6" class="java.lang.String" itemvalue="packaging" />
            <item index="7" class="java.lang.String" itemvalue="gunicorn" />
            <item index="8" class="java.lang.String" itemvalue="dotenv" />
            <item index="9" class="java.lang.String" itemvalue="flask" />
            <item index="10" class="java.lang.String" itemvalue="psycopg2-binary" />
          </list>
        </value>
      </option>
    </inspection_tool>
    <inspection_tool class="PyPep8Inspection" enabled="true" level="WEAK WARNING" enabled_by_default="true">
      <option name="ignoredErrors">
        <list>
          <option value="W605" />
        </list>
      </option>
    </inspection_tool>
    <inspection_tool class="PyPep8NamingInspection" enabled="true" level="WEAK WARNING" enabled_by_default="true">
      <option name="ignoredErrors">
        <list>
          <option value="N802" />
          <option value="N803" />
          <option value="N806" />
          <option value="N801" />
        </list>
      </option>
    </inspection_tool>
    <inspection_tool class="PyShadowingBuiltinsInspection" enabled="true" level="WEAK WARNING" enabled_by_default="true">
      <option name="ignoredNames">
        <list>
          <option value="str" />
        </list>
      </option>
    </inspection_tool>
    <inspection_tool class="PyUnresolvedReferencesInspection" enabled="true" level="WARNING" enabled_by_default="true">
      <option name="ignoredIdentifiers">
        <list>
          <option value="dict.*" />
        </list>
      </option>
    </inspection_tool>
    <inspection_tool class="SqlResolveInspection" enabled="true" level="ERROR" enabled_by_default="true">
      <option name="suppressForPossibleStringLiterals" value="true" />
    </inspection_tool>
  </profile>
</component>

================================================================================

Filename: .idea/inspectionProfiles/profiles_settings.xml
Content:
<component name="InspectionProjectProfileManager">
  <settings>
    <option name="USE_PROJECT_PROFILE" value="false" />
    <version value="1.0" />
  </settings>
</component>

================================================================================

Filename: .idea/misc.xml
Content:
<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="Black">
    <option name="sdkName" value="Python 3.12 (CS300)" />
  </component>
  <component name="KubernetesApiPersistence">{}</component>
  <component name="KubernetesApiProvider">{
  &quot;isMigrated&quot;: true
}</component>
</project>

================================================================================

Filename: .idea/modules.xml
Content:
<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ProjectModuleManager">
    <modules>
      <module fileurl="file://$PROJECT_DIR$/.idea/CS300.iml" filepath="$PROJECT_DIR$/.idea/CS300.iml" />
    </modules>
  </component>
</project>

================================================================================

Filename: .idea/vcs.xml
Content:
<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="VcsDirectoryMappings">
    <mapping directory="$PROJECT_DIR$" vcs="Git" />
  </component>
</project>

================================================================================

Filename: .vscode/settings.json
Content:
{
    "cSpell.words": [
        "matplotlib",
        "numpy",
        "psutil"
    ]
}

================================================================================

Filename: cloud/.gitkeep
Content:


================================================================================

Filename: cloud/Dockerfile
Content:
FROM nginx:latest

# Copy nginx configuration
COPY nginx.conf /etc/nginx/nginx.conf

# Create required log directories
RUN mkdir -p /var/log/nginx

# Install curl for health checks
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Expose port
EXPOSE 80

# Default command
CMD ["nginx", "-g", "daemon off;"]

================================================================================

Filename: cloud/nginx.conf
Content:
events {
    worker_connections 1024;
}

http {
    server {
        listen 80;
        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        location /health {
            return 200 'healthy';
            add_header Content-Type text/plain;
        }

        location / {
            return 200 'Cloud Service Running';
            add_header Content-Type text/plain;
        }
    }
}

================================================================================

Filename: cloud_py/Dockerfile
Content:
FROM python:3.12-slim

LABEL maintainer="CS300 Team" component="cloud_py" version="1.0"

# No tc needed typically for cloud, but keep curl for healthcheck
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY cloud_py/requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY cloud_py/cloud_app.py ./
# No entrypoint.sh needed unless adding features like delay simulation *within* cloud
COPY shared_modules ./shared_modules

HEALTHCHECK --interval=30s --timeout=3s \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "cloud_app.py"]


================================================================================

Filename: cloud_py/cloud_app.py
Content:
import math
import os
import threading
import time
import json
import traceback
import socket
from flask import Flask, request, jsonify
from werkzeug.middleware.dispatcher import DispatcherMiddleware
import traceback

from shared_modules.client_module import ClientModule
from shared_modules.concentration_calculator_module import ConcentrationCalculatorModule
from shared_modules.connector_module import ConnectorModule

from shared_modules.metrics import *
from shared_modules.cpu_monitor import get_container_cpu_percent_non_blocking # Optional for cloud

# --- Metrics ---
MY_TIER = "cloud"
# General Metrics (already imported via metrics *)
# MODULE_EXECUTIONS, MODULE_LATENCY, MODULE_ERRORS, PASSTHROUGH_COUNT (Passthrough N/A here)

# Cloud Specific Metrics
CLOUD_REQUEST_COUNT = Counter('cloud_requests_total', 'Total requests received by cloud')
CLOUD_INTERNAL_LATENCY = Histogram('cloud_internal_processing_latency_seconds', 'Cloud internal processing latency')
CLOUD_ERROR_COUNT = Counter('cloud_general_errors_total', 'Total general errors in cloud (outside modules)')
CPU_UTILIZATION = Gauge('cpu_utilization_percent', 'CPU utilization', ['container_name']) # Optional
container_name = socket.gethostname()

app = Flask(__name__)

# --- Configuration ---
try:
    cloud_processing_level = int(os.getenv('CLOUD_PROCESSING_LEVEL', 3)) # Cloud defaults to capable of all
except ValueError:
    print(f"WARN ({container_name}): Invalid CLOUD_PROCESSING_LEVEL, defaulting to 3.")
    cloud_processing_level = 3
# Effective level is less critical here as it's the end, but keep for consistency
effective_cloud_processing_level = max(0, cloud_processing_level)

print(f"--- Python Cloud Configuration ({container_name}) ---")
print(f"Cloud Processing Level (Config): {cloud_processing_level}")
print(f"Cloud Processing Level (Effective): {effective_cloud_processing_level}")
print(f"------------------------------------------")
# ---

# --- Initialize Modules Conditionally ---
# Cloud *can* run anything if configured and modules available
client_module = None
concentration_calculator = None
connector_module = None

if effective_cloud_processing_level >= 1 and ClientModule:
    client_module = ClientModule()
    print(f"INFO ({container_name}): Client Module (L1) initialized on Cloud.")
elif effective_cloud_processing_level >= 1: print(f"WARN ({container_name}): L1 requested but module not found.")

if effective_cloud_processing_level >= 2 and ConcentrationCalculatorModule:
    # Dependency check (only relevant if L1 was *supposed* to run here but didn't init)
    if effective_cloud_processing_level >= 1 and not client_module:
        print(f"WARN ({container_name}): Cannot initialize Calculator (L>=2) if Client (L1) is not also active/found when Cloud level is >= 1. Degrading.")
        effective_cloud_processing_level = 0
    else:
        concentration_calculator = ConcentrationCalculatorModule()
        print(f"INFO ({container_name}): Concentration Calculator Module (L2) initialized on Cloud.")
elif effective_cloud_processing_level >= 2: print(f"WARN ({container_name}): L2 requested but module not found.")

if effective_cloud_processing_level >= 3 and ConnectorModule:
    if concentration_calculator: # Check direct dependency
        connector_module = ConnectorModule()
        print(f"INFO ({container_name}): Connector Module (L3) initialized on Cloud.")
    else:
        print(f"WARN ({container_name}): Cannot initialize Connector (L3) on Cloud without Calculator (L>=2). Degrading level.")
        effective_cloud_processing_level = min(effective_cloud_processing_level, 2 if concentration_calculator else 0)
elif effective_cloud_processing_level >= 3: print(f"WARN ({container_name}): L3 requested but module not found.")
# ---

# --- CPU Monitoring (Optional for Cloud) ---
def collect_cpu_metrics():
    while True:
        cpu_info = get_container_cpu_percent_non_blocking()
        if cpu_info:
            normalized_cpu = cpu_info.get('cpu_percent_normalized', math.nan) # Cloud might not have quota
            raw_cpu = cpu_info.get('cpu_percent_raw', math.nan)
            # Report raw if normalized isn't available
            display_cpu = normalized_cpu if not math.isnan(normalized_cpu) else raw_cpu
            if not math.isnan(display_cpu): CPU_UTILIZATION.labels(container_name=container_name).set(display_cpu)
            else: CPU_UTILIZATION.labels(container_name=container_name).set(-1.0) # Indicate NaN
        else: CPU_UTILIZATION.labels(container_name=container_name).set(-2.0) # Indicate error
        time.sleep(5) # Cloud monitoring can be less frequent

def start_cpu_monitoring():
    cpu_thread = threading.Thread(target=collect_cpu_metrics, daemon=True)
    cpu_thread.start()
    print(f"Cloud ({container_name}): Background CPU monitoring started (optional)")
# ---

# --- Metrics Endpoint ---
app.wsgi_app = DispatcherMiddleware(app.wsgi_app, { '/metrics': make_wsgi_app() })

@app.route('/health')
def health_check():
    return 'healthy', 200

# Renamed endpoint, receives data from the PROXY
@app.route('/', methods=['POST'])
def process_proxy_data():
    CLOUD_REQUEST_COUNT.inc()
    processing_start_time = time.time()
    level_received = 0
    current_data = None
    level_processed_here = 0 # Track highest level processed *on the cloud*
    processing_error = False
    # The response goes back to the PROXY
    final_response_to_proxy = ({"error": "Unknown cloud processing error"}, 500)

    try:
        if not request.is_json: raise TypeError("Request must be JSON")

        incoming_data_full = request.get_json()
        if not incoming_data_full or "payload" not in incoming_data_full or "last_processed_level" not in incoming_data_full:
            raise ValueError("Missing or invalid data structure from proxy")

        level_received = incoming_data_full.get("last_processed_level", 0)
        current_data = incoming_data_full.get("payload")
        level_processed_here = level_received # Start assuming no processing
        
        request_id = current_data.get("request_id", "unknown") if isinstance(current_data, dict) else "unknown"
        print(f"Cloud ({container_name}, L{effective_cloud_processing_level}): Received data processed up to L{level_received}.")

        # Cloud is the end, no passthrough. Process everything possible up to its level.

        # --- Processing Pipeline ---

        # Level 1: Client
        if not processing_error and level_received < 1 and effective_cloud_processing_level >= 1 and client_module:
            module_name = "client"
            print(f"Cloud ({container_name}): Running {module_name} (L1)...")
            try:
                with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                    client_output = client_module.process_eeg(current_data)
                if not client_output:
                    processing_error = True
                    final_response_to_proxy = ({"status": "data_discarded_by_cloud_client", "reason": "quality"}), 400
                else:
                    current_data = client_output
                    level_processed_here = 1
                    MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
            except Exception as client_exc:
                MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                processing_error = True; final_response_to_proxy = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(client_exc)}), 500
                print(f"ERROR ({container_name}) during {module_name}: {client_exc}")

        # Level 2: Calculator
        if not processing_error and level_received < 2 and effective_cloud_processing_level >= 2 and concentration_calculator:
            module_name = "calculator"
            print(f"Cloud ({container_name}): Running {module_name} (L2)...")
            if level_processed_here < 1: # Check dependency
                dep_error_msg = f"Cloud {module_name} (L2) needs L1 input, but only reached L{level_processed_here}."
                MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                processing_error = True; final_response_to_proxy = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                print(f"ERROR ({container_name}): {dep_error_msg}")
            else:
                try:
                    with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                        calc_output = concentration_calculator.calculate_concentration(current_data)
                    if not calc_output or 'error' in calc_output: raise ValueError(f"{module_name} error: {calc_output.get('error', 'Unknown')}")
                    current_data = calc_output
                    level_processed_here = 2
                    MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                except Exception as calc_exc:
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_proxy = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(calc_exc)}), 500
                    print(f"ERROR ({container_name}) during {module_name}: {calc_exc}")

        # Level 3: Connector
        if not processing_error and level_received < 3 and effective_cloud_processing_level >= 3 and connector_module:
            module_name = "connector"
            print(f"Cloud ({container_name}): Running {module_name} (L3)...")
            if level_processed_here < 2: # Check dependency
                dep_error_msg = f"Cloud {module_name} (L3) needs L2 input, but only reached L{level_processed_here}."
                MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                processing_error = True; final_response_to_proxy = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                print(f"ERROR ({container_name}): {dep_error_msg}")
            else:
                try:
                    with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                        conn_output = connector_module.process_concentration_data(current_data)
                    # --- CALCULATE AND RECORD E2E LATENCY ---
                    if conn_output and 'error' not in conn_output:
                        creation_time = current_data.get('creation_time') # Get from data BEFORE overwriting
                        if creation_time:
                            e2e_latency = time.time() - creation_time
                            E2E_LATENCY.labels(final_tier=MY_TIER).observe(e2e_latency)
                            print(f"Gateway ({container_name}) ReqID:{request_id[-6:]}: L3 Complete. E2E Latency: {e2e_latency:.4f}s")
                        else:
                            print(f"WARN ({container_name}) ReqID:{request_id[-6:]}: Missing creation_time for E2E latency calc.")
                    # ---------------------------------------

                    
                    if not conn_output or 'error' in conn_output: raise ValueError(f"{module_name} error: {conn_output.get('error', 'Unknown')}")
                    current_data = conn_output
                    level_processed_here = 3
                    MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                except Exception as conn_exc:
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_proxy = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(conn_exc)}), 500
                    print(f"ERROR ({container_name}) during {module_name}: {conn_exc}")


        # Record internal processing time
        internal_processing_duration = time.time() - processing_start_time
        CLOUD_INTERNAL_LATENCY.observe(internal_processing_duration)

        # --- Final Response ---
        # Cloud is the end point, so it always returns the result (or error)
        if not processing_error:
            print(f"Cloud ({container_name}): Final processing complete (up to L{level_processed_here}).")
            # Structure the final response for the proxy
            final_response_to_proxy = ({"status": "processing_complete", "final_payload_preview": json.dumps(current_data)[:100], "processed_up_to": level_processed_here}), 200
        # else: final_response_to_proxy is already set in the except blocks

        # Return the determined response and status code TO THE PROXY
        return jsonify(final_response_to_proxy[0]), final_response_to_proxy[1]

    except (TypeError, ValueError) as req_err: # Catch specific request format errors
        CLOUD_ERROR_COUNT.inc()
        print(f"ERROR ({container_name}): Invalid request data from proxy: {req_err}")
        return jsonify({"error": f"Bad Request from Proxy: {req_err}"}), 400
    except Exception as e: # Catch all other unexpected errors
        CLOUD_ERROR_COUNT.inc()
        internal_processing_duration = time.time() - processing_start_time
        CLOUD_INTERNAL_LATENCY.observe(internal_processing_duration)
        print(f"FATAL Error in Cloud ({container_name}): {type(e).__name__} - {e}")
        print(traceback.format_exc())
        # Return generic error to proxy
        return jsonify({"error": "Internal server error on cloud"}), 500

if __name__ == '__main__':
    print("Python Cloud Service Starting...")
    start_cpu_monitoring() # Optional CPU monitoring
    app.run(host='0.0.0.0', port=8000)

================================================================================

Filename: cloud_py/requirements.txt
Content:
flask
requests
numpy   
prometheus-client==0.17.1
flask-prometheus-metrics==1.0.0
PyYAML==6.0.1

================================================================================

Filename: config/Config-1.json
Content:
{
    "numOfDepts": 2,
    "numOfMobilesPerDept": 5,
    "cloudResources": {
        "cpu": 44800,
        "memory": 40000,
        "storage": 1000000,
        "bandwidth": 10000
    },
    "proxyResources": {
        "cpu": 2800,
        "memory": 4000,
        "storage": 500000,
        "bandwidth": 10000
    },
    "edgeResources": {
        "cpu": 1000,
        "memory": 1000,
        "storage": 100000,
        "bandwidth": 1000
    },
    "applicationModules": [
        {
            "name": "client",
            "requiredCpu": 10,
            "requiredMemory": 100
        },
        {
            "name": "concentration_calculator",
            "requiredCpu": 10,
            "requiredMemory": 200
        },
        {
            "name": "connector",
            "requiredCpu": 10,
            "requiredMemory": 150
        }
    ],
    "dataFlows": [
        {
            "source": "client",
            "destination": "concentration_calculator",
            "dataSize": 3500,
            "cpuLoad": 0.5
        },
        {
            "source": "concentration_calculator",
            "destination": "connector",
            "dataSize": 100,
            "cpuLoad": 0.2
        },
        {
            "source": "connector",
            "destination": "client",
            "dataSize": 100,
            "cpuLoad": 0.3
        }
    ]
}

================================================================================

Filename: config/grafana-dashboards/dash.json
Content:
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 3,
  "links": [],
  "liveNow": true,
  "panels": [
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 100,
      "panels": [],
      "title": "Simulation Overview",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "#EAB839",
                "value": 0.5
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 0,
        "y": 1
      },
      "id": 101,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "mean"
          ],
          "fields": "",
          "values": false
        },
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "avg(gateway_request_latency_seconds{job=\"mobile\"})",
          "instant": false,
          "legendFormat": "Avg Mobile RTT to Gateway",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Avg Mobile -> Gateway Latency (RTT)",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              }
            ]
          },
          "unit": "ops"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 6,
        "y": 1
      },
      "id": 102,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(eeg_data_processed_total{job=\"mobile\"}[1m]))",
          "instant": true,
          "legendFormat": "Total EEG Data Input Rate",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Total System Input Rate (EEG)",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "#EAB839",
                "value": 0.01
              },
              {
                "color": "red",
                "value": 0.05
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 12,
        "y": 1
      },
      "id": 103,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "(sum(rate(gateway_request_failures_total{job=\"mobile\"}[5m])) + sum(rate(gateway_errors_total{job=\"gateway\"}[5m])) + sum(rate(gateway_forward_to_proxy_failures_total{job=\"gateway\"}[5m])) + sum(rate(proxy_errors_total{job=\"proxy\"}[5m]))) / sum(rate(eeg_data_processed_total{job=\"mobile\"}[5m]))",
          "instant": true,
          "legendFormat": "Overall System Error Rate",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Overall System Error Rate (%)",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          },
          "unit": "percent",
          "min": 0,
          "max": 100
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 18,
        "y": 1
      },
      "id": 300,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "mean"
          ],
          "fields": "",
          "values": false
        },
        "textMode": "auto"
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "avg(cpu_utilization_percent{job=\"mobile\"}>0)",
          "instant": true,
          "legendFormat": "Avg Mobile CPU %",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Avg Mobile CPU (Normalized)",
      "type": "stat"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 7
      },
      "id": 13,
      "panels": [],
      "title": "Mobile Device Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 15,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "smooth"
          },
          "unit": "ops"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 8
      },
      "id": 14,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(eeg_data_processed_total{job=\"mobile\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Processed",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile EEG Processing Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "axisPlacement": "right",
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 8
      },
      "id": 15,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "gateway_request_latency_seconds{job=\"mobile\"}",
          "instant": false,
          "legendFormat": "{{instance}} Latency",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile -> Gateway Latency (RTT)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 8
      },
      "id": 301,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"mobile\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 80,
            "stacking": {
              "mode": "normal"
            },
            "drawStyle": "bars"
          },
          "unit": "errs"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 16
      },
      "id": 19,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_request_failures_total{job=\"mobile\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} GW Failure Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile Gateway Request Failure Rate (Per Instance)",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 24
      },
      "id": 10,
      "panels": [],
      "title": "Gateway Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 15,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "smooth"
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 25
      },
      "id": 11,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_requests_total{job=\"gateway\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} RX Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Gateway Request RX Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "axisPlacement": "right",
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 25
      },
      "id": 12,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(gateway_request_duration_seconds_bucket{job=\"gateway\"}[5m])) by (le, instance))",
          "instant": false,
          "legendFormat": "{{instance}} p95",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_request_duration_seconds_sum{job=\"gateway\"}[5m])) by (instance) / sum(rate(gateway_request_duration_seconds_count{job=\"gateway\"}[5m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Avg",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Gateway Calculator Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 25
      },
      "id": 112,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(gateway_forward_to_proxy_latency_seconds_bucket{job=\"gateway\"}[5m])) by (le, instance))",
          "instant": false,
          "legendFormat": "{{instance}} p95",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_forward_to_proxy_latency_seconds_sum{job=\"gateway\"}[5m])) by (instance) / sum(rate(gateway_forward_to_proxy_latency_seconds_count{job=\"gateway\"}[5m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Avg",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Gateway -> Proxy Forward Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 25
      },
      "id": 302,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"gateway\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Gateway CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 80,
            "stacking": {
              "mode": "normal"
            },
            "drawStyle": "bars"
          },
          "unit": "errs"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 33
      },
      "id": 113,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_errors_total{job=\"gateway\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Calc Errors",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(gateway_forward_to_proxy_failures_total{job=\"gateway\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Fwd Failures",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Gateway Error / Failure Rate",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 41
      },
      "id": 200,
      "panels": [],
      "title": "Proxy Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 15,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "smooth"
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 42
      },
      "id": 201,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(proxy_requests_total{job=\"proxy\"}[1m])",
          "instant": false,
          "legendFormat": "Proxy RX Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Proxy Request RX Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "axisPlacement": "right",
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 42
      },
      "id": 202,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(proxy_connector_latency_seconds_bucket{job=\"proxy\"}[5m])) by (le))",
          "instant": false,
          "legendFormat": "Proxy p95 Latency",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(proxy_connector_latency_seconds_sum{job=\"proxy\"}[5m])) / sum(rate(proxy_connector_latency_seconds_count{job=\"proxy\"}[5m]))",
          "instant": false,
          "legendFormat": "Proxy Avg Latency",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Proxy Connector Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 42
      },
      "id": 303,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"proxy\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Proxy CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 80,
            "stacking": {
              "mode": "normal"
            },
            "drawStyle": "bars"
          },
          "unit": "errs"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 50
      },
      "id": 203,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(proxy_errors_total{job=\"proxy\"}[1m])",
          "instant": false,
          "legendFormat": "Proxy Error Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Proxy Processing Error Rate",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 58
      },
      "id": 310,
      "panels": [],
      "title": "Cloud Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 59
      },
      "id": 311,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"cloud\"}>0",
          "instant": false,
          "legendFormat": "{{container_name}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Cloud CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 67
      },
      "id": 16,
      "panels": [],
      "title": "EEG Signal Characteristics (Mobile)",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "mappings": [],
          "max": 1,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "red"
              },
              {
                "color": "#EAB839",
                "value": 0.5
              },
              {
                "color": "green",
                "value": 0.8
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 68
      },
      "id": 17,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "lastNotNull"
          ],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "eeg_quality_score{job=\"mobile\"}",
          "instant": false,
          "legendFormat": "{{instance}} Quality",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "EEG Signal Quality Score",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 80,
            "stacking": {
              "mode": "normal"
            },
            "drawStyle": "bars"
          },
          "unit": "ops"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 68
      },
      "id": 18,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(eeg_discarded_total{job=\"mobile\"}[1m])) by (instance)",
          "instant": false,
          "legendFormat": "{{instance}} Discard Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile Data Discard Rate (Quality)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "lineWidth": 1,
            "fillOpacity": 10,
            "stacking": {
              "mode": "none"
            },
            "drawStyle": "line",
            "lineInterpolation": "linear"
          },
          "unit": ""
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 68
      },
      "id": 23,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "lastNotNull"
          ],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "eeg_alpha_power{job=\"mobile\"}",
          "instant": false,
          "legendFormat": "{{instance}} Alpha Power",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "eeg_noise_level{job=\"mobile\"}",
          "instant": false,
          "legendFormat": "{{instance}} Noise Level",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "EEG Alpha Power & Noise Level",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "tags": [
    "eeg",
    "fog",
    "edge",
    "simulation",
    "proxy",
    "cpu"
  ],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-15m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "EEG Fog Simulation Monitoring",
  "uid": "dash",
  "version": 3
}

================================================================================

Filename: config/grafana-dashboards/dashv2.json
Content:
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 2,
  "links": [],
  "liveNow": true,
  "panels": [
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 100,
      "panels": [],
      "title": "Simulation Overview & Final Processing",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "#EAB839",
                "value": 0.5
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 0,
        "y": 1
      },
      "id": 101,
      "options": {
        "colorMode": "value",
        "graphMode": "none",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "mean"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "avg(gateway_request_latency_seconds{job=\"mobile\"})",
          "format": "stat",
          "instant": true,
          "legendFormat": "Avg Upstream RTT",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Avg Mobile Upstream RTT",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "decimals": 2,
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "#EAB839",
                "value": 1
              },
              {
                "color": "red",
                "value": 5
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 5,
        "x": 5,
        "y": 1
      },
      "id": 103,
      "options": {
        "colorMode": "value",
        "graphMode": "none",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "(\n  sum(increase(module_errors_total[5m]))\n+ sum(increase(gateway_request_failures_total[5m]))\n+ sum(increase(gateway_forward_to_proxy_failures_total[5m]))\n+ sum(increase(proxy_forward_to_cloud_failures_total[5m]))\n)\n/ sum(increase(module_executions_total{module=\"client\"}[5m])) * 100\n",
          "format": "stat",
          "instant": true,
          "legendFormat": "Overall Error %",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Overall System Error Rate (%)",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "#EAB839",
                "value": 1
              },
              {
                "color": "red",
                "value": 2
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 7,
        "x": 10,
        "y": 1
      },
      "id": 104,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "mean"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(e2e_processing_latency_seconds_bucket[5m])) by (le, final_tier))",
          "format": "stat",
          "instant": true,
          "legendFormat": "p95 ({{final_tier}})",
          "range": false,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(e2e_processing_latency_seconds_sum[5m])) by (final_tier) / sum(rate(e2e_processing_latency_seconds_count[5m])) by (final_tier)",
          "format": "stat",
          "instant": true,
          "legendFormat": "Avg ({{final_tier}})",
          "range": false,
          "refId": "B"
        }
      ],
      "title": "End-to-End Latency (L3 Completion)",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            }
          },
          "mappings": [],
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 7,
        "x": 17,
        "y": 1
      },
      "id": 400,
      "options": {
        "displayLabels": [
          "name",
          "percent"
        ],
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "pieType": "pie",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum by (tier) (increase(module_executions_total{module=\"connector\"}[5m]))",
          "instant": true,
          "legendFormat": "{{tier}}",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Final Processing Location (Connector Exec Rate by Tier)",
      "type": "piechart"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 0,
        "y": 4
      },
      "id": 102,
      "options": {
        "colorMode": "value",
        "graphMode": "none",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "sum(rate(module_executions_total{module=\"client\"}[1m]))",
          "format": "stat",
          "instant": true,
          "legendFormat": "Input Rate (Client Exec)",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Total System Input Rate",
      "type": "stat"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 7
      },
      "id": 13,
      "panels": [],
      "title": "Mobile Device Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 15,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "smooth",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 8
      },
      "id": 401,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_executions_total{tier=\"mobile\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile: Module Execution Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 8
      },
      "id": 402,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(module_execution_latency_seconds_bucket{tier=\"mobile\"}[5m])) by (le, module))",
          "instant": false,
          "legendFormat": "{{module}} p95",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile: Module Latency (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "bars",
            "fillOpacity": 80,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "errps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 8
      },
      "id": 403,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_errors_total{tier=\"mobile\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Error Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(gateway_request_failures_total{job=\"mobile\"}[1m])",
          "instant": false,
          "legendFormat": "GW Send Fail Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Mobile: Module/Send Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "max": 100,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 8
      },
      "id": 301,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"mobile\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Mobile: CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 16
      },
      "id": 10,
      "panels": [],
      "title": "Gateway Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 15,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "smooth",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 17
      },
      "id": 411,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_executions_total{tier=\"gateway\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(passthrough_requests_total{tier=\"gateway\"}[1m])",
          "instant": false,
          "legendFormat": "Passthrough Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Gateway: Module Execution & Passthrough Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 17
      },
      "id": 412,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(module_execution_latency_seconds_bucket{tier=\"gateway\"}[5m])) by (le, module))",
          "instant": false,
          "legendFormat": "{{module}} p95",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Gateway: Module Latency (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "bars",
            "fillOpacity": 80,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "errps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 17
      },
      "id": 413,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_errors_total{tier=\"gateway\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Error Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(gateway_forward_to_proxy_failures_total[1m])",
          "instant": false,
          "legendFormat": "Proxy Fwd Fail Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Gateway: Module/Forward Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "max": 100,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 17
      },
      "id": 302,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"gateway\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Gateway: CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 25
      },
      "id": 200,
      "panels": [],
      "title": "Proxy Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 15,
            "lineInterpolation": "smooth",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 26
      },
      "id": 421,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_executions_total{tier=\"proxy\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(passthrough_requests_total{tier=\"proxy\"}[1m])",
          "instant": false,
          "legendFormat": "Passthrough Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Proxy: Module Execution & Passthrough Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisPlacement": "auto",
            "drawStyle": "line",
            "fillOpacity": 10,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 26
      },
      "id": 422,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(module_execution_latency_seconds_bucket{tier=\"proxy\"}[5m])) by (le, module))",
          "instant": false,
          "legendFormat": "{{module}} p95",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Proxy: Module Latency (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "bars",
            "fillOpacity": 80,
            "lineWidth": 1,
            "stacking": {
              "mode": "normal"
            }
          },
          "unit": "errps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 26
      },
      "id": 423,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_errors_total{tier=\"proxy\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Error Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(proxy_forward_to_cloud_failures_total[1m])",
          "instant": false,
          "legendFormat": "Cloud Fwd Fail Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Proxy: Module/Forward Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 10,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "max": 100,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 26
      },
      "id": 303,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"proxy\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Proxy: CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 34
      },
      "id": 310,
      "panels": [],
      "title": "Cloud Performance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 15,
            "lineInterpolation": "smooth",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 0,
        "y": 35
      },
      "id": 431,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_executions_total{tier=\"cloud\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Rate",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(passthrough_requests_total{tier=\"cloud\"}[1m])",
          "hide": true,
          "instant": false,
          "legendFormat": "Passthrough Rate",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Cloud: Module Execution Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisPlacement": "auto",
            "drawStyle": "line",
            "fillOpacity": 10,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 6,
        "y": 35
      },
      "id": 432,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(module_execution_latency_seconds_bucket{tier=\"cloud\"}[5m])) by (le, module))",
          "instant": false,
          "legendFormat": "{{module}} p95",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Cloud: Module Latency (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "bars",
            "fillOpacity": 80,
            "lineWidth": 1,
            "stacking": {
              "mode": "normal"
            }
          },
          "unit": "errps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 35
      },
      "id": 433,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(module_errors_total{tier=\"cloud\"}[1m])",
          "instant": false,
          "legendFormat": "{{module}} Error Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Cloud: Module Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "drawStyle": "line",
            "fillOpacity": 10,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "stacking": {
              "mode": "none"
            }
          },
          "max": 100,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "orange",
                "value": 80
              },
              {
                "color": "red",
                "value": 95
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 35
      },
      "id": 311,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "cpu_utilization_percent{job=\"cloud\"}>0",
          "instant": false,
          "legendFormat": "{{instance}} CPU %",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Cloud: CPU Utilization (Normalized)",
      "type": "timeseries"
    },
    {
      "collapsed": true,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 43
      },
      "id": 16,
      "panels": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "fieldConfig": {
            "defaults": {
              "color": {
                "mode": "palette-classic"
              },
              "custom": {
                "drawStyle": "line",
                "fillOpacity": 10,
                "lineInterpolation": "linear",
                "lineWidth": 1,
                "stacking": {
                  "mode": "none"
                }
              },
              "mappings": [],
              "max": 1,
              "min": 0,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {
                    "color": "red"
                  },
                  {
                    "color": "#EAB839",
                    "value": 0.5
                  },
                  {
                    "color": "green",
                    "value": 0.8
                  }
                ]
              },
              "unit": "short"
            },
            "overrides": []
          },
          "gridPos": {
            "h": 8,
            "w": 6,
            "x": 0,
            "y": 44
          },
          "id": 17,
          "options": {
            "legend": {
              "calcs": [
                "mean",
                "lastNotNull"
              ],
              "displayMode": "table",
              "placement": "right",
              "showLegend": true
            },
            "tooltip": {
              "mode": "multi"
            }
          },
          "pluginVersion": "11.6.0",
          "targets": [
            {
              "datasource": {
                "type": "prometheus",
                "uid": "PBFA97CFB590B2093"
              },
              "editorMode": "code",
              "expr": "eeg_quality_score{job=\"mobile\"}",
              "instant": false,
              "legendFormat": "{{instance}} Quality",
              "range": true,
              "refId": "A"
            }
          ],
          "title": "EEG Signal Quality Score",
          "type": "timeseries"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "fieldConfig": {
            "defaults": {
              "color": {
                "mode": "palette-classic"
              },
              "custom": {
                "drawStyle": "bars",
                "fillOpacity": 80,
                "lineWidth": 1,
                "stacking": {
                  "mode": "normal"
                }
              },
              "unit": "ops"
            },
            "overrides": []
          },
          "gridPos": {
            "h": 8,
            "w": 6,
            "x": 6,
            "y": 44
          },
          "id": 18,
          "options": {
            "legend": {
              "displayMode": "list",
              "placement": "bottom",
              "showLegend": true
            },
            "tooltip": {
              "mode": "multi"
            }
          },
          "pluginVersion": "11.6.0",
          "targets": [
            {
              "datasource": {
                "type": "prometheus",
                "uid": "PBFA97CFB590B2093"
              },
              "editorMode": "code",
              "expr": "sum(rate(eeg_discarded_total{job=\"mobile\"}[1m])) by (instance)",
              "instant": false,
              "legendFormat": "{{instance}} Discard Rate",
              "range": true,
              "refId": "A"
            }
          ],
          "title": "Mobile Data Discard Rate (Quality)",
          "type": "timeseries"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "fieldConfig": {
            "defaults": {
              "color": {
                "mode": "palette-classic"
              },
              "custom": {
                "drawStyle": "line",
                "fillOpacity": 10,
                "lineInterpolation": "linear",
                "lineWidth": 1,
                "stacking": {
                  "mode": "none"
                }
              },
              "unit": ""
            },
            "overrides": []
          },
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 12,
            "y": 44
          },
          "id": 23,
          "options": {
            "legend": {
              "calcs": [
                "mean",
                "lastNotNull"
              ],
              "displayMode": "table",
              "placement": "right",
              "showLegend": true
            },
            "tooltip": {
              "mode": "multi"
            }
          },
          "pluginVersion": "11.6.0",
          "targets": [
            {
              "datasource": {
                "type": "prometheus",
                "uid": "PBFA97CFB590B2093"
              },
              "editorMode": "code",
              "expr": "eeg_alpha_power{job=\"mobile\"}",
              "instant": false,
              "legendFormat": "{{instance}} Alpha Power",
              "range": true,
              "refId": "A"
            },
            {
              "datasource": {
                "type": "prometheus",
                "uid": "PBFA97CFB590B2093"
              },
              "editorMode": "code",
              "expr": "eeg_noise_level{job=\"mobile\"}",
              "instant": false,
              "legendFormat": "{{instance}} Noise Level",
              "range": true,
              "refId": "B"
            }
          ],
          "title": "EEG Alpha Power & Noise Level",
          "type": "timeseries"
        }
      ],
      "title": "EEG Signal Characteristics (Mobile)",
      "type": "row"
    }
  ],
  "preload": false,
  "refresh": "10s",
  "schemaVersion": 41,
  "tags": [
    "eeg",
    "fog",
    "edge",
    "simulation",
    "proxy",
    "cpu",
    "level-based",
    "module-placement"
  ],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-15m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "EEG Fog Sim [Level-Based Processing]",
  "uid": "dashv2",
  "version": 1
}

================================================================================

Filename: config/grafana-dashboards/eeg-monitoring.json
Content:
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "rate(eeg_data_processed_total[5m])",
          "instant": false,
          "legendFormat": "EEG Processing Rate",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "EEG Processing Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "10.2.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "PBFA97CFB590B2093"
          },
          "editorMode": "code",
          "expr": "gateway_request_latency_seconds",
          "instant": false,
          "legendFormat": "Gateway Latency",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Gateway Latency",
      "type": "gauge"
    }
  ],
  "refresh": "5s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-5m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "EEG Monitoring",
  "version": 1,
  "weekStart": ""
}

================================================================================

Filename: config/grafana-provisioning/dashboards/main.yaml
Content:
# config/grafana-provisioning/dashboards/main.yaml
apiVersion: 1

providers:
  # <string> An identifier for this provider. Required. It must be unique for each provider.
- name: 'default' # A descriptive name for the provider
  # <int> Org id. Default is 1
  orgId: 1
  # <string> Name of the dashboard folder. Required.
  folder: '' # Root folder, or specify a name like 'EEG Simulation'
  # <string> Provider type. Default is file
  type: file
  # <bool> Disable / Enable dashboard deletion
  disableDeletion: false
  # <bool> Enable dashboard editing
  editable: true
  # <map> Options to configure the provider
  options:
    # <string> Path to dashboard files on disk. Required.
    path: /var/lib/grafana/dashboards # This is the path INSIDE the container where you mounted the dashboards
    # <bool> If set to true, Grafana will update dashboards automatically when the json files change
    # Does not always work reliably with Docker volume mounts without extra tooling.
    # Set to false for more predictable behavior on restart.
    # foldersFromFilesStructure: true # Use if you want subdirs in host path to become Grafana folders

================================================================================

Filename: config/grafana-provisioning/datasources/loki.yaml
Content:
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy # Use Grafana backend as proxy
    url: http://loki:3100 # Internal URL Grafana uses to reach Loki
    jsonData:
      # Optional: Set derived fields for better linking from logs to traces/metrics
      # derivedFields:
      #   - datasourceUid: 'PBFA97CFB590B2093' # UID of your Prometheus datasource
      #     matcherRegex: 'traceID=(\w+)'
      #     name: 'TraceID'
      #     url: '/explore?orgId=1&left=["now-1h","now","Tempo",{"query":"$${__value.raw}"}]'
      #     urlDisplayLabel: 'View Trace'
      maxLines: 1000
    version: 1
    editable: true
    isDefault: false # Keep Prometheus as default for now

================================================================================

Filename: config/grafana-provisioning/datasources/prometheus.yaml
Content:
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    version: 1
    editable: true
    jsonData:
      timeInterval: "15s"
      queryTimeout: "60s"
      httpMethod: "POST"

================================================================================

Filename: config/grafana.ini
Content:
[paths]
datadir = /var/lib/grafana

[server]
http_port = 3000

[security]
admin_user = admin

[auth.anonymous]
enabled = true

[datasources]
allow_loading_unsigned_plugins = true

# Remove the [[datasources]] section as we'll configure it through provisioning
[provisioning]
allow_ui_updates = true

[dashboards]
default_home_dashboard_uid = dash

# [dashboards.json]
# enabled = true
# path = /var/lib/grafana/dashboards



================================================================================

Filename: config/loki-config.yaml
Content:
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  instance_addr: 127.0.0.1
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2025-04-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

limits_config:
  reject_old_samples: false
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 100
  ingestion_burst_size_mb: 200
  max_global_streams_per_user: 10000
  max_streams_per_user: 0
  max_entries_limit_per_query: 10000
  allow_structured_metadata: false

================================================================================

Filename: config/prometheus.yml
Content:
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: mobile
    metrics_path: /metrics    # Add this line
    static_configs:
      - targets:
          - mobile1_1:9090
          - mobile1_2:9090
          - mobile1_3:9090
          - mobile1_4:9090
          - mobile2_1:9090
          - mobile2_2:9090
          - mobile2_3:9090
          - mobile2_4:9090

  - job_name: gateway
    metrics_path: /metrics    # Add this line
    static_configs:
      - targets:
          - gateway1:8000
          - gateway2:8000
  
  - job_name: proxy
    metrics_path: /metrics    # Add this line
    static_configs:
      - targets:
          - proxy_py:8000

  - job_name: cloud 
    metrics_path: /metrics
    static_configs:
      - targets:
          - cloud_py:8000

================================================================================

Filename: config/promtail-config.yaml
Content:
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions/positions.yaml # Path inside Promtail container

clients:
  - url: http://loki:3100/loki/api/v1/push # Tells Promtail where Loki is

scrape_configs:
  - job_name: system # Scrapes systemd journal if needed (optional, comment out if not needed/available)
    journal:
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: ['__journal__systemd_unit']
        target_label: 'unit'

  - job_name: containers # Primary job: Scrapes Docker container logs
    docker_sd_configs: # Uses Docker service discovery via the mounted socket
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        # Optional: Filter containers to scrape if needed
        # filters:
        #   - name: label
        #     values: ["logging=promtail"] # Only scrape containers with this label
    relabel_configs: # Extracts useful labels from Docker metadata
      # Example: Use the compose service name as the 'job' label
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'job'
      # Example: Use container name as 'instance' label
      - source_labels: ['__meta_docker_container_name__']
        regex: '/(.*)' # Extract name without leading /
        target_label: 'instance'
      # Keep container ID
      - source_labels: ['__meta_docker_container_id__']
        target_label: 'container_id'
      # Keep log stream type (stdout/stderr)
      - source_labels: ['__meta_docker_container_log_stream__']
        target_label: 'logstream'

================================================================================

Filename: docker-compose.yaml
Content:
name: CS300
services:
  # cloud:
  #   build: ./cloud
  #   networks:
  #     - cloud_proxy_net
  #   cap_add:
  #     - NET_ADMIN
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost/health"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 60s
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"
  #   volumes:
  #     - ./logs/cloud:/var/log/nginx

  # proxy:
  #   build: ./proxy
  #   networks:
  #     - cloud_proxy_net
  #     - proxy_gateways_net
  #   cap_add:
  #     - NET_ADMIN
  #   ports:
  #     - "9090:9090"
  #   depends_on:
  #     gateway1:
  #       condition: service_healthy
  #     gateway2:
  #       condition: service_healthy
  #   environment:
  #     - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
  #     - LATENCY_PROXY_TO_CLOUD=${LATENCY_PROXY_TO_CLOUD:-0ms}
  #     # Pass jitter/loss if using them in entrypoint.sh
  #     - JITTER_PROXY_TO_CLOUD=${JITTER_PROXY_TO_CLOUD:-}
  #     - LOSS_PROXY_TO_CLOUD=${LOSS_PROXY_TO_CLOUD:-}
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost/health"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 60s
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"
  #   volumes:
  #     - ./logs/proxy:/var/log/nginx
  #     - ./config:/app/config

  cloud_py: 
    build:
      context: .
      dockerfile: cloud_py/Dockerfile
    networks:
      - cloud_proxy_net # Connects only to the proxy network
    cap_add:
      - NET_ADMIN # Keep if needed for potential future network manipulation
    ports:
      - "8081:8000" # Expose cloud service on host port 8081 (internal 8000)
    deploy:
      resources:
        limits:
          cpus: '1' # Example: Give cloud more resources
          memory: 1G
    environment:
      - PYTHONUNBUFFERED=1
      - CLOUD_PROCESSING_LEVEL=${CLOUD_PROCESSING_LEVEL:-3}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Should start faster than Nginx
    logging:
      driver: "json-file"
      options:
          max-size: "10m"
          max-file: "3"

  proxy_py: # Renamed service for clarity
    build:
      context: .
      dockerfile: proxy_py/Dockerfile
    networks:
      - cloud_proxy_net    # Connects up to cloud
      - proxy_gateways_net # Connects down to gateways
    depends_on:
      cloud_py:
        condition: service_healthy
    cap_add:
      - NET_ADMIN 
    ports:
      - "8080:8000" # Expose proxy on host port 8080 (internal 8000)
    deploy: 
      resources:
        limits:
          cpus: '0.1' 
          memory: 512M 
    environment:
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_PROXY_TO_CLOUD=${LATENCY_PROXY_TO_CLOUD:-0ms}
      - JITTER_PROXY_TO_CLOUD=${JITTER_PROXY_TO_CLOUD:-} # Pass empty if unset in .env
      - LOSS_PROXY_TO_CLOUD=${LOSS_PROXY_TO_CLOUD:-}
      - PYTHONUNBUFFERED=1
      - PROXY_PROCESSING_LEVEL=${PROXY_PROCESSING_LEVEL:-3}
      - CLOUD_URL=${CLOUD_URL:-http://cloud_py:8000}

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
          max-size: "10m"
          max-file: "3"
    volumes:
      - ./logs/proxy:/app/logs
      - ./config:/app/config


  gateway1:
    build:
      context: . 
      dockerfile: gateway/Dockerfile
    networks:
      - proxy_gateways_net
      - gateway1_mobiles_net
    cap_add:
      - NET_ADMIN
    ports:
      - "9091:8000" 
    depends_on:
      proxy_py:
          condition: service_healthy
    deploy: 
      resources:
        limits:
          cpus: '0.1'
          memory: 512M 
    environment:
      - PROXY_URL=http://proxy_py:8000/
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_GATEWAY_TO_PROXY=${LATENCY_GATEWAY_TO_PROXY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_GATEWAY_TO_PROXY=${JITTER_GATEWAY_TO_PROXY:-}
      - LOSS_GATEWAY_TO_PROXY=${LOSS_GATEWAY_TO_PROXY:-}
      # Pass other necessary env vars if any (like PYTHONUNBUFFERED)
      - PYTHONUNBUFFERED=1
      - GATEWAY_PROCESSING_LEVEL=${GATEWAY_PROCESSING_LEVEL:-2}

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs/edge1:/app/logs
      - ./config:/app/config

  gateway2:
    build:
      context: . 
      dockerfile: gateway/Dockerfile
    networks:
      - proxy_gateways_net
      - gateway2_mobiles_net
    cap_add:
      - NET_ADMIN
    ports:
      - "9092:8000"  
    depends_on:
      proxy_py:
          condition: service_healthy
    deploy: 
      resources:
        limits:
          cpus: '0.1' 
          memory: 512M 
    environment:
      - PROXY_URL=http://proxy_py:8000/
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_GATEWAY_TO_PROXY=${LATENCY_GATEWAY_TO_PROXY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_GATEWAY_TO_PROXY=${JITTER_GATEWAY_TO_PROXY:-}
      - LOSS_GATEWAY_TO_PROXY=${LOSS_GATEWAY_TO_PROXY:-}
      # Pass other necessary env vars if any (like PYTHONUNBUFFERED)
      - PYTHONUNBUFFERED=1
      - GATEWAY_PROCESSING_LEVEL=${GATEWAY_PROCESSING_LEVEL:-2}

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs/edge2:/app/logs
      - ./config:/app/config

  mobile1_1:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway1_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway1 
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}

    cap_add:
      - NET_ADMIN
    ports:
      - "9093:9090"
    deploy: 
      resources:
        limits:
          cpus: '0.05' 
          memory: 256M 
    depends_on:
      gateway1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs/mobile1_1:/app/logs
      - ./config:/app/config

  mobile1_2:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway1_mobiles_net
    environment:
      - GATEWAY=gateway1 
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9094:9090" 
    deploy: 
      resources:
        limits:
          cpus: '0.05' 
          memory: 256M 
    depends_on:
      gateway1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile1_3:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway1_mobiles_net
    environment:
      - GATEWAY=gateway1 
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9096:9090" 
    deploy:
      resources:
        limits:
          cpus: '0.05' 
          memory: 256M 
    depends_on:
      gateway1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile1_4:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway1_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway1 # Keep existing ones
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9097:9090" 
    deploy: # <-- ADD THIS SECTION (Optional)
      resources:
        limits:
          cpus: '0.05' # Example: Limit mobile to 20% of a core
          memory: 256M # Example: Limit mobile RAM
    depends_on:
      gateway1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile2_1:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway2_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway2 # Keep existing ones
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9098:9090" 
    deploy: # <-- ADD THIS SECTION (Optional)
      resources:
        limits:
          cpus: '0.05' # Example: Limit mobile to 20% of a core
          memory: 256M # Example: Limit mobile RAM
    depends_on:
      gateway2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile2_2:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway2_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway2 # Keep existing ones
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9099:9090" 
    deploy: # <-- ADD THIS SECTION (Optional)
      resources:
        limits:
          cpus: '0.05' # Example: Limit mobile to 20% of a core
          memory: 256M # Example: Limit mobile RAM
    depends_on:
      gateway2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile2_3:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway2_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway2 # Keep existing ones
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9100:9090" 
    deploy: # <-- ADD THIS SECTION (Optional)
      resources:
        limits:
          cpus: '0.05' # Example: Limit mobile to 20% of a core
          memory: 256M # Example: Limit mobile RAM
    depends_on:
      gateway2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mobile2_4:
    build:
      context: . 
      dockerfile: mobile/Dockerfile
    networks:
      - gateway2_mobiles_net
    # MERGE environment variables
    environment:
      - GATEWAY=gateway2 # Keep existing ones
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Add the latency variables
      - ENABLE_LATENCY=${ENABLE_LATENCY:-false}
      - LATENCY_MOBILE_TO_GATEWAY=${LATENCY_MOBILE_TO_GATEWAY:-0ms}
      # Pass jitter/loss if using them in entrypoint.sh
      - JITTER_MOBILE_TO_GATEWAY=${JITTER_MOBILE_TO_GATEWAY:-}
      - LOSS_MOBILE_TO_GATEWAY=${LOSS_MOBILE_TO_GATEWAY:-}
      # Add the placement strategy and load threshold variables
      # - PLACEMENT_STRATEGY=${PLACEMENT_STRATEGY:-ewmp}
      # - MOBILE_CPU_LIMIT=${MOBILE_CPU_LIMIT:-0.2}
      # - MOBILE_LOAD_THRESHOLD=${MOBILE_LOAD_THRESHOLD:-0.8}
      - MOBILE_PROCESSING_LEVEL=${MOBILE_PROCESSING_LEVEL:-1}
    cap_add:
      - NET_ADMIN
    ports:
      - "9101:9090" 
    deploy: # <-- ADD THIS SECTION (Optional)
      resources:
        limits:
          cpus: '0.05' # Example: Limit mobile to 20% of a core
          memory: 256M # Example: Limit mobile RAM
    depends_on:
      gateway2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'  # Add this line
    ports:
      - "9095:9090"
    networks:
      - monitoring_net
      - cloud_proxy_net
      - proxy_gateways_net
      - gateway1_mobiles_net
      - gateway2_mobiles_net
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    volumes:
      - ./config/grafana.ini:/etc/grafana/grafana.ini:ro
      - ./config/grafana-provisioning:/etc/grafana/provisioning:ro  # Add this line
      - ./config/grafana-dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - monitoring_net
    depends_on:
      - prometheus
      - loki
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: unless-stopped

  loki:
    image: grafana/loki:latest 
    volumes:
      - ./config/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - monitoring_net 
    logging:
      driver: "json-file"
      options: { max-size: "10m", max-file: "3" }
    restart: unless-stopped

  promtail:
    image: grafana/promtail:latest 
    volumes:
      - ./config/promtail-config.yaml:/etc/promtail/config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro 
      - promtail_positions:/tmp/positions 
    command: -config.file=/etc/promtail/config.yml
    networks:
      - monitoring_net 
    logging:
      driver: "json-file"
      options: { max-size: "10m", max-file: "3" }
    restart: unless-stopped

networks:
  cloud_proxy_net:
  proxy_gateways_net:
  gateway1_mobiles_net:
  gateway2_mobiles_net:
  monitoring_net:

volumes:
  prometheus_data:
  grafana_data:
  loki_data:
  promtail_positions:

================================================================================

Filename: gateway/Dockerfile
Content:
FROM python:3.12-slim

LABEL maintainer="CS300 Team" \
    component="gateway" \
    version="1.0"

RUN apt-get update && apt-get install -y \
    iproute2 \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY gateway/requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY gateway/gateway.py ./
COPY gateway/entrypoint.sh ./
COPY shared_modules ./shared_modules 

RUN chmod +x ./entrypoint.sh

HEALTHCHECK --interval=30s --timeout=3s \
    CMD curl -f http://localhost:8000/health || exit 1

ENTRYPOINT ["./entrypoint.sh"]
CMD ["python", "gateway.py"]

================================================================================

Filename: gateway/entrypoint.sh
Content:
#!/bin/sh
# gateway/entrypoint.sh

echo "Gateway Entrypoint: Starting setup..."
echo "DEBUG: ENABLE_LATENCY is set to: [$ENABLE_LATENCY]"
echo "DEBUG: LATENCY_GATEWAY_TO_PROXY is set to: [$LATENCY_GATEWAY_TO_PROXY]"

# Apply tc rules if enabled and variable is set
if [ "$ENABLE_LATENCY" = "true" ] && [ -n "$LATENCY_GATEWAY_TO_PROXY" ]; then
  echo "Gateway Entrypoint: Applying $LATENCY_GATEWAY_TO_PROXY delay to eth0 (towards Proxy/Mobiles)"
  
  # Build the tc command based on which parameters are set
  TC_CMD="tc qdisc replace dev eth0 root netem delay ${LATENCY_GATEWAY_TO_PROXY:-0ms}"
  
  # Add jitter parameter only if it's set
  if [ -n "$JITTER_GATEWAY_TO_PROXY" ]; then
    TC_CMD="$TC_CMD $JITTER_GATEWAY_TO_PROXY"
  fi
  
  # Add loss parameter only if it's set
  if [ -n "$LOSS_GATEWAY_TO_PROXY" ]; then
    TC_CMD="$TC_CMD loss $LOSS_GATEWAY_TO_PROXY"
  fi
  
  # Execute the assembled command
  echo "Executing: $TC_CMD"
  eval $TC_CMD
  
  # Verify (optional)
  tc qdisc show dev eth0
else
  echo "Gateway Entrypoint: Latency disabled or LATENCY_GATEWAY_TO_PROXY not set."
fi

echo "Gateway Entrypoint: Setup complete. Starting application..."
# Execute the main command passed to the script (CMD from Dockerfile or command from compose)
exec "$@"

================================================================================

Filename: gateway/gateway.py
Content:
import math
import os
import threading
import time
import requests
import json
import traceback
import socket
from flask import Flask, request, jsonify
from werkzeug.middleware.dispatcher import DispatcherMiddleware
import traceback

from shared_modules.client_module import ClientModule
from shared_modules.concentration_calculator_module import ConcentrationCalculatorModule
from shared_modules.connector_module import ConnectorModule


from shared_modules.metrics import *
from shared_modules.cpu_monitor import get_container_cpu_percent_non_blocking

# --- Metrics ---
MY_TIER = "gateway"
# General Metrics (already imported via metrics *)
# MODULE_EXECUTIONS, MODULE_LATENCY, MODULE_ERRORS, PASSTHROUGH_COUNT

# Gateway Specific Metrics
REQUEST_COUNT = Counter('gateway_requests_total', 'Total number of requests received by gateway')
REQUEST_LATENCY = Histogram('gateway_internal_processing_latency_seconds', 'Gateway internal processing latency (excluding upstream wait)')
FORWARD_TO_PROXY_COUNT = Counter('gateway_forward_to_proxy_total', 'Total requests forwarded to proxy')
FORWARD_TO_PROXY_LATENCY = Histogram('gateway_forward_to_proxy_latency_seconds', 'Latency for forwarding request to proxy (network RTT + proxy processing)')
FORWARD_TO_PROXY_FAILURES = Counter('gateway_forward_to_proxy_failures_total', 'Failures forwarding to proxy')
ERROR_COUNT = Counter('gateway_general_errors_total', 'Total general processing errors on gateway (outside modules)') # Renamed for clarity
CPU_UTILIZATION = Gauge('cpu_utilization_percent', 'Current CPU utilization percentage', ['container_name'])
container_name = socket.gethostname()

# --- CPU Monitoring (Keep as is) ---
def collect_cpu_metrics():
    while True:
        cpu_info = get_container_cpu_percent_non_blocking()
        if cpu_info:
            normalized_cpu = cpu_info.get('cpu_percent_normalized', math.nan)
            if not math.isnan(normalized_cpu): CPU_UTILIZATION.labels(container_name=container_name).set(normalized_cpu)
            else: CPU_UTILIZATION.labels(container_name=container_name).set(-1.0) # Indicate NaN
        else: CPU_UTILIZATION.labels(container_name=container_name).set(-2.0) # Indicate error getting info
        time.sleep(1)

def start_cpu_monitoring():
    cpu_thread = threading.Thread(target=collect_cpu_metrics, daemon=True)
    cpu_thread.start()
    print(f"Gateway ({container_name}): Background CPU monitoring started")
# ---

app = Flask(__name__)

# --- Configuration ---
proxy_url = os.getenv('PROXY_URL') # e.g., http://proxy_py:8000/process
try:
    gateway_processing_level = int(os.getenv('GATEWAY_PROCESSING_LEVEL', 2))
except ValueError:
    print(f"WARN ({container_name}): Invalid GATEWAY_PROCESSING_LEVEL, defaulting to 2.")
    gateway_processing_level = 2
effective_gateway_processing_level = max(0, gateway_processing_level)

print(f"--- Gateway Configuration ({container_name}) ---")
print(f"Proxy URL: {proxy_url}")
print(f"Gateway Processing Level (Config): {gateway_processing_level}")
print(f"Gateway Processing Level (Effective): {effective_gateway_processing_level}")
print(f"--------------------------------------")
# ---

# --- Initialize Modules Conditionally ---
client_module = None
concentration_calculator = None
connector_module = None

if effective_gateway_processing_level >= 1 and ClientModule:
    client_module = ClientModule()
    print(f"INFO ({container_name}): Client Module (L1) initialized on Gateway.")
elif effective_gateway_processing_level >= 1: print(f"WARN ({container_name}): L1 requested but module not found.")

if effective_gateway_processing_level >= 2 and ConcentrationCalculatorModule:
    # Check L1 dependency IF L1 is also supposed to run here
    if effective_gateway_processing_level >= 1 and not client_module:
        print(f"WARN ({container_name}): Cannot initialize Calculator (L>=2) if Client (L1) is not also active/found when Gateway level is >= 1. Degrading.")
        effective_gateway_processing_level = 0 # Degrade if L1 is missing but needed implicitly
    else:
        concentration_calculator = ConcentrationCalculatorModule()
        print(f"INFO ({container_name}): Concentration Calculator Module (L2) initialized on Gateway.")
elif effective_gateway_processing_level >= 2: print(f"WARN ({container_name}): L2 requested but module not found.")


if effective_gateway_processing_level >= 3 and ConnectorModule:
    if concentration_calculator: # Check direct dependency
        connector_module = ConnectorModule()
        print(f"INFO ({container_name}): Connector Module (L3) initialized on Gateway.")
    else:
        print(f"WARN ({container_name}): Cannot initialize Connector (L3) on Gateway without Calculator (L>=2). Degrading level.")
        effective_gateway_processing_level = min(effective_gateway_processing_level, 2 if concentration_calculator else 0) # Degrade
elif effective_gateway_processing_level >= 3: print(f"WARN ({container_name}): L3 requested but module not found.")
# ---

app.wsgi_app = DispatcherMiddleware(app.wsgi_app, { '/metrics': make_wsgi_app() })

@app.route('/health')
def health_check():
    return jsonify({'status': 'healthy'}), 200

# Renamed endpoint for clarity, receives data from MOBILE
@app.route('/', methods=['POST'])
def process_mobile_data():
    REQUEST_COUNT.inc()
    processing_start_time = time.time()
    level_received = 0
    current_data = None
    level_processed_here = 0 # Track highest level processed *on this gateway*
    processing_error = False
    final_response_to_mobile = ({"error": "Unknown gateway processing error"}, 500)

    try:
        if not request.is_json: raise TypeError("Request must be JSON")

        incoming_data_full = request.get_json()
        if not incoming_data_full or "payload" not in incoming_data_full or "last_processed_level" not in incoming_data_full:
            raise ValueError("Missing or invalid data structure from mobile")

        level_received = incoming_data_full.get("last_processed_level", 0)
        current_data = incoming_data_full.get("payload")
        level_processed_here = level_received # Start assuming no processing happens here

        print(f"Gateway ({container_name}, L{effective_gateway_processing_level}): Received data processed up to L{level_received}.")

        # --- Check for Passthrough First ---
        if effective_gateway_processing_level == 0 or level_received >= effective_gateway_processing_level:
            print(f"Gateway ({container_name}): Passthrough triggered (Received L{level_received}, Gateway Level {effective_gateway_processing_level})")
            PASSTHROUGH_COUNT.labels(tier=MY_TIER).inc()
            # Skip processing modules, data remains as received
            # level_processed_here remains level_received
        else:
            # --- Processing Pipeline (Only if not passthrough) ---

            # Level 1: Client
            if not processing_error and level_received < 1 and effective_gateway_processing_level >= 1 and client_module:
                module_name = "client"
                print(f"Gateway ({container_name}): Running {module_name} (L1)...")
                try:
                    with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                        client_output = client_module.process_eeg(current_data)
                    if not client_output:
                        processing_error = True # Stop processing
                        # Metric EEG_DISCARDED_TOTAL incremented inside client_module
                        final_response_to_mobile = ({"status": "data_discarded_by_gateway_client", "reason": "quality"}), 400 # Inform mobile
                    else:
                        current_data = client_output
                        level_processed_here = 1
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                        # Metric EEG_DATA_PROCESSED incremented inside client_module
                except Exception as client_exc:
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_mobile = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(client_exc)}), 500
                    print(f"ERROR ({container_name}) during {module_name}: {client_exc}")

            # Level 2: Calculator
            if not processing_error and level_received < 2 and effective_gateway_processing_level >= 2 and concentration_calculator:
                module_name = "calculator"
                print(f"Gateway ({container_name}): Running {module_name} (L2)...")
                if level_processed_here < 1: # Check dependency
                    dep_error_msg = f"Gateway {module_name} (L2) needs L1 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_mobile = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                    print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            calc_output = concentration_calculator.calculate_concentration(current_data)
                        if not calc_output or 'error' in calc_output: raise ValueError(f"{module_name} error: {calc_output.get('error', 'Unknown')}")
                        current_data = calc_output
                        level_processed_here = 2
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as calc_exc:
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True; final_response_to_mobile = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(calc_exc)}), 500
                        print(f"ERROR ({container_name}) during {module_name}: {calc_exc}")


            # Level 3: Connector
            if not processing_error and level_received < 3 and effective_gateway_processing_level >= 3 and connector_module:
                module_name = "connector"
                print(f"Gateway ({container_name}): Running {module_name} (L3)...")
                if level_processed_here < 2: # Check dependency
                    dep_error_msg = f"Gateway {module_name} (L3) needs L2 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_mobile = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                    print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            conn_output = connector_module.process_concentration_data(current_data)
                        # --- CALCULATE AND RECORD E2E LATENCY ---
                        if conn_output and 'error' not in conn_output:
                            creation_time = current_data.get('creation_time') # Get from data BEFORE overwriting
                            if creation_time:
                                e2e_latency = time.time() - creation_time
                                E2E_LATENCY.labels(final_tier=MY_TIER).observe(e2e_latency)
                                print(f"Gateway ({container_name}) ReqID:{request_id[-6:]}: L3 Complete. E2E Latency: {e2e_latency:.4f}s")
                            else:
                                print(f"WARN ({container_name}) ReqID:{request_id[-6:]}: Missing creation_time for E2E latency calc.")
                        # ---------------------------------------

                        
                        if not conn_output or 'error' in conn_output: raise ValueError(f"{module_name} error: {conn_output.get('error', 'Unknown')}")
                        current_data = conn_output
                        level_processed_here = 3
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as conn_exc:
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True; final_response_to_mobile = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(conn_exc)}), 500
                        print(f"ERROR ({container_name}) during {module_name}: {conn_exc}")


        # Record internal processing time (might be ~0 for passthrough)
        internal_processing_duration = time.time() - processing_start_time
        REQUEST_LATENCY.observe(internal_processing_duration)

        # --- Forwarding Decision ---
        if not processing_error:
            if level_processed_here < 3: # Need to forward UPWARDS
                if proxy_url:
                    data_to_forward = {"payload": current_data, "last_processed_level": level_processed_here}
                    print(f"Gateway ({container_name}): Forwarding data (processed up to L{level_processed_here}) to Proxy ({proxy_url})...")
                    forward_start_time = time.time()
                    try:
                        proxy_response = requests.post(proxy_url, json=data_to_forward, timeout=(5, 10)) # connect, read
                        forward_duration = time.time() - forward_start_time
                        FORWARD_TO_PROXY_LATENCY.observe(forward_duration) # Observe RTT + Proxy time
                        proxy_response.raise_for_status()
                        FORWARD_TO_PROXY_COUNT.inc()
                        print(f"Gateway ({container_name}): Forward success to Proxy. RTT+ProxyTime: {forward_duration:.4f}s")
                        # --- IMPORTANT: Relay proxy's response back to mobile ---
                        try:
                            # Add the level processed *here* for mobile's info
                            response_payload = proxy_response.json()
                            response_payload['processed_up_to'] = response_payload.get('processed_up_to', level_processed_here) # Pass highest level seen
                            final_response_to_mobile = (response_payload, proxy_response.status_code)
                        except json.JSONDecodeError:
                            final_response_to_mobile = ({"status": f"proxy_status_{proxy_response.status_code}_no_json", "processed_up_to": level_processed_here}), proxy_response.status_code
                    except requests.exceptions.RequestException as e:
                        FORWARD_TO_PROXY_FAILURES.inc()
                        error_detail = f"{type(e).__name__}: {e}"
                        print(f"ERROR ({container_name}): Failed to forward to Proxy: {error_detail}")
                        final_response_to_mobile = ({"status": "forward_to_proxy_failed", "processed_up_to": level_processed_here, "detail": error_detail}), 502
                else: # Cannot forward
                    print(f"WARN ({container_name}): PROXY_URL not set, cannot forward incomplete processing (L{level_processed_here}).")
                    final_response_to_mobile = ({"status": f"processed_L{level_processed_here}_cannot_forward_no_proxy"}), 500
            else: # level_processed_here == 3 (Final processing done here on Gateway)
                print(f"Gateway ({container_name}): Final processing complete (L3).")
                # Structure the response for mobile
                final_response_to_mobile = ({"status": "processing_complete", "final_payload_preview": json.dumps(current_data)[:100], "processed_up_to": 3}), 200

        # Return the determined response and status code TO THE MOBILE
        return jsonify(final_response_to_mobile[0]), final_response_to_mobile[1]

    except (TypeError, ValueError) as req_err: # Catch specific request format errors
        ERROR_COUNT.inc()
        print(f"ERROR ({container_name}): Invalid request data: {req_err}")
        return jsonify({"error": f"Bad Request: {req_err}"}), 400
    except Exception as e: # Catch all other unexpected errors
        ERROR_COUNT.inc()
        # Still record latency if possible
        internal_processing_duration = time.time() - processing_start_time
        REQUEST_LATENCY.observe(internal_processing_duration)
        print(f"FATAL Error in Gateway ({container_name}): {type(e).__name__} - {e}")
        print(traceback.format_exc())
        # Return generic error to mobile
        return jsonify({"error": "Internal server error on gateway"}), 500

if __name__ == '__main__':
    start_cpu_monitoring()
    app.run(host='0.0.0.0', port=8000)

================================================================================

Filename: gateway/requirements.txt
Content:
flask
requests
numpy
prometheus-client==0.17.1
flask-prometheus-metrics==1.0.0
PyYAML==6.0.1
psutil

================================================================================

Filename: mobile/Dockerfile
Content:
FROM python:3.12-slim

# Add labels for better container identification
LABEL maintainer="CS300 Team" \
    component="mobile" \
    version="1.0"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    iproute2 \
    curl \
    && rm -rf /var/lib/apt/lists/*
WORKDIR /app

COPY mobile/requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY mobile/mobile.py ./
COPY mobile/entrypoint.sh ./
COPY shared_modules ./shared_modules 

RUN chmod +x ./entrypoint.sh

HEALTHCHECK --interval=30s --timeout=3s \
    CMD curl -f http://localhost:9090/health || exit 1

ENTRYPOINT ["./entrypoint.sh"]
CMD ["python", "mobile.py"]

================================================================================

Filename: mobile/entrypoint.sh
Content:
#!/bin/sh
# mobile/entrypoint.sh

echo "Gateway Entrypoint: Starting setup..."
echo "DEBUG: ENABLE_LATENCY is set to: [$ENABLE_LATENCY]"
echo "DEBUG: LATENCY_MOBILE_TO_GATEWAY is set to: [$LATENCY_MOBILE_TO_GATEWAY]"

# Apply tc rules if enabled and variable is set
if [ "$ENABLE_LATENCY" = "true" ] && [ -n "$LATENCY_MOBILE_TO_GATEWAY" ]; then
  echo "Gateway Entrypoint: Applying $LATENCY_MOBILE_TO_GATEWAY delay to eth0 (towards Proxy/Mobiles)"
  
  # Build the tc command based on which parameters are set
  TC_CMD="tc qdisc replace dev eth0 root netem delay ${LATENCY_MOBILE_TO_GATEWAY:-0ms}"
  
  # Add jitter parameter only if it's set
  if [ -n "$JITTER_MOBILE_TO_GATEWAY" ]; then
    TC_CMD="$TC_CMD $JITTER_MOBILE_TO_GATEWAY"
  fi
  
  # Add loss parameter only if it's set
  if [ -n "$LOSS_MOBILE_TO_GATEWAY" ]; then
    TC_CMD="$TC_CMD loss $LOSS_MOBILE_TO_GATEWAY"
  fi
  
  # Execute the assembled command
  echo "Executing: $TC_CMD"
  eval $TC_CMD
  
  # Verify (optional)
  tc qdisc show dev eth0
else
  echo "Gateway Entrypoint: Latency disabled or LATENCY_MOBILE_TO_GATEWAY not set."
fi

echo "Gateway Entrypoint: Setup complete. Starting application..."
# Execute the main command passed to the script (CMD from Dockerfile or command from compose)
exec "$@"

================================================================================

Filename: mobile/mobile.py
Content:
import math
import os
import threading
import time
import uuid
import requests
import json
import socket
import numpy as np
from flask import Flask
from werkzeug.middleware.dispatcher import DispatcherMiddleware
import traceback

from shared_modules.client_module import ClientModule
from shared_modules.concentration_calculator_module import ConcentrationCalculatorModule
from shared_modules.connector_module import ConnectorModule

from shared_modules.cpu_monitor import get_container_cpu_percent_non_blocking
from shared_modules.metrics import * # Imports Counters, Gauges etc.

MY_TIER = "mobile"

app = Flask(__name__)

# --- Metrics & Monitoring Setup ---
CPU_UTILIZATION = Gauge('cpu_utilization_percent', 'Current CPU utilization percentage', ['container_name'])
container_name = socket.gethostname()

def collect_cpu_metrics():
    while True:
        cpu_info = get_container_cpu_percent_non_blocking()
        if cpu_info:
            normalized_cpu = cpu_info.get('cpu_percent_normalized', math.nan)
            if not math.isnan(normalized_cpu): CPU_UTILIZATION.labels(container_name=container_name).set(normalized_cpu)
            else: CPU_UTILIZATION.labels(container_name=container_name).set(-1.0) # Indicate NaN
        else: CPU_UTILIZATION.labels(container_name=container_name).set(-2.0) # Indicate error getting info
        time.sleep(1) # Check frequency

def start_cpu_monitoring():
    cpu_thread = threading.Thread(target=collect_cpu_metrics, daemon=True)
    cpu_thread.start()
    print(f"Mobile ({container_name}): Background CPU monitoring started")

@app.route('/health')
def health_check():
    return 'healthy', 200

app.wsgi_app = DispatcherMiddleware(app.wsgi_app, { '/metrics': make_wsgi_app() })
# --- Configuration ---
gateway_name = os.getenv('GATEWAY')
gateway_url = f'http://{gateway_name}:8000/' if gateway_name else None
try:
    mobile_processing_level = int(os.getenv('MOBILE_PROCESSING_LEVEL', 1))
except ValueError:
    print(f"WARN ({container_name}): Invalid MOBILE_PROCESSING_LEVEL, defaulting to 1.")
    mobile_processing_level = 1
effective_mobile_processing_level = max(0, mobile_processing_level)

print(f"--- Mobile Configuration ({container_name}) ---")
print(f"Gateway URL: {gateway_url}")
print(f"Mobile Processing Level (Config): {mobile_processing_level}")
print(f"Mobile Processing Level (Effective): {effective_mobile_processing_level}")
print(f"------------------------------------------")
# ---

# --- Sensor Simulator (Keep as is) ---
class SensorSimulator:
    def __init__(self, transmission_time=0.1, sampling_rate=250):   
        self.transmission_time = transmission_time
        self.last_transmission_time = 0
        self.sampling_rate = sampling_rate
        self.base_amplitude = 1.0
        self.noise_level = 0.001

    def generate_eeg_data(self):
        current_time = time.time()
        t = np.linspace(0, self.transmission_time, int(self.sampling_rate * self.transmission_time), endpoint=False)
        alpha_freq = 10
        alpha_wave = self.base_amplitude * np.sin(2 * np.pi * alpha_freq * t)
        noise = np.random.normal(0, self.base_amplitude * self.noise_level, len(t))
        eeg_signal = alpha_wave + noise
        return {
            "eeg_values": eeg_signal.tolist(),
            "timestamp": current_time,
            "sampling_rate": self.sampling_rate,
            "creation_time": time.time(),
            "request_id": str(uuid.uuid4()),
        }
# ---

# --- Gateway Connector (Keep as is) ---
class GatewayConnector:
    def __init__(self, gateway_url, max_retries=3, retry_delay=1):
        self.gateway_url = gateway_url
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.session = requests.Session()
        self.session.headers.update({'Content-Type': 'application/json', 'User-Agent': f'EEGMobileClient/1.0 ({container_name})'})

    def send_data(self, data_to_send: dict):
        if not self.gateway_url:
            print(f"ERROR ({container_name}): Gateway URL not set.")
            return None
        if not data_to_send or "payload" not in data_to_send or "last_processed_level" not in data_to_send:
            print(f"ERROR ({container_name}): Invalid data structure for send_data: {data_to_send}")
            return None

        for attempt in range(self.max_retries):
            start_time_gw = time.time()
            try:
                response = self.session.post(self.gateway_url, json=data_to_send, timeout=(5, 10)) # 5s connect, 10s read
                request_time = time.time() - start_time_gw
                GATEWAY_REQUEST_LATENCY.set(request_time) # Gauge for RTT

                response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

                try:
                    ack_data = response.json()
                    print(f"Mobile ({container_name}): Received ACK/Response (L{ack_data.get('processed_up_to', 'N/A')}) from {gateway_name}. RTT: {request_time:.3f}s")
                    return ack_data
                except json.JSONDecodeError:
                    print(f"Mobile ({container_name}): Received non-JSON ACK from {gateway_name} (Status: {response.status_code}). RTT: {request_time:.3f}s")
                    return {"status": f"gateway_status_{response.status_code}"} # Return status info

            except requests.exceptions.Timeout:
                print(f"Mobile ({container_name}): Attempt {attempt + 1}/{self.max_retries} to {gateway_name} timed out.")
                GATEWAY_REQUEST_FAILURES.inc()
            except requests.exceptions.ConnectionError:
                print(f"Mobile ({container_name}): Attempt {attempt + 1}/{self.max_retries} connection to {gateway_name} failed.")
                GATEWAY_REQUEST_FAILURES.inc()
            except requests.exceptions.HTTPError as http_err:
                print(f"Mobile ({container_name}): Attempt {attempt + 1}/{self.max_retries} HTTP error from {gateway_name}: {http_err}")
                # Could potentially increment a different counter for server-side errors vs network errors
                GATEWAY_REQUEST_FAILURES.inc() # Count as failure for now
            except requests.exceptions.RequestException as e:
                print(f"Mobile ({container_name}): Attempt {attempt + 1}/{self.max_retries} generic request error to {gateway_name}: {type(e).__name__}")
                GATEWAY_REQUEST_FAILURES.inc()

            if attempt < self.max_retries - 1:
                time.sleep(self.retry_delay * (attempt + 1))
            else:
                print(f"Mobile ({container_name}): Failed to send data to {gateway_name} after all retries.")
                # Consider setting latency gauge to a high value or specific error code?
        return None # Failed after retries
# ---

# --- Initialize Modules Conditionally ---
client_module = None
concentration_calculator = None
connector_module = None

if effective_mobile_processing_level >= 1 and ClientModule:
    client_module = ClientModule()
    print(f"INFO ({container_name}): Client Module (L1) initialized on Mobile.")
if effective_mobile_processing_level >= 2 and ConcentrationCalculatorModule:
    concentration_calculator = ConcentrationCalculatorModule()
    print(f"INFO ({container_name}): Concentration Calculator Module (L2) initialized on Mobile.")
else:
    if effective_mobile_processing_level >= 2: print(f"WARN ({container_name}): L2 requested but module not found.")

if effective_mobile_processing_level >= 3 and ConnectorModule:
    if concentration_calculator: # Check dependency
        connector_module = ConnectorModule()
        print(f"INFO ({container_name}): Connector Module (L3) initialized on Mobile.")
    else:
        print(f"WARN ({container_name}): Cannot initialize Connector (L3) on Mobile without Calculator (L>=2). Degrading level.")
        effective_mobile_processing_level = min(effective_mobile_processing_level, 1) # Degrade gracefully
else:
    if effective_mobile_processing_level >= 3: print(f"WARN ({container_name}): L3 requested but module not found.")
# ---

# --- Initialize Sensor and Connector ---
sensor = SensorSimulator(transmission_time=0.1, sampling_rate=250)
gateway_connector = GatewayConnector(gateway_url)
# ---

if __name__ == '__main__':
    print(f"Mobile client started ({container_name}, Effective Level {effective_mobile_processing_level}) - connecting to gateway at {gateway_url}")
    start_cpu_monitoring()

    flask_thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=9090, debug=False, use_reloader=False), daemon=True)
    flask_thread.start()

    last_send_attempt_time = 0
    send_interval = 0.1 # Target ~10Hz send rate

    while True:
        main_loop_start_time = time.time()
        try:
            raw_eeg_data = sensor.generate_eeg_data()
            if not raw_eeg_data: continue

            request_id = raw_eeg_data.get("request_id", "unknown") # Get ID for logging
            print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Generated data.")
            
            level_received = 0 # Input is always L0 for mobile
            current_data = raw_eeg_data
            level_processed_here = 0 # Track highest level processed *in this service*
            processing_error = False

            # --- Processing Pipeline ---
            # Only run modules if the effective level allows AND the data hasn't reached that level yet

            # Level 1: Client
            if not processing_error and level_received < 1 and effective_mobile_processing_level >= 1 and client_module:
                module_name = "client"
                print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Running {module_name} (L1)...")
                try:
                    with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                        client_output = client_module.process_eeg(current_data)

                    if not client_output:
                        # Data discarded by client (quality)
                        processing_error = True # Stop processing this data point
                        # Metric EEG_DISCARDED_TOTAL incremented inside client_module
                    else:
                        current_data = client_output
                        level_processed_here = 1
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                        # Metric EEG_DATA_PROCESSED incremented inside client_module
                except Exception as client_exc:
                    print(f"ERROR ({container_name}) during {module_name}: {client_exc}")
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True

            # Level 2: Calculator
            if not processing_error and level_received < 2 and effective_mobile_processing_level >= 2 and concentration_calculator:
                module_name = "calculator"
                print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Running {module_name} (L2)...")
                if level_processed_here < 1: # Check dependency
                    dep_error_msg = f"Mobile {module_name} (L2) needs L1 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            calc_output = concentration_calculator.calculate_concentration(current_data)
                        if not calc_output or 'error' in calc_output: raise ValueError(f"{module_name} error: {calc_output.get('error', 'Unknown')}")
                        current_data = calc_output
                        level_processed_here = 2
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as calc_exc:
                        print(f"ERROR ({container_name}) during {module_name}: {calc_exc}")
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True

            # Level 3: Connector
            if not processing_error and level_received < 3 and effective_mobile_processing_level >= 3 and connector_module:
                module_name = "connector"
                print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Running {module_name} (L3)...")
                
                if level_processed_here < 2: # Check dependency
                    dep_error_msg = f"Mobile {module_name} (L3) needs L2 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            conn_output = connector_module.process_concentration_data(current_data)
                        # --- CALCULATE AND RECORD E2E LATENCY ---
                        if conn_output and 'error' not in conn_output:
                            creation_time = current_data.get('creation_time')
                            if creation_time:
                                e2e_latency = time.time() - creation_time
                                E2E_LATENCY.labels(final_tier=MY_TIER).observe(e2e_latency)
                                print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: L3 Complete. E2E Latency: {e2e_latency:.4f}s")
                            else:
                                print(f"WARN ({container_name}) ReqID:{request_id[-6:]}: Missing creation_time for E2E latency calc.")
                        # ---------------------------------------
                        if not conn_output or 'error' in conn_output: raise ValueError(f"{module_name} error: {conn_output.get('error', 'Unknown')}")
                        current_data = conn_output
                        level_processed_here = 3
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as conn_exc:
                        print(f"ERROR ({container_name}) during {module_name}: {conn_exc}")
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True


            # --- Sending Logic ---
            current_time = time.time()
            if current_time - last_send_attempt_time >= send_interval:
                last_send_attempt_time = current_time
                if not processing_error:
                    # Always send unless processing is fully complete *and* no higher tier exists
                    # (Mobile always has a higher tier - Gateway)
                    if level_processed_here < 3 : # Always true if L3 not run on mobile
                        if gateway_connector.gateway_url:
                            data_to_send = {
                                "payload": current_data,
                                "last_processed_level": level_processed_here # Send the level achieved *here*
                            }
                            
                            print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Sending data (processed up to L{level_processed_here}) to Gateway...")
                            # Response handling can update local state if needed, e.g., display
                            response_from_upstream = gateway_connector.send_data(data_to_send)
                            if response_from_upstream and client_module:
                                # Example: update display based on final result if it comes back
                                final_level = response_from_upstream.get('final_concentration_level')
                                if final_level: client_module.update_concentration_display(response_from_upstream)
                        else:
                            print(f"WARN ({container_name}): No Gateway URL, cannot send.")
                    else: # level_processed_here == 3
                        print(f"Mobile ({container_name}) ReqID:{request_id[-6:]}: Processing finished locally (L3). Not sending.")
                        if client_module: client_module.update_concentration_display(current_data) # Update display
                        # No send needed if mobile does everything


                elif processing_error: # If there was an error during processing
                    print(f"Mobile ({container_name}): Send skipped due to processing error/discard.")
                    # Optionally send an error status upstream? For now, just skip.

        except KeyboardInterrupt:
            print(f"\nShutting down mobile client ({container_name})...")
            break
        except Exception as e:
            print(f"FATAL Error in mobile main loop ({container_name}): {type(e).__name__} - {str(e)}")
            print(traceback.format_exc())
            time.sleep(5) # Avoid rapid looping on fatal error

        # --- Loop Speed Control ---
        elapsed_time = time.time() - main_loop_start_time
        sleep_time = max(0, send_interval - elapsed_time) # Try to adhere to send_interval
        time.sleep(sleep_time)

================================================================================

Filename: mobile/requirements.txt
Content:
flask
requests
matplotlib
numpy
psutil
prometheus-client
flask-prometheus-metrics
PyYAML

================================================================================

Filename: proxy/Dockerfile
Content:
FROM nginx:latest

COPY main.conf /etc/nginx/nginx.conf
COPY nginx.conf /etc/nginx/conf.d/nginx.conf
RUN apt-get update && apt-get install -y iproute2

COPY entrypoint.sh /entrypoint.sh 
RUN chmod +x /entrypoint.sh     

ENTRYPOINT ["/entrypoint.sh"]
CMD ["nginx", "-g", "daemon off;"]

================================================================================

Filename: proxy/default.conf
Content:
upstream gateways {
    server gateway1:8000;
    server gateway2:8000;
    keepalive 32;
}

server {
    listen 80 default_server;
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    location /health {
        return 200 'healthy';
        add_header Content-Type text/plain;
    }

    location / {
        proxy_pass http://gateways;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        proxy_next_upstream_timeout 10s;
    }
}

================================================================================

Filename: proxy/entrypoint.sh
Content:
#!/bin/sh
# proxy/entrypoint.sh

echo "Gateway Entrypoint: Starting setup..."
echo "DEBUG: ENABLE_LATENCY is set to: [$ENABLE_LATENCY]"
echo "DEBUG: LATENCY_PROXY_TO_CLOUD is set to: [$LATENCY_PROXY_TO_CLOUD]"

# Apply tc rules if enabled and variable is set
if [ "$ENABLE_LATENCY" = "true" ] && [ -n "$LATENCY_PROXY_TO_CLOUD" ]; then
  echo "Gateway Entrypoint: Applying $LATENCY_PROXY_TO_CLOUD delay to eth0 (towards Proxy/Mobiles)"
  
  # Build the tc command based on which parameters are set
  TC_CMD="tc qdisc replace dev eth0 root netem delay ${LATENCY_PROXY_TO_CLOUD:-0ms}"
  
  # Add jitter parameter only if it's set
  if [ -n "$JITTER_PROXY_TO_CLOUD" ]; then
    TC_CMD="$TC_CMD $JITTER_PROXY_TO_CLOUD"
  fi
  
  # Add loss parameter only if it's set
  if [ -n "$LOSS_PROXY_TO_CLOUD" ]; then
    TC_CMD="$TC_CMD loss $LOSS_PROXY_TO_CLOUD"
  fi
  
  # Execute the assembled command
  echo "Executing: $TC_CMD"
  eval $TC_CMD
  
  # Verify (optional)
  tc qdisc show dev eth0
else
  echo "Gateway Entrypoint: Latency disabled or LATENCY_PROXY_TO_CLOUD not set."
fi

echo "Gateway Entrypoint: Setup complete. Starting application..."
# Execute the main command passed to the script (CMD from Dockerfile or command from compose)
exec "$@"

================================================================================

Filename: proxy/main.conf
Content:
events {
    worker_connections 1024;
}

http {
    include /etc/nginx/conf.d/nginx.conf;
}

================================================================================

Filename: proxy/nginx.conf
Content:
upstream gateways {
    server gateway1:8000;
    server gateway2:8000;
    keepalive 32;
}

server {
    listen 80 default_server;
    resolver 127.0.0.11 valid=10s;
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    location /health {
        return 200 'healthy';
        add_header Content-Type text/plain;
    }

    location / {
        proxy_pass http://gateways;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        proxy_next_upstream_timeout 10s;
    }
}

================================================================================

Filename: proxy/requirements.txt
Content:
flask
requests 
prometheus-client

================================================================================

Filename: proxy_py/Dockerfile
Content:
FROM python:3.12-slim

LABEL maintainer="CS300 Team" component="proxy_py" version="1.0"

RUN apt-get update && apt-get install -y \
    iproute2 \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY proxy_py/requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY proxy_py/proxy_app.py ./
COPY proxy_py/entrypoint.sh ./
COPY shared_modules ./shared_modules 

RUN chmod +x ./entrypoint.sh

HEALTHCHECK --interval=30s --timeout=3s \
    CMD curl -f http://localhost:8000/health || exit 1

ENTRYPOINT ["./entrypoint.sh"]
CMD ["python", "proxy_app.py"]


================================================================================

Filename: proxy_py/entrypoint.sh
Content:
#!/bin/sh
# gateway/entrypoint.sh

echo "Proxy Entrypoint: Starting setup..."
echo "DEBUG: ENABLE_LATENCY is set to: [$ENABLE_LATENCY]"
echo "DEBUG: LATENCY_PROXY_TO_CLOUD is set to: [$LATENCY_PROXY_TO_CLOUD]"

# Apply tc rules if enabled and variable is set
if [ "$ENABLE_LATENCY" = "true" ] && [ -n "$LATENCY_PROXY_TO_CLOUD" ]; then
  echo "Proxy Entrypoint: Applying $LATENCY_PROXY_TO_CLOUD delay to eth0 (towards Proxy/Mobiles)"
  
  # Build the tc command based on which parameters are set
  TC_CMD="tc qdisc replace dev eth0 root netem delay ${LATENCY_PROXY_TO_CLOUD:-0ms}"
  
  # Add jitter parameter only if it's set
  if [ -n "$JITTER_PROXY_TO_CLOUD" ]; then
    TC_CMD="$TC_CMD $JITTER_PROXY_TO_CLOUD"
  fi
  
  # Add loss parameter only if it's set
  if [ -n "$LOSS_PROXY_TO_CLOUD" ]; then
    TC_CMD="$TC_CMD loss $LOSS_PROXY_TO_CLOUD"
  fi
  
  # Execute the assembled command
  echo "Executing: $TC_CMD"
  eval $TC_CMD
  
  # Verify (optional)
  tc qdisc show dev eth0
else
  echo "Proxy Entrypoint: Latency disabled or LATENCY_PROXY_TO_CLOUD not set."
fi

echo "Proxy Entrypoint: Setup complete. Starting application..."
# Execute the main command passed to the script (CMD from Dockerfile or command from compose)
exec "$@"

================================================================================

Filename: proxy_py/proxy_app.py
Content:
import math
import os
import threading
import time
import requests
import json
import traceback
import socket
from flask import Flask, request, jsonify
from werkzeug.middleware.dispatcher import DispatcherMiddleware
import traceback

from shared_modules.client_module import ClientModule
from shared_modules.concentration_calculator_module import ConcentrationCalculatorModule
from shared_modules.connector_module import ConnectorModule

from shared_modules.metrics import *
from shared_modules.cpu_monitor import get_container_cpu_percent_non_blocking

# --- Metrics ---
MY_TIER = "proxy"
# General Metrics (already imported via metrics *)
# MODULE_EXECUTIONS, MODULE_LATENCY, MODULE_ERRORS, PASSTHROUGH_COUNT

# Proxy Specific Metrics
PROXY_REQUEST_COUNT = Counter('proxy_requests_total', 'Total requests received by proxy')
PROXY_INTERNAL_LATENCY = Histogram('proxy_internal_processing_latency_seconds', 'Proxy internal processing latency (excluding upstream wait)')
FORWARD_TO_CLOUD_COUNT = Counter('proxy_forward_to_cloud_total', 'Total requests forwarded to cloud')
FORWARD_TO_CLOUD_LATENCY = Histogram('proxy_forward_to_cloud_latency_seconds', 'Latency for forwarding request to cloud (RTT + cloud processing)')
FORWARD_TO_CLOUD_FAILURES = Counter('proxy_forward_to_cloud_failures_total', 'Failures forwarding to cloud')
PROXY_ERROR_COUNT = Counter('proxy_general_errors_total', 'Total general errors in proxy (outside modules)')
CPU_UTILIZATION = Gauge('cpu_utilization_percent', 'CPU utilization', ['container_name'])
container_name = socket.gethostname()

app = Flask(__name__)

# --- Configuration ---
try:
    proxy_processing_level = int(os.getenv('PROXY_PROCESSING_LEVEL', 3)) # Default higher for proxy
except ValueError:
    print(f"WARN ({container_name}): Invalid PROXY_PROCESSING_LEVEL, defaulting to 3.")
    proxy_processing_level = 3
effective_proxy_processing_level = max(0, proxy_processing_level)

cloud_url = os.getenv('CLOUD_URL') # e.g., http://cloud_py:8000/

print(f"--- Python Proxy Configuration ({container_name}) ---")
print(f"Proxy Processing Level (Config): {proxy_processing_level}")
print(f"Proxy Processing Level (Effective): {effective_proxy_processing_level}")
print(f"Cloud Forward URL: {cloud_url}")
print(f"------------------------------------------")
# ---

# --- Initialize Modules Conditionally ---
client_module = None
concentration_calculator = None
connector_module = None

if effective_proxy_processing_level >= 1 and ClientModule:
    client_module = ClientModule()
    print(f"INFO ({container_name}): Client Module (L1) initialized on Proxy.")
elif effective_proxy_processing_level >= 1: print(f"WARN ({container_name}): L1 requested but module not found.")

if effective_proxy_processing_level >= 2 and ConcentrationCalculatorModule:
    # Check L1 dependency IF L1 is also supposed to run here
    if effective_proxy_processing_level >= 1 and not client_module:
        print(f"WARN ({container_name}): Cannot initialize Calculator (L>=2) if Client (L1) is not also active/found when Proxy level is >= 1. Degrading.")
        effective_proxy_processing_level = 0
    else:
        concentration_calculator = ConcentrationCalculatorModule()
        print(f"INFO ({container_name}): Concentration Calculator Module (L2) initialized on Proxy.")
elif effective_proxy_processing_level >= 2: print(f"WARN ({container_name}): L2 requested but module not found.")

if effective_proxy_processing_level >= 3 and ConnectorModule:
    if concentration_calculator: # Check direct dependency
        connector_module = ConnectorModule()
        print(f"INFO ({container_name}): Connector Module (L3) initialized on Proxy.")
    else:
        print(f"WARN ({container_name}): Cannot initialize Connector (L3) on Proxy without Calculator (L>=2). Degrading level.")
        # Degrade to L2 if calc exists, else L0 if L1 was missing too
        effective_proxy_processing_level = min(effective_proxy_processing_level, 2 if concentration_calculator else 0)
elif effective_proxy_processing_level >= 3: print(f"WARN ({container_name}): L3 requested but module not found.")
# ---

# --- CPU Monitoring (Keep as is) ---
def collect_cpu_metrics():
    while True:
        cpu_info = get_container_cpu_percent_non_blocking()
        if cpu_info:
            normalized_cpu = cpu_info.get('cpu_percent_normalized', math.nan)
            if not math.isnan(normalized_cpu): CPU_UTILIZATION.labels(container_name=container_name).set(normalized_cpu)
            else: CPU_UTILIZATION.labels(container_name=container_name).set(-1.0) # Indicate NaN
        else: CPU_UTILIZATION.labels(container_name=container_name).set(-2.0) # Indicate error getting info
        time.sleep(1)

def start_cpu_monitoring():
    cpu_thread = threading.Thread(target=collect_cpu_metrics, daemon=True)
    cpu_thread.start()
    print(f"Proxy ({container_name}): Background CPU monitoring started")
# ---

# --- Metrics Endpoint ---
app.wsgi_app = DispatcherMiddleware(app.wsgi_app, { '/metrics': make_wsgi_app() })

@app.route('/health')
def health_check():
    return 'healthy', 200

# Renamed endpoint, receives data from the GATEWAY
@app.route('/', methods=['POST'])
def process_gateway_data():
    PROXY_REQUEST_COUNT.inc()
    processing_start_time = time.time()
    level_received = 0
    current_data = None
    level_processed_here = 0 # Track highest level processed *on this proxy*
    processing_error = False
    # The response goes back to the GATEWAY
    final_response_to_gateway = ({"error": "Unknown proxy processing error"}, 500)

    try:
        if not request.is_json: raise TypeError("Request must be JSON")

        incoming_data_full = request.get_json()
        if not incoming_data_full or "payload" not in incoming_data_full or "last_processed_level" not in incoming_data_full:
            raise ValueError("Missing or invalid data structure from gateway")

        level_received = incoming_data_full.get("last_processed_level", 0)
        current_data = incoming_data_full.get("payload")
        level_processed_here = level_received # Start assuming no processing here
        
        request_id = current_data.get("request_id", "unknown") if isinstance(current_data, dict) else "unknown"
        print(f"Proxy ({container_name}, L{effective_proxy_processing_level}): Received data processed up to L{level_received}.")

        # --- Check for Passthrough ---
        if effective_proxy_processing_level == 0 or level_received >= effective_proxy_processing_level:
            print(f"Proxy ({container_name}): Passthrough triggered (Received L{level_received}, Proxy Level {effective_proxy_processing_level})")
            PASSTHROUGH_COUNT.labels(tier=MY_TIER).inc()
        else:
            # --- Processing Pipeline ---

            # Level 1: Client
            if not processing_error and level_received < 1 and effective_proxy_processing_level >= 1 and client_module:
                module_name = "client"
                print(f"Proxy ({container_name}): Running {module_name} (L1)...")
                try:
                    with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                        client_output = client_module.process_eeg(current_data)
                    if not client_output:
                        processing_error = True
                        final_response_to_gateway = ({"status": "data_discarded_by_proxy_client", "reason": "quality"}), 400
                    else:
                        current_data = client_output
                        level_processed_here = 1
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                except Exception as client_exc:
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_gateway = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(client_exc)}), 500
                    print(f"ERROR ({container_name}) during {module_name}: {client_exc}")

            # Level 2: Calculator
            if not processing_error and level_received < 2 and effective_proxy_processing_level >= 2 and concentration_calculator:
                module_name = "calculator"
                print(f"Proxy ({container_name}): Running {module_name} (L2)...")
                if level_processed_here < 1: # Check dependency
                    dep_error_msg = f"Proxy {module_name} (L2) needs L1 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_gateway = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                    print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            calc_output = concentration_calculator.calculate_concentration(current_data)
                        if not calc_output or 'error' in calc_output: raise ValueError(f"{module_name} error: {calc_output.get('error', 'Unknown')}")
                        current_data = calc_output
                        level_processed_here = 2
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as calc_exc:
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True; final_response_to_gateway = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(calc_exc)}), 500
                        print(f"ERROR ({container_name}) during {module_name}: {calc_exc}")

            # Level 3: Connector
            if not processing_error and level_received < 3 and effective_proxy_processing_level >= 3 and connector_module:
                module_name = "connector"
                print(f"Proxy ({container_name}): Running {module_name} (L3)...")
                if level_processed_here < 2: # Check dependency
                    dep_error_msg = f"Proxy {module_name} (L3) needs L2 input, but only reached L{level_processed_here}."
                    MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                    processing_error = True; final_response_to_gateway = ({"status":"dependency_error", "detail": dep_error_msg}), 500
                    print(f"ERROR ({container_name}): {dep_error_msg}")
                else:
                    try:
                        with MODULE_LATENCY.labels(tier=MY_TIER, module=module_name).time():
                            conn_output = connector_module.process_concentration_data(current_data)
                        # --- CALCULATE AND RECORD E2E LATENCY ---
                        if conn_output and 'error' not in conn_output:
                            creation_time = current_data.get('creation_time') # Get from data BEFORE overwriting
                            if creation_time:
                                e2e_latency = time.time() - creation_time
                                E2E_LATENCY.labels(final_tier=MY_TIER).observe(e2e_latency)
                                print(f"Gateway ({container_name}) ReqID:{request_id[-6:]}: L3 Complete. E2E Latency: {e2e_latency:.4f}s")
                            else:
                                print(f"WARN ({container_name}) ReqID:{request_id[-6:]}: Missing creation_time for E2E latency calc.")
                        # ---------------------------------------

                        
                        if not conn_output or 'error' in conn_output: raise ValueError(f"{module_name} error: {conn_output.get('error', 'Unknown')}")
                        current_data = conn_output
                        level_processed_here = 3
                        MODULE_EXECUTIONS.labels(tier=MY_TIER, module=module_name).inc()
                    except Exception as conn_exc:
                        MODULE_ERRORS.labels(tier=MY_TIER, module=module_name).inc()
                        processing_error = True; final_response_to_gateway = ({"status": f"{module_name}_error_on_{MY_TIER}", "detail": str(conn_exc)}), 500
                        print(f"ERROR ({container_name}) during {module_name}: {conn_exc}")


        # Record internal processing time
        internal_processing_duration = time.time() - processing_start_time
        PROXY_INTERNAL_LATENCY.observe(internal_processing_duration)

        # --- Forwarding Decision ---
        if not processing_error:
            if level_processed_here < 3: # Need to forward UPWARDS to Cloud
                if cloud_url:
                    data_to_forward = {"payload": current_data, "last_processed_level": level_processed_here}
                    print(f"Proxy ({container_name}): Forwarding data (processed up to L{level_processed_here}) to Cloud ({cloud_url})...")
                    forward_start_time = time.time()
                    try:
                        cloud_response = requests.post(cloud_url, json=data_to_forward, timeout=(10, 20)) # Longer timeout for cloud
                        forward_duration = time.time() - forward_start_time
                        FORWARD_TO_CLOUD_LATENCY.observe(forward_duration) # RTT + Cloud time
                        cloud_response.raise_for_status()
                        FORWARD_TO_CLOUD_COUNT.inc()
                        print(f"Proxy ({container_name}): Forward success to Cloud. RTT+CloudTime: {forward_duration:.4f}s")
                        # Relay cloud's response back to the gateway
                        try:
                            response_payload = cloud_response.json()
                            response_payload['processed_up_to'] = response_payload.get('processed_up_to', level_processed_here)
                            final_response_to_gateway = (response_payload, cloud_response.status_code)
                        except json.JSONDecodeError:
                            final_response_to_gateway = ({"status": f"cloud_status_{cloud_response.status_code}_no_json", "processed_up_to": level_processed_here}), cloud_response.status_code
                    except requests.exceptions.RequestException as e:
                        FORWARD_TO_CLOUD_FAILURES.inc()
                        error_detail = f"{type(e).__name__}: {e}"
                        print(f"ERROR ({container_name}): Failed to forward to Cloud: {error_detail}")
                        final_response_to_gateway = ({"status": "forward_to_cloud_failed", "processed_up_to": level_processed_here, "detail": error_detail}), 502
                else: # Cannot forward
                    print(f"WARN ({container_name}): CLOUD_URL not set, cannot forward incomplete processing (L{level_processed_here}).")
                    final_response_to_gateway = ({"status": f"processed_L{level_processed_here}_cannot_forward_no_cloud"}), 500
            else: # level_processed_here == 3 (Final processing done here on Proxy)
                print(f"Proxy ({container_name}): Final processing complete (L3).")
                # Structure the response for the gateway
                final_response_to_gateway = ({"status": "processing_complete", "final_payload_preview": json.dumps(current_data)[:100], "processed_up_to": 3}), 200

        # Return the determined response and status code TO THE GATEWAY
        return jsonify(final_response_to_gateway[0]), final_response_to_gateway[1]

    except (TypeError, ValueError) as req_err: # Catch specific request format errors
        PROXY_ERROR_COUNT.inc()
        print(f"ERROR ({container_name}): Invalid request data from gateway: {req_err}")
        return jsonify({"error": f"Bad Request from Gateway: {req_err}"}), 400
    except Exception as e: # Catch all other unexpected errors
        PROXY_ERROR_COUNT.inc()
        internal_processing_duration = time.time() - processing_start_time
        PROXY_INTERNAL_LATENCY.observe(internal_processing_duration)
        print(f"FATAL Error in Proxy ({container_name}): {type(e).__name__} - {e}")
        print(traceback.format_exc())
        # Return generic error to gateway
        return jsonify({"error": "Internal server error on proxy"}), 500

if __name__ == '__main__':
    print("Python Proxy Service Starting...")
    start_cpu_monitoring()
    app.run(host='0.0.0.0', port=8000)

================================================================================

Filename: proxy_py/requirements.txt
Content:
flask
requests
matplotlib
numpy
psutil
prometheus-client
flask-prometheus-metrics
PyYAML

================================================================================

Filename: requirements.txt
Content:
flask
requests
matplotlib
psutil
python-json-logger==2.0.7
prometheus-client==0.17.1
flask-prometheus-metrics==1.0.0
PyYAML==6.0.1

================================================================================

Filename: shared_modules/__init__.py
Content:


================================================================================

Filename: shared_modules/client_module.py
Content:
import json
import time
import numpy as np
from typing import Dict, Any, Union, Optional
from .metrics import *


class ClientModule:
    def __init__(self):
        # Initialize parameters based on iFogSim implementation
        self.buffer_size = 100  # Match concentration calculator's window size
        self.sampling_rate = 250  # Hz, matching concentration calculator
        self.buffer = []
        self.last_send_time = 0
        self.send_interval = 0.1  # 100ms interval between sends
        self.error_threshold = 0.5  # 50% threshold for signal quality

    def process_eeg(self, eeg_data: Union[str, Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Process incoming EEG data and prepare it for transmission.
        
        Args:
            eeg_data: Raw EEG data from sensor, can be JSON string or dict
            
        Returns:
            Processed EEG data ready for transmission, or None if data is invalid
        """
        original_request_id = None
        original_creation_time = None
        try:
            # Parse input data if it's a string
            if isinstance(eeg_data, str):
                try:
                    data = json.loads(eeg_data)
                except json.JSONDecodeError:
                    # Handle non-JSON string input (for backward compatibility)
                    data = {"eeg_values": [float(eeg_data)], "timestamp": time.time()}
            else:
                data = eeg_data

            # Extract or create EEG values
            eeg_values = data.get('eeg_values', [])
            if not eeg_values:
                print("Client Module: No EEG values found in data")
                return None
            
            original_request_id = data.get('request_id')
            original_creation_time = data.get('creation_time')
            # Quality check on EEG values
            quality_score = self._calculate_quality_score(eeg_values)
            EEG_QUALITY_SCORE.set(quality_score)
            
            if not self._check_signal_quality(eeg_values):
                print(f"Client Module: Discarding low quality EEG data (score: {quality_score:.2f})")
                EEG_DISCARDED_TOTAL.inc()
                return None

            # Calculate and record alpha power
            alpha_power = self._calculate_alpha_power(eeg_values)
            EEG_ALPHA_POWER.set(alpha_power)
            
            # Calculate and record noise level
            noise_level = self._calculate_noise_level(eeg_values)
            EEG_NOISE_LEVEL.set(noise_level)

            # Add to buffer and maintain size
            self.buffer.extend(eeg_values)
            if len(self.buffer) > self.buffer_size:
                self.buffer = self.buffer[-self.buffer_size:]

            # Rate limiting for data transmission
            current_time = time.time()
            if current_time - self.last_send_time < self.send_interval:
                return None

            self.last_send_time = current_time

            # Prepare data for transmission
            processed_data = {
                "eeg_values": self.buffer.copy(),
                "timestamp": data.get('timestamp', current_time),
                "sampling_rate": self.sampling_rate,
                "metadata": {
                    "buffer_size": self.buffer_size,
                    "quality_score": self._calculate_quality_score(eeg_values)
                }
            }
            if original_request_id:
                processed_data['request_id'] = original_request_id
            if original_creation_time:
                processed_data['creation_time'] = original_creation_time
            print(f"Client Module: Sending processed data to Gateway")
            return processed_data

        except Exception as e:
            print(f"Client Module Error: {str(e)}")
            return None

    def _check_signal_quality(self, eeg_values: list) -> bool:
        """Check if the EEG signal quality is acceptable.
        
        Args:
            eeg_values: List of EEG samples to check
            
        Returns:
            True if signal quality is acceptable, False otherwise
        """
        quality_score = self._calculate_quality_score(eeg_values)
        return quality_score >= (1 - self.error_threshold)

    def _calculate_quality_score(self, eeg_values: list) -> float:
        """Calculate a quality score for the EEG signal.
        
        Args:
            eeg_values: List of EEG samples
            
        Returns:
            Quality score between 0 and 1
        """
        if not eeg_values:
            return 0.0

        try:
            # Simple quality metrics:
            # 1. Check for NaN or infinity values
            valid_values = [v for v in eeg_values if isinstance(v, (int, float)) 
                        and not isinstance(v, bool) and -1e6 <= v <= 1e6]
            
            if not valid_values:
                return 0.0

            # 2. Calculate signal-to-noise ratio (simplified)
            mean_signal = sum(valid_values) / len(valid_values)
            variance = sum((x - mean_signal) ** 2 for x in valid_values) / len(valid_values)
            print(f"Calculated variance: {variance}") # DEBUG
            quality_score = 1.0 / (1.0 + variance) if variance > 0 else 1.0
            print(f"Calculated quality score (before clamp): {quality_score}") # DEBUG
            final_score = min(1.0, max(0.0, quality_score))
            print(f"Final quality score: {final_score}") # DEBUG
            return final_score

        except Exception as e:
            print(f"Error calculating quality score: {str(e)}")
            return 0.0

    def update_concentration_display(self, concentration_data: Dict[str, Any]) -> None:
        """Update the display with concentration information.
        
        Args:
            concentration_data: Dictionary containing concentration level and metadata
        """
        try:
            level = concentration_data.get('concentration_level', 'UNKNOWN')
            value = concentration_data.get('concentration_value', 0.0)
            timestamp = concentration_data.get('timestamp', time.time())
            
            print(f"Client Module: Concentration Update at {timestamp}")
            print(f"Level: {level}, Value: {value:.2f}")
            
            if 'metadata' in concentration_data:
                print(f"Additional Info: {concentration_data['metadata']}")
                
        except Exception as e:
            print(f"Error updating concentration display: {str(e)}")

    def _calculate_alpha_power(self, eeg_values: list) -> float:
        """Calculate power in alpha frequency band (8-13 Hz)"""
        if not eeg_values:
            return 0.0
        # Simple approximation of alpha power
        return np.mean(np.abs(eeg_values))

    def _calculate_noise_level(self, eeg_values: list) -> float:
        """Calculate approximate noise level in the signal"""
        if not eeg_values:
            return 0.0
        return np.std(eeg_values) / np.mean(np.abs(eeg_values)) if np.mean(np.abs(eeg_values)) > 0 else 0.0

# Example usage
if __name__ == "__main__":
    client = ClientModule()
    
    # Test with various input types
    test_data = [
        {"eeg_values": [0.5, 0.6, 0.7], "timestamp": time.time()},
        "0.8",  # Simple string value
        "{\"eeg_values\": [0.9, 1.0, 1.1]}",  # JSON string
        {"eeg_values": [], "timestamp": time.time()},  # Empty values
        "invalid_data"  # Should be handled gracefully
    ]
    
    for data in test_data:
        result = client.process_eeg(data)
        if result:
            print(f"Processed result: {result}\n")
        else:
            print(f"Data processing failed for input: {data}\n")
            

================================================================================

Filename: shared_modules/concentration_calculator_module.py
Content:
import json
import numpy as np
from typing import Dict, Any, Union

class ConcentrationCalculatorModule:
    def __init__(self):
        # Initialize thresholds and parameters based on iFogSim implementation
        self.eeg_window_size = 100  # Number of EEG samples to analyze
        self.concentration_threshold = 0.7  # Threshold for high concentration
        self.sampling_rate = 250  # Hz, typical for EEG
        self.buffer = []

    def calculate_concentration(self, sensor_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate concentration level from EEG sensor data.
        
        Args:
            sensor_data: Raw EEG data from sensor, can be JSON string or dict
            
        Returns:
            Dict containing concentration level and metadata
        """
        original_request_id = None
        original_creation_time = None
        try:
            # Parse input data if it's a string
            if isinstance(sensor_data, str):
                data = json.loads(sensor_data)
            else:
                data = sensor_data

            # Extract EEG values
            eeg_values = data.get('eeg_values', [])
            if not eeg_values:
                raise ValueError("No EEG values found in sensor data")
            
            original_request_id = data.get('request_id')
            original_creation_time = data.get('creation_time')
            # Add to buffer and maintain window size
            self.buffer.extend(eeg_values)
            if len(self.buffer) > self.eeg_window_size:
                self.buffer = self.buffer[-self.eeg_window_size:]

            # Calculate concentration metrics
            # Using signal power in alpha band (8-13 Hz) as concentration indicator
            # Higher alpha power typically indicates relaxed attention/concentration
            concentration_value = self._calculate_alpha_power(self.buffer)
            
            # Normalize concentration value to 0-1 range
            normalized_concentration = min(1.0, max(0.0, concentration_value))
            
            # Determine concentration level
            concentration_level = "HIGH" if normalized_concentration >= self.concentration_threshold else "LOW"
            
            result = {
                "concentration_level": concentration_level,
                "concentration_value": normalized_concentration,
                "timestamp": data.get('timestamp', None),
                "metadata": {
                    "window_size": self.eeg_window_size,
                    "threshold": self.concentration_threshold
                }
            }
            if original_request_id:
                result['request_id'] = original_request_id
            if original_creation_time:
                result['creation_time'] = original_creation_time
            print(f"Concentration Calculator: Processed result = {result}")
            return result

        except Exception as e:
            print(f"Error calculating concentration: {str(e)}")
            error_result = { "error": str(e), "concentration_level": "ERROR" }
            # Optionally add tracking fields to error result too?
            if original_request_id: error_result['request_id'] = original_request_id
            if original_creation_time: error_result['creation_time'] = original_creation_time
            return error_result

    def _calculate_alpha_power(self, eeg_data: list) -> float:
        """Calculate the power in alpha frequency band (8-13 Hz).
        
        Args:
            eeg_data: List of EEG samples
            
        Returns:
            Normalized power value in alpha band
        """
        try:
            # Convert to numpy array for calculations
            signal = np.array(eeg_data)
            
            # Apply FFT to get frequency components
            fft_vals = np.abs(np.fft.rfft(signal))
            fft_freq = np.fft.rfftfreq(len(signal), 1.0/self.sampling_rate)
            
            # Calculate power in alpha band (8-13 Hz)
            alpha_mask = (fft_freq >= 8) & (fft_freq <= 13)
            alpha_power = np.mean(fft_vals[alpha_mask]**2)
            
            # Normalize by total power
            total_power = np.mean(fft_vals**2)
            normalized_power = alpha_power / total_power if total_power > 0 else 0
            
            return normalized_power
            
        except Exception as e:
            print(f"Error in alpha power calculation: {str(e)}")
            return 0.0

# Example Usage
if __name__ == "__main__":
    calculator = ConcentrationCalculatorModule()
    data1 = "processed_valid_eeg_some_data"
    calculator.calculate_concentration(data1)
    data2 = "processed_invalid_eeg_error_data" # Example of invalid data - though Client Module should ideally filter this out
    calculator.calculate_concentration(data2)

================================================================================

Filename: shared_modules/connector_module.py
Content:
import time 
import json
from typing import Dict, Any
import socket 


class ConnectorModule:
    def __init__(self):
        self.location = socket.gethostname()
        print(f"Connector Module Initialized on {self.location}")

    def process_concentration_data(self, concentration_result: Dict[str, Any]) -> Dict[str, Any]:
        original_request_id = None
        original_creation_time = None
        try:
            print(f"Connector Module ({self.location}): Received: {json.dumps(concentration_result)[:150]}...")
            
            original_request_id = concentration_result.get('request_id')
            original_creation_time = concentration_result.get('creation_time')
            final_result = {
                "final_concentration_level": concentration_result.get("concentration_level", "UNKNOWN_FINAL"),
                "original_concentration_value": concentration_result.get("concentration_value"),
                "processed_timestamp": time.time(),
                "source": f"{self.location}_connector"
            }
            if original_request_id:
                final_result['request_id'] = original_request_id
            if original_creation_time:
                final_result['creation_time'] = original_creation_time
            print(f"Connector Module ({self.location}): Processed final result.")
            return final_result
        except Exception as e:
            print(f"Error in Connector Module ({self.location}): {str(e)}")
            error_result = { "error": str(e), "source": f"{self.location}_connector_error" }
            # Optionally add tracking fields to error result too?
            if original_request_id: error_result['request_id'] = original_request_id
            if original_creation_time: error_result['creation_time'] = original_creation_time
            return error_result
        

================================================================================

Filename: shared_modules/cpu_monitor.py
Content:
import os
import time
import json
import math

# --- Previous state ---
_last_cpu_check_time = None
_last_cpu_usage_value = None
_last_cpu_usage_key = None # To remember if it was ns or usec

def get_cpu_usage():
    """
    Get the CPU usage of the current Docker container.
    
    Returns:
        dict: Dictionary containing CPU usage information
    """
    # CPU usage in nanoseconds
    try:
        # For Docker with cgroups v1
        with open('/sys/fs/cgroup/cpu/cpuacct.usage', 'r') as f:
            cpu_usage_ns = int(f.read().strip())
            return {'cpu_usage_ns': cpu_usage_ns}
    except FileNotFoundError:
        pass
    
    try:
        # For Docker with cgroups v2
        with open('/sys/fs/cgroup/cpu.stat', 'r') as f:
            stats = {}
            for line in f:
                key, value = line.strip().split()
                stats[key] = int(value)
            return stats
    except FileNotFoundError:
        pass
    
    # Try alternative paths for different container runtimes
    cgroup_paths = [
        # Docker with custom cgroup path
        '/sys/fs/cgroup/cpu,cpuacct/cpuacct.usage',
        # Kubernetes
        '/sys/fs/cgroup/cpuacct/cpuacct.usage',
        # Docker Compose with project name
        '/sys/fs/cgroup/cpu/docker/cpuacct.usage',
        # cgroups v2 unified hierarchy
        '/sys/fs/cgroup/cpu.stat',
    ]
    
    for path in cgroup_paths:
        try:
            if path.endswith('cpu.stat'):
                with open(path, 'r') as f:
                    stats = {}
                    for line in f:
                        key, value = line.strip().split()
                        stats[key] = int(value)
                    return stats
            else:
                with open(path, 'r') as f:
                    cpu_usage_ns = int(f.read().strip())
                    return {'cpu_usage_ns': cpu_usage_ns}
        except (FileNotFoundError, IOError):
            continue
    
    raise Exception("Could not find CPU usage information in cgroup filesystem")


def get_cpu_quota():
    """
    Get the CPU quota and period for the container.
    
    Returns:
        dict: Dictionary with CPU quota and period
    """
    quota = -1
    period = 100000  # Default period in microseconds
    
    # Try different cgroup paths
    quota_paths = [
        # cgroups v1
        '/sys/fs/cgroup/cpu/cpu.cfs_quota_us',
        '/sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us',
        # cgroups v2
        '/sys/fs/cgroup/cpu.max',
    ]
    
    period_paths = [
        # cgroups v1
        '/sys/fs/cgroup/cpu/cpu.cfs_period_us',
        '/sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us',
        # cgroups v2 uses the same cpu.max file for both quota and period
    ]
    
    # Try to get quota
    for path in quota_paths:
        try:
            with open(path, 'r') as f:
                content = f.read().strip()
                # For cgroups v2, the format is "quota period"
                if ' ' in content:
                    quota_val, period_val = content.split()
                    if quota_val != 'max':
                        quota = int(quota_val)
                    period = int(period_val)
                    break
                else:
                    quota = int(content)
                    break
        except (FileNotFoundError, IOError):
            continue
    
    # If we didn't get period from cpu.max, try dedicated period files
    if ' ' not in content and quota != -1:
        for path in period_paths:
            try:
                with open(path, 'r') as f:
                    period = int(f.read().strip())
                    break
            except (FileNotFoundError, IOError):
                continue
    
    return {'cpu_quota_us': quota, 'cpu_period_us': period}


def get_container_cpu_percent_non_blocking():
    """
    Calculate CPU usage percentage since the last call. Non-blocking.

    Returns:
        dict or None: CPU usage info if possible, None if first call or error.
                      Includes 'cpu_percent_normalized' crucial for placement.
    """
    global _last_cpu_check_time, _last_cpu_usage_value, _last_cpu_usage_key

    current_time = time.monotonic() # Use monotonic clock for intervals
    current_usage_info = get_cpu_usage()

    if current_usage_info is None:
        print("WARN: get_cpu_usage() returned None.")
        return None

    # Determine current usage value and key
    current_usage_value = None
    current_usage_key = None
    if 'usage_usec' in current_usage_info:
         current_usage_value = current_usage_info['usage_usec']
         current_usage_key = 'usage_usec'
    elif 'cpu_usage_ns' in current_usage_info:
         current_usage_value = current_usage_info['cpu_usage_ns']
         current_usage_key = 'cpu_usage_ns'
    else:
         print("WARN: Could not determine usage key ('usage_usec' or 'cpu_usage_ns')")
         return None # Cannot calculate

    # --- Check if we have previous data to calculate delta ---
    if _last_cpu_check_time is None or _last_cpu_usage_value is None or _last_cpu_usage_key is None:
        print("INFO: First CPU usage reading, cannot calculate percentage yet.")
        # Store current state for the *next* call
        _last_cpu_check_time = current_time
        _last_cpu_usage_value = current_usage_value
        _last_cpu_usage_key = current_usage_key
        return None # Cannot calculate percentage on first run

    # --- Calculate delta ---
    time_delta_sec = current_time - _last_cpu_check_time
    usage_delta = current_usage_value - _last_cpu_usage_value

    # Convert previous/current usage to nanoseconds for consistent calculation
    usage_delta_ns = 0
    if current_usage_key == 'usage_usec' and _last_cpu_usage_key == 'usage_usec':
        usage_delta_ns = usage_delta * 1000
    elif current_usage_key == 'cpu_usage_ns' and _last_cpu_usage_key == 'cpu_usage_ns':
        usage_delta_ns = usage_delta
    else:
        # Handle potential unit mismatch between readings (unlikely but possible)
        print(f"WARN: CPU usage unit mismatch between readings ({_last_cpu_usage_key} -> {current_usage_key}). Recalculating on next cycle.")
        # Reset and wait for next cycle
        _last_cpu_check_time = current_time
        _last_cpu_usage_value = current_usage_value
        _last_cpu_usage_key = current_usage_key
        return None


    # Store current state for the *next* call BEFORE returning
    _last_cpu_check_time = current_time
    _last_cpu_usage_value = current_usage_value
    _last_cpu_usage_key = current_usage_key


    # --- Percentage Calculation ---
    if time_delta_sec <= 0:
         print("WARN: Time delta is zero or negative, cannot calculate CPU percent.")
         return None # Avoid division by zero

    time_delta_ns = time_delta_sec * 1_000_000_000

    # Raw percentage relative to one core
    cpu_percent_raw = (usage_delta_ns / time_delta_ns) * 100

    # Get quota info
    quota_info = get_cpu_quota()
    cpu_quota_us = quota_info.get('cpu_quota_us', -1)
    cpu_period_us = quota_info.get('cpu_period_us', 100000)

    cpu_percent_normalized = math.nan # Default to NaN if no quota
    num_cores_allocated = math.nan

    # Calculate normalized percentage if quota is set
    if cpu_quota_us > 0 and cpu_period_us > 0:
        num_cores_allocated = cpu_quota_us / cpu_period_us
        if num_cores_allocated > 0:
            cpu_percent_normalized = cpu_percent_raw / num_cores_allocated
            # Clamp between 0 and 100
            cpu_percent_normalized = max(0.0, min(100.0, cpu_percent_normalized))
        else:
            print("WARN: Calculated zero allocated cores based on quota/period.")
    else:
        print("INFO: No CPU quota set (-1 or max), normalized percentage not applicable.")


    return {
        'cpu_percent_raw': max(0.0, cpu_percent_raw), # Percentage relative to 1 core (can be > 100)
        'cpu_percent_normalized': cpu_percent_normalized, # Percentage relative to quota (0-100 or NaN)
        'num_cores_allocated': num_cores_allocated, # Effective cores from quota (or NaN)
        'interval_sec': time_delta_sec
    }

def monitor_container_cpu(interval=1.0, count=10):
    print(f"Monitoring container CPU usage (non-blocking) approx every {interval}s intervals:")
    print("Timestamp".ljust(25), "Norm %".rjust(8), "Raw %".rjust(8), "Cores".rjust(6))
    print("-" * 50)

    # Initial call to prime the _last values
    get_container_cpu_percent_non_blocking()
    time.sleep(interval) # Initial wait

    for _ in range(count):
        start_call = time.monotonic()
        cpu_info = get_container_cpu_percent_non_blocking()
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        if cpu_info:
             norm_str = f"{cpu_info['cpu_percent_normalized']:7.2f}%" if not math.isnan(cpu_info['cpu_percent_normalized']) else "  N/A  "
             raw_str = f"{cpu_info['cpu_percent_raw']:7.2f}%"
             cores_str = f"{cpu_info['num_cores_allocated']:5.2f}" if not math.isnan(cpu_info['num_cores_allocated']) else " N/A "
             print(f"{timestamp}  {norm_str} {raw_str} {cores_str}")
        else:
            print(f"{timestamp}  {'Waiting'.rjust(8)} {' '.rjust(8)} {' '.rjust(6)}")

        # Wait for the remainder of the interval
        elapsed = time.monotonic() - start_call
        wait_time = interval - elapsed
        if wait_time > 0:
            time.sleep(wait_time)


if __name__ == "__main__":
    print("Container CPU Usage Monitor")
    print("==========================")
    try:
        monitor_container_cpu(interval=2.0, count=5)
    except Exception as e:
        print(f"Error: {e}")
        print("\nFallback to cgroup filesystem inspection:")
        
        # Try to list available cgroup files as a fallback
        cgroup_dirs = [
            '/sys/fs/cgroup',
            '/sys/fs/cgroup/cpu',
            '/sys/fs/cgroup/cpuacct',
            '/sys/fs/cgroup/cpu,cpuacct'
        ]
        
        for directory in cgroup_dirs:
            if os.path.exists(directory):
                print(f"\nContents of {directory}:")
                try:
                    print(os.listdir(directory))
                except:
                    print("Cannot access directory")


================================================================================

Filename: shared_modules/metrics.py
Content:
from prometheus_client import *

# --- Keep existing Mobile specific metrics ---
EEG_DATA_PROCESSED = Counter('eeg_data_processed_total', 'Total number of EEG data points processed by client module')
# Mobile perspective RTT Gauge
GATEWAY_REQUEST_LATENCY = Gauge('gateway_request_latency_seconds', 'Gateway request round trip time in seconds (mobile perspective)')
GATEWAY_REQUEST_FAILURES = Counter('gateway_request_failures_total', 'Total failed requests sent from mobile to gateway')
# EEG Signal quality metrics (only relevant for mobile)
EEG_QUALITY_SCORE = Gauge('eeg_quality_score', 'Current EEG signal quality score')
EEG_DISCARDED_TOTAL = Counter('eeg_discarded_total', 'Total number of discarded EEG data points by client module')
EEG_ALPHA_POWER = Gauge('eeg_alpha_power', 'Current alpha wave power in EEG signal (mobile calc)')
EEG_NOISE_LEVEL = Gauge('eeg_noise_level', 'Current noise level in EEG signal (mobile calc)')

# --- General Labeled Metrics (Can be defined here or in each service) ---
MODULE_EXECUTIONS = Counter(
    'module_executions_total',
    'Number of times a module successfully executed',
    ['tier', 'module']
)
MODULE_LATENCY = Histogram(
    'module_execution_latency_seconds',
    'Latency of module execution',
    ['tier', 'module']
)
MODULE_ERRORS = Counter(
    'module_errors_total',
    'Number of errors during module execution',
    ['tier', 'module']
)
PASSTHROUGH_COUNT = Counter(
    'passthrough_requests_total',
    'Number of requests passed through without processing',
    ['tier']
)
E2E_LATENCY = Histogram(
    'e2e_processing_latency_seconds',
    'End-to-end latency from data creation to final L3 processing completion',
    ['final_tier'] # Label to indicate which tier finished L3
)

================================================================================
